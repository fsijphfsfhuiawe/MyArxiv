{"2024-10-16T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.12791v1","updated":"2024-10-16T17:59:52Z","published":"2024-10-16T17:59:52Z","title":"Context is Key(NMF): Modelling Topical Information Dynamics in Chinese\n  Diaspora Media","summary":"  Does the People's Republic of China (PRC) interfere with European elections\nthrough ethnic Chinese diaspora media? This question forms the basis of an\nongoing research project exploring how PRC narratives about European elections\nare represented in Chinese diaspora media, and thus the objectives of PRC news\nmedia manipulation. In order to study diaspora media efficiently and at scale,\nit is necessary to use techniques derived from quantitative text analysis, such\nas topic modelling. In this paper, we present a pipeline for studying\ninformation dynamics in Chinese media. Firstly, we present KeyNMF, a new\napproach to static and dynamic topic modelling using transformer-based\ncontextual embedding models. We provide benchmark evaluations to demonstrate\nthat our approach is competitive on a number of Chinese datasets and metrics.\nSecondly, we integrate KeyNMF with existing methods for describing information\ndynamics in complex systems. We apply this pipeline to data from five news\nsites, focusing on the period of time leading up to the 2024 European\nparliamentary elections. Our methods and results demonstrate the effectiveness\nof KeyNMF for studying information dynamics in Chinese media and lay groundwork\nfor further work addressing the broader research questions.\n","authors":["Ross Deans Kristensen-McLachlan","Rebecca M. M. Hicke","Márton Kardos","Mette Thunø"],"pdf_url":"https://arxiv.org/pdf/2410.12791v1.pdf","comment":"Accepted to the 2024 Computational Humanities Research Conference\n  (CHR)"},{"id":"http://arxiv.org/abs/2410.12788v1","updated":"2024-10-16T17:59:32Z","published":"2024-10-16T17:59:32Z","title":"Meta-Chunking: Learning Efficient Text Segmentation via Logical\n  Perception","summary":"  Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline, which impacts the quality of knowledge-intensive\ntasks. This paper introduces the concept of Meta-Chunking, which refers to a\ngranularity between sentences and paragraphs, consisting of a collection of\nsentences within a paragraph that have deep linguistic logical connections. To\nimplement Meta-Chunking, we designed two strategies based on LLMs: Margin\nSampling Chunking and Perplexity Chunking. The former employs LLMs to perform\nbinary classification on whether consecutive sentences need to be segmented,\nmaking decisions based on the probability difference obtained from margin\nsampling. The latter precisely identifies text chunk boundaries by analyzing\nthe characteristics of perplexity distribution. Additionally, considering the\ninherent complexity of different texts, we propose a strategy that combines\nMeta-Chunking with dynamic merging to achieve a balance between fine-grained\nand coarse-grained text chunking. Experiments conducted on eleven datasets\ndemonstrate that Meta-Chunking can more efficiently improve the performance of\nsingle-hop and multi-hop question answering based on RAG. For instance, on the\n2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only\nconsuming 45.8% of the time. Our code is available at\nhttps://github.com/IAAR-Shanghai/Meta-Chunking.\n","authors":["Jihao Zhao","Zhiyuan Ji","Pengnian Qi","Simin Niu","Bo Tang","Feiyu Xiong","Zhiyu Li"],"pdf_url":"https://arxiv.org/pdf/2410.12788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12784v1","updated":"2024-10-16T17:58:19Z","published":"2024-10-16T17:58:19Z","title":"JudgeBench: A Benchmark for Evaluating LLM-based Judges","summary":"  LLM-based judges have emerged as a scalable alternative to human evaluation\nand are increasingly used to assess, compare, and improve models. However, the\nreliability of LLM-based judges themselves is rarely scrutinized. As LLMs\nbecome more advanced, their responses grow more sophisticated, requiring\nstronger judges to evaluate them. Existing benchmarks primarily focus on a\njudge's alignment with human preferences, but often fail to account for more\nchallenging tasks where crowdsourced human preference is a poor indicator of\nfactual and logical correctness. To address this, we propose a novel evaluation\nframework to objectively evaluate LLM-based judges. Based on this framework, we\npropose JudgeBench, a benchmark for evaluating LLM-based judges on challenging\nresponse pairs spanning knowledge, reasoning, math, and coding. JudgeBench\nleverages a novel pipeline for converting existing difficult datasets into\nchallenging response pairs with preference labels reflecting objective\ncorrectness. Our comprehensive evaluation on a collection of prompted judges,\nfine-tuned judges, multi-agent judges, and reward models shows that JudgeBench\nposes a significantly greater challenge than previous benchmarks, with many\nstrong models (e.g., GPT-4o) performing just slightly better than random\nguessing. Overall, JudgeBench offers a reliable platform for assessing\nincreasingly advanced LLM-based judges. Data and code are available at\nhttps://github.com/ScalerLab/JudgeBench .\n","authors":["Sijun Tan","Siyuan Zhuang","Kyle Montgomery","William Y. Tang","Alejandro Cuadron","Chenguang Wang","Raluca Ada Popa","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2410.12784v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.12782v1","updated":"2024-10-16T17:56:49Z","published":"2024-10-16T17:56:49Z","title":"In-Context Learning Enables Robot Action Prediction in LLMs","summary":"  Recently, Large Language Models (LLMs) have achieved remarkable success using\nin-context learning (ICL) in the language domain. However, leveraging the ICL\ncapabilities within LLMs to directly predict robot actions remains largely\nunexplored. In this paper, we introduce RoboPrompt, a framework that enables\noff-the-shelf text-only LLMs to directly predict robot actions through ICL\nwithout training. Our approach first heuristically identifies keyframes that\ncapture important moments from an episode. Next, we extract end-effector\nactions from these keyframes as well as the estimated initial object poses, and\nboth are converted into textual descriptions. Finally, we construct a\nstructured template to form ICL demonstrations from these textual descriptions\nand a task instruction. This enables an LLM to directly predict robot actions\nat test time. Through extensive experiments and analysis, RoboPrompt shows\nstronger performance over zero-shot and ICL baselines in simulated and\nreal-world settings.\n","authors":["Yida Yin","Zekai Wang","Yuvan Sharma","Dantong Niu","Trevor Darrell","Roei Herzig"],"pdf_url":"https://arxiv.org/pdf/2410.12782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12777v1","updated":"2024-10-16T17:51:25Z","published":"2024-10-16T17:51:25Z","title":"Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned\n  Concepts","summary":"  With the rapid progress of diffusion-based content generation, significant\nefforts are being made to unlearn harmful or copyrighted concepts from\npretrained diffusion models (DMs) to prevent potential model misuse. However,\nit is observed that even when DMs are properly unlearned before release,\nmalicious finetuning can compromise this process, causing DMs to relearn the\nunlearned concepts. This occurs partly because certain benign concepts (e.g.,\n\"skin\") retained in DMs are related to the unlearned ones (e.g., \"nudity\"),\nfacilitating their relearning via finetuning. To address this, we propose\nmeta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an\nunlearned DM when used as is; moreover, if the meta-unlearned DM undergoes\nmalicious finetuning on unlearned concepts, the related benign concepts\nretained within it will be triggered to self-destruct, hindering the relearning\nof unlearned concepts. Our meta-unlearning framework is compatible with most\nexisting unlearning methods, requiring only the addition of an\neasy-to-implement meta objective. We validate our approach through empirical\nexperiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4\nand SDXL), supported by extensive ablation studies. Our code is available at\nhttps://github.com/sail-sg/Meta-Unlearning.\n","authors":["Hongcheng Gao","Tianyu Pang","Chao Du","Taihang Hu","Zhijie Deng","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.12777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12774v1","updated":"2024-10-16T17:49:45Z","published":"2024-10-16T17:49:45Z","title":"Identifying Task Groupings for Multi-Task Learning Using Pointwise\n  V-Usable Information","summary":"  The success of multi-task learning can depend heavily on which tasks are\ngrouped together. Naively grouping all tasks or a random set of tasks can\nresult in negative transfer, with the multi-task models performing worse than\nsingle-task models. Though many efforts have been made to identify task\ngroupings and to measure the relatedness among different tasks, it remains a\nchallenging research topic to define a metric to identify the best task\ngrouping out of a pool of many potential task combinations. We propose a metric\nof task relatedness based on task difficulty measured by pointwise V-usable\ninformation (PVI). PVI is a recently proposed metric to estimate how much\nusable information a dataset contains given a model. We hypothesize that tasks\nwith not statistically different PVI estimates are similar enough to benefit\nfrom the joint learning process. We conduct comprehensive experiments to\nevaluate the feasibility of this metric for task grouping on 15 NLP datasets in\nthe general, biomedical, and clinical domains. We compare the results of the\njoint learners against single learners, existing baseline methods, and recent\nlarge language models, including Llama 2 and GPT-4. The results show that by\ngrouping tasks with similar PVI estimates, the joint learners yielded\ncompetitive results with fewer total parameters, with consistent performance\nacross domains.\n","authors":["Yingya Li","Timothy Miller","Steven Bethard","Guergana Savova"],"pdf_url":"https://arxiv.org/pdf/2410.12774v1.pdf","comment":"main paper 12 pages, Appendix 7 pages, 1 figure, 18 tables"},{"id":"http://arxiv.org/abs/2404.12494v2","updated":"2024-10-16T17:45:10Z","published":"2024-04-18T20:17:23Z","title":"BIRD: A Trustworthy Bayesian Inference Framework for Large Language\n  Models","summary":"  Predictive models often need to work with incomplete information in\nreal-world tasks. Consequently, they must provide reliable probability or\nconfidence estimation, especially in large-scale decision making and planning\ntasks. Current large language models (LLM) are insufficient for such accurate\nestimations, but they can generate relevant factors that may affect the\nprobabilities, produce coarse-grained probabilities when the information is\nmore complete, and help determine which factors are relevant to specific\ndownstream contexts. In this paper, we make use of these capabilities of LLMs\nto provide a significantly more accurate probabilistic estimation. We propose\nBIRD, a novel probabilistic inference framework that aligns a Bayesian network\nwith LLM abductions and then estimates more accurate probabilities in a\ndeduction step. We show BIRD provides reliable probability estimations that are\n30\\% better than those provided directly by LLM baselines. These estimates can\nfurther contribute to better and more trustworthy decision-making.\n","authors":["Yu Feng","Ben Zhou","Weidong Lin","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2404.12494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12759v1","updated":"2024-10-16T17:30:58Z","published":"2024-10-16T17:30:58Z","title":"Unitary Multi-Margin BERT for Robust Natural Language Processing","summary":"  Recent developments in adversarial attacks on deep learning leave many\nmission-critical natural language processing (NLP) systems at risk of\nexploitation. To address the lack of computationally efficient adversarial\ndefense methods, this paper reports a novel, universal technique that\ndrastically improves the robustness of Bidirectional Encoder Representations\nfrom Transformers (BERT) by combining the unitary weights with the multi-margin\nloss. We discover that the marriage of these two simple ideas amplifies the\nprotection against malicious interference. Our model, the unitary multi-margin\nBERT (UniBERT), boosts post-attack classification accuracies significantly by\n5.3% to 73.8% while maintaining competitive pre-attack accuracies. Furthermore,\nthe pre-attack and post-attack accuracy tradeoff can be adjusted via a single\nscalar parameter to best fit the design requirements for the target\napplications.\n","authors":["Hao-Yuan Chang","Kang L. Wang"],"pdf_url":"https://arxiv.org/pdf/2410.12759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12757v1","updated":"2024-10-16T17:25:25Z","published":"2024-10-16T17:25:25Z","title":"StyleDistance: Stronger Content-Independent Style Embeddings with\n  Synthetic Parallel Examples","summary":"  Style representations aim to embed texts with similar writing styles closely\nand texts with different styles far apart, regardless of content. However, the\ncontrastive triplets often used for training these representations may vary in\nboth style and content, leading to potential content leakage in the\nrepresentations. We introduce StyleDistance, a novel approach to training\nstronger content-independent style embeddings. We use a large language model to\ncreate a synthetic dataset of near-exact paraphrases with controlled style\nvariations, and produce positive and negative examples across 40 distinct style\nfeatures for precise contrastive learning. We assess the quality of our\nsynthetic data and embeddings through human and automatic evaluations.\nStyleDistance enhances the content-independence of style embeddings, which\ngeneralize to real-world benchmarks and outperform leading style\nrepresentations in downstream applications. Our model can be found at\nhttps://huggingface.co/StyleDistance/styledistance .\n","authors":["Ajay Patel","Jiacheng Zhu","Justin Qiu","Zachary Horvitz","Marianna Apidianaki","Kathleen McKeown","Chris Callison-Burch"],"pdf_url":"https://arxiv.org/pdf/2410.12757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10267v2","updated":"2024-10-16T17:22:54Z","published":"2023-11-17T01:27:01Z","title":"Energy and Carbon Considerations of Fine-Tuning BERT","summary":"  Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP\ncommunity, existing work quantifying energy costs and associated carbon\nemissions has largely focused on language model pre-training. Although a single\npre-training run draws substantially more energy than fine-tuning, fine-tuning\nis performed more frequently by many more individual actors, and thus must be\naccounted for when considering the energy and carbon footprint of NLP. In order\nto better characterize the role of fine-tuning in the landscape of energy and\ncarbon emissions in NLP, we perform a careful empirical study of the\ncomputational costs of fine-tuning across tasks, datasets, hardware\ninfrastructure and measurement modalities. Our experimental results allow us to\nplace fine-tuning energy and carbon costs into perspective with respect to\npre-training and inference, and outline recommendations to NLP researchers and\npractitioners who wish to improve their fine-tuning energy efficiency.\n","authors":["Xiaorong Wang","Clara Na","Emma Strubell","Sorelle Friedler","Sasha Luccioni"],"pdf_url":"https://arxiv.org/pdf/2311.10267v2.pdf","comment":"EMNLP 2023 Findings; First two authors contributed equally; 12 pages"},{"id":"http://arxiv.org/abs/2410.12750v1","updated":"2024-10-16T17:12:06Z","published":"2024-10-16T17:12:06Z","title":"Comparative Analysis of Extrinsic Factors for NER in French","summary":"  Named entity recognition (NER) is a crucial task that aims to identify\nstructured information, which is often replete with complex, technical terms\nand a high degree of variability. Accurate and reliable NER can facilitate the\nextraction and analysis of important information. However, NER for other than\nEnglish is challenging due to limited data availability, as the high expertise,\ntime, and expenses are required to annotate its data. In this paper, by using\nthe limited data, we explore various factors including model structure, corpus\nannotation scheme and data augmentation techniques to improve the performance\nof a NER model for French. Our experiments demonstrate that these approaches\ncan significantly improve the model's F1 score from original CRF score of 62.41\nto 79.39. Our findings suggest that considering different extrinsic factors and\ncombining these techniques is a promising approach for improving NER\nperformance where the size of data is limited.\n","authors":["Grace Yang","Zhiyi Li","Yandong Liu","Jungyeul Park"],"pdf_url":"https://arxiv.org/pdf/2410.12750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14391v3","updated":"2024-10-16T17:01:16Z","published":"2023-11-24T10:15:34Z","title":"ÚFAL CorPipe at CRAC 2023: Larger Context Improves Multilingual\n  Coreference Resolution","summary":"  We present CorPipe, the winning entry to the CRAC 2023 Shared Task on\nMultilingual Coreference Resolution. Our system is an improved version of our\nearlier multilingual coreference pipeline, and it surpasses other participants\nby a large margin of 4.5 percent points. CorPipe first performs mention\ndetection, followed by coreference linking via an antecedent-maximization\napproach on the retrieved spans. Both tasks are trained jointly on all\navailable corpora using a shared pretrained language model. Our main\nimprovements comprise inputs larger than 512 subwords and changing the mention\ndecoding to support ensembling. The source code is available at\nhttps://github.com/ufal/crac2023-corpipe.\n","authors":["Milan Straka"],"pdf_url":"https://arxiv.org/pdf/2311.14391v3.pdf","comment":"Accepted to CRAC 2023 (the Sixth Workshop on Computational Models of\n  Reference, Anaphora and Coreference)"},{"id":"http://arxiv.org/abs/2209.07278v3","updated":"2024-10-16T16:56:17Z","published":"2022-09-15T13:11:39Z","title":"ÚFAL CorPipe at CRAC 2022: Effectivity of Multilingual Models for\n  Coreference Resolution","summary":"  We describe the winning submission to the CRAC 2022 Shared Task on\nMultilingual Coreference Resolution. Our system first solves mention detection\nand then coreference linking on the retrieved spans with an\nantecedent-maximization approach, and both tasks are fine-tuned jointly with\nshared Transformer weights. We report results of fine-tuning a wide range of\npretrained models. The center of this contribution are fine-tuned multilingual\nmodels. We found one large multilingual model with sufficiently large encoder\nto increase performance on all datasets across the board, with the benefit not\nlimited only to the underrepresented languages or groups of typologically\nrelative languages. The source code is available at\nhttps://github.com/ufal/crac2022-corpipe.\n","authors":["Milan Straka","Jana Straková"],"pdf_url":"https://arxiv.org/pdf/2209.07278v3.pdf","comment":"Accepted to CRAC 2022 (Fifth Workshop on Computational Models of\n  Reference, Anaphora and Coreference)"},{"id":"http://arxiv.org/abs/2410.12735v1","updated":"2024-10-16T16:51:01Z","published":"2024-10-16T16:51:01Z","title":"CREAM: Consistency Regularized Self-Rewarding Language Models","summary":"  Recent self-rewarding large language models (LLM) have successfully applied\nLLM-as-a-Judge to iteratively improve the alignment performance without the\nneed of human annotations for preference data. These methods commonly utilize\nthe same LLM to act as both the policy model (which generates responses) and\nthe reward model (which scores and ranks those responses). The ranked responses\nare then used as preference pairs to train the LLM via direct alignment\ntechnologies (e.g. DPO). However, it is noteworthy that throughout this\nprocess, there is no guarantee of accuracy in the rewarding and ranking, which\nis critical for ensuring accurate rewards and high-quality preference data.\nEmpirical results from relatively small LLMs (e.g., 7B parameters) also\nindicate that improvements from self-rewarding may diminish after several\niterations in certain situations, which we hypothesize is due to accumulated\nbias in the reward system. This bias can lead to unreliable preference data for\ntraining the LLM. To address this issue, we first formulate and analyze the\ngeneralized iterative preference fine-tuning framework for self-rewarding\nlanguage model. We then introduce the regularization to this generalized\nframework to mitigate the overconfident preference labeling in the\nself-rewarding process. Based on this theoretical insight, we propose a\nConsistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages\nthe rewarding consistency across different iterations to regularize the\nself-rewarding training, helping the model to learn from more reliable\npreference data. With this explicit regularization, our empirical results\ndemonstrate the superiority of CREAM in improving both reward consistency and\nalignment performance. The code is publicly available at\nhttps://github.com/Raibows/CREAM.\n","authors":["Zhaoyang Wang","Weilei He","Zhiyuan Liang","Xuchao Zhang","Chetan Bansal","Ying Wei","Weitong Zhang","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2410.12735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12722v1","updated":"2024-10-16T16:31:24Z","published":"2024-10-16T16:31:24Z","title":"WorldMedQA-V: a multilingual, multimodal medical examination dataset for\n  multimodal language models evaluation","summary":"  Multimodal/vision language models (VLMs) are increasingly being deployed in\nhealthcare settings worldwide, necessitating robust benchmarks to ensure their\nsafety, efficacy, and fairness. Multiple-choice question and answer (QA)\ndatasets derived from national medical examinations have long served as\nvaluable evaluation tools, but existing datasets are largely text-only and\navailable in a limited subset of languages and countries. To address these\nchallenges, we present WorldMedQA-V, an updated multilingual, multimodal\nbenchmarking dataset designed to evaluate VLMs in healthcare. WorldMedQA-V\nincludes 568 labeled multiple-choice QAs paired with 568 medical images from\nfour countries (Brazil, Israel, Japan, and Spain), covering original languages\nand validated English translations by native clinicians, respectively. Baseline\nperformance for common open- and closed-source models are provided in the local\nlanguage and English translations, and with and without images provided to the\nmodel. The WorldMedQA-V benchmark aims to better match AI systems to the\ndiverse healthcare environments in which they are deployed, fostering more\nequitable, effective, and representative applications.\n","authors":["João Matos","Shan Chen","Siena Placino","Yingya Li","Juan Carlos Climent Pardo","Daphna Idan","Takeshi Tohyama","David Restrepo","Luis F. Nakayama","Jose M. M. Pascual-Leone","Guergana Savova","Hugo Aerts","Leo A. Celi","A. Ian Wong","Danielle S. Bitterman","Jack Gallifant"],"pdf_url":"https://arxiv.org/pdf/2410.12722v1.pdf","comment":"submitted for review, total of 14 pages"},{"id":"http://arxiv.org/abs/2407.00463v5","updated":"2024-10-16T16:13:32Z","published":"2024-06-29T15:20:11Z","title":"Open-Source Conversational AI with SpeechBrain 1.0","summary":"  SpeechBrain is an open-source Conversational AI toolkit based on PyTorch,\nfocused particularly on speech processing tasks such as speech recognition,\nspeech enhancement, speaker recognition, text-to-speech, and much more. It\npromotes transparency and replicability by releasing both the pre-trained\nmodels and the complete \"recipes\" of code and algorithms required for training\nthem. This paper presents SpeechBrain 1.0, a significant milestone in the\nevolution of the toolkit, which now has over 200 recipes for speech, audio, and\nlanguage processing tasks, and more than 100 models available on Hugging Face.\nSpeechBrain 1.0 introduces new technologies to support diverse learning\nmodalities, Large Language Model (LLM) integration, and advanced decoding\nstrategies, along with novel models, tasks, and modalities. It also includes a\nnew benchmark repository, offering researchers a unified platform for\nevaluating models across diverse tasks.\n","authors":["Mirco Ravanelli","Titouan Parcollet","Adel Moumen","Sylvain de Langen","Cem Subakan","Peter Plantinga","Yingzhi Wang","Pooneh Mousavi","Luca Della Libera","Artem Ploujnikov","Francesco Paissan","Davide Borra","Salah Zaiem","Zeyu Zhao","Shucong Zhang","Georgios Karakasidis","Sung-Lin Yeh","Pierre Champion","Aku Rouhe","Rudolf Braun","Florian Mai","Juan Zuluaga-Gomez","Seyed Mahed Mousavi","Andreas Nautsch","Xuechen Liu","Sangeet Sagar","Jarod Duret","Salima Mdhaffar","Gaelle Laperriere","Mickael Rouvier","Renato De Mori","Yannick Esteve"],"pdf_url":"https://arxiv.org/pdf/2407.00463v5.pdf","comment":"Accepted to the Journal of Machine Learning research (JMLR), Machine\n  Learning Open Source Software"},{"id":"http://arxiv.org/abs/2410.12705v1","updated":"2024-10-16T16:11:49Z","published":"2024-10-16T16:11:49Z","title":"WorldCuisines: A Massive-Scale Benchmark for Multilingual and\n  Multicultural Visual Question Answering on Global Cuisines","summary":"  Vision Language Models (VLMs) often struggle with culture-specific knowledge,\nparticularly in languages other than English and in underrepresented cultural\ncontexts. To evaluate their understanding of such knowledge, we introduce\nWorldCuisines, a massive-scale benchmark for multilingual and multicultural,\nvisually grounded language understanding. This benchmark includes a visual\nquestion answering (VQA) dataset with text-image pairs across 30 languages and\ndialects, spanning 9 language families and featuring over 1 million data\npoints, making it the largest multicultural VQA benchmark to date. It includes\ntasks for identifying dish names and their origins. We provide evaluation\ndatasets in two sizes (12k and 60k instances) alongside a training dataset (1\nmillion instances). Our findings show that while VLMs perform better with\ncorrect location context, they struggle with adversarial contexts and\npredicting specific regional cuisines and languages. To support future\nresearch, we release a knowledge base with annotated food entries and images\nalong with the VQA data.\n","authors":["Genta Indra Winata","Frederikus Hudi","Patrick Amadeus Irawan","David Anugraha","Rifki Afina Putri","Yutong Wang","Adam Nohejl","Ubaidillah Ariq Prathama","Nedjma Ousidhoum","Afifa Amriani","Anar Rzayev","Anirban Das","Ashmari Pramodya","Aulia Adila","Bryan Wilie","Candy Olivia Mawalim","Ching Lam Cheng","Daud Abolade","Emmanuele Chersoni","Enrico Santus","Fariz Ikhwantri","Garry Kuwanto","Hanyang Zhao","Haryo Akbarianto Wibowo","Holy Lovenia","Jan Christian Blaise Cruz","Jan Wira Gotama Putra","Junho Myung","Lucky Susanto","Maria Angelica Riera Machin","Marina Zhukova","Michael Anugraha","Muhammad Farid Adilazuarda","Natasha Santosa","Peerat Limkonchotiwat","Raj Dabre","Rio Alexander Audino","Samuel Cahyawijaya","Shi-Xiong Zhang","Stephanie Yulia Salim","Yi Zhou","Yinxuan Gui","David Ifeoluwa Adelani","En-Shiun Annie Lee","Shogo Okada","Ayu Purwarianti","Alham Fikri Aji","Taro Watanabe","Derry Tanti Wijaya","Alice Oh","Chong-Wah Ngo"],"pdf_url":"https://arxiv.org/pdf/2410.12705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12704v1","updated":"2024-10-16T16:10:59Z","published":"2024-10-16T16:10:59Z","title":"Sarcasm Detection in a Less-Resourced Language","summary":"  The sarcasm detection task in natural language processing tries to classify\nwhether an utterance is sarcastic or not. It is related to sentiment analysis\nsince it often inverts surface sentiment. Because sarcastic sentences are\nhighly dependent on context, and they are often accompanied by various\nnon-verbal cues, the task is challenging. Most of related work focuses on\nhigh-resourced languages like English. To build a sarcasm detection dataset for\na less-resourced language, such as Slovenian, we leverage two modern\ntechniques: a machine translation specific medium-size transformer model, and a\nvery large generative language model. We explore the viability of translated\ndatasets and how the size of a pretrained transformer affects its ability to\ndetect sarcasm. We train ensembles of detection models and evaluate models'\nperformance. The results show that larger models generally outperform smaller\nones and that ensembling can slightly improve sarcasm detection performance.\nOur best ensemble approach achieves an $\\text{F}_1$-score of 0.765 which is\nclose to annotators' agreement in the source language.\n","authors":["Lazar Đoković","Marko Robnik-Šikonja"],"pdf_url":"https://arxiv.org/pdf/2410.12704v1.pdf","comment":"4 pages, published in the Slovenian Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2404.15219v2","updated":"2024-10-16T16:01:59Z","published":"2024-04-23T16:51:26Z","title":"Unsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of\n  the Noisy Channel","summary":"  Training task-oriented dialogue systems typically requires turn-level\nannotations for interacting with their APIs: e.g. a dialogue state and the\nsystem actions taken at each step. These annotations can be costly to produce,\nerror-prone, and require both domain and annotation expertise. With advances in\nLLMs, we hypothesize that unlabeled data and a schema definition are sufficient\nfor building a working task-oriented dialogue system, completely unsupervised.\nWe consider a novel unsupervised setting of only (1) a well-defined API schema\n(2) a set of unlabeled dialogues between a user and agent. We propose an\ninnovative approach using expectation-maximization (EM) that infers turn-level\nannotations as latent variables using a noisy channel model to build an\nend-to-end dialogue agent. Evaluating our approach on the MultiWOZ benchmark,\nour method more than doubles the dialogue success rate of a strong GPT-3.5\nbaseline.\n","authors":["Brendan King","Jeffrey Flanigan"],"pdf_url":"https://arxiv.org/pdf/2404.15219v2.pdf","comment":"To be presented at Empirical Methods in Natural Language Processing\n  (EMNLP 2024). 18 Pages, 8 Figures"},{"id":"http://arxiv.org/abs/2410.12694v1","updated":"2024-10-16T15:54:11Z","published":"2024-10-16T15:54:11Z","title":"VividMed: Vision Language Model with Versatile Visual Grounding for\n  Medicine","summary":"  Recent advancements in Vision Language Models (VLMs) have demonstrated\nremarkable promise in generating visually grounded responses. However, their\napplication in the medical domain is hindered by unique challenges. For\ninstance, most VLMs rely on a single method of visual grounding, whereas\ncomplex medical tasks demand more versatile approaches. Additionally, while\nmost VLMs process only 2D images, a large portion of medical images are 3D. The\nlack of medical data further compounds these obstacles. To address these\nchallenges, we present VividMed, a vision language model with versatile visual\ngrounding for medicine. Our model supports generating both semantic\nsegmentation masks and instance-level bounding boxes, and accommodates various\nimaging modalities, including both 2D and 3D data. We design a three-stage\ntraining procedure and an automatic data synthesis pipeline based on open\ndatasets and models. Besides visual grounding tasks, VividMed also excels in\nother common downstream tasks, including Visual Question Answering (VQA) and\nreport generation. Ablation studies empirically show that the integration of\nvisual grounding ability leads to improved performance on these tasks. Our code\nis publicly available at https://github.com/function2-llx/MMMM.\n","authors":["Lingxiao Luo","Bingda Tang","Xuanzhong Chen","Rong Han","Ting Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12691v1","updated":"2024-10-16T15:51:18Z","published":"2024-10-16T15:51:18Z","title":"Building Better: Avoiding Pitfalls in Developing Language Resources when\n  Data is Scarce","summary":"  Language is a symbolic capital that affects people's lives in many ways\n(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,\ncultures, traditions, and societies in general. Hence, data in a given language\nshould be viewed as more than a collection of tokens. Good data collection and\nlabeling practices are key to building more human-centered and socially aware\ntechnologies. While there has been a rising interest in mid- to low-resource\nlanguages within the NLP community, work in this space has to overcome unique\nchallenges such as data scarcity and access to suitable annotators. In this\npaper, we collect feedback from those directly involved in and impacted by NLP\nartefacts for mid- to low-resource languages. We conduct a quantitative and\nqualitative analysis of the responses and highlight the main issues related to\n(1) data quality such as linguistic and cultural data suitability; and (2) the\nethics of common annotation practices such as the misuse of online community\nservices. Based on these findings, we make several recommendations for the\ncreation of high-quality language artefacts that reflect the cultural milieu of\nits speakers, while simultaneously respecting the dignity and labor of data\nworkers.\n","authors":["Nedjma Ousidhoum","Meriem Beloucif","Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2410.12691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01474v2","updated":"2024-10-16T15:45:35Z","published":"2024-05-02T17:07:25Z","title":"Understanding Figurative Meaning through Explainable Visual Entailment","summary":"  Large Vision-Language Models (VLMs) have demonstrated strong capabilities in\ntasks requiring a fine-grained understanding of literal meaning in images and\ntext, such as visual question-answering or visual entailment. However, there\nhas been little exploration of these models' capabilities when presented with\nimages and captions containing figurative meaning, such as metaphors or humor.\nTo close this gap, we propose a new task framing the figurative meaning\nunderstanding problem as an explainable visual entailment task, where the model\nhas to predict whether the image (premise) entails a caption (hypothesis) and\njustify the predicted label with a textual explanation. The figurative\nphenomena can be present either in the image, the caption, or both. Utilizing a\nhuman-AI collaboration approach, we build the accompanying expert-verified\ndataset V-FLUTE, containing 6,027 {image, caption, label, explanation}\ninstances spanning five diverse figurative phenomena: metaphors, similes,\nidioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs\nstruggle to generalize from literal to figurative meaning, particularly when it\nis present in images. Further, we identify common types of errors in VLM\nreasoning via human evaluation.\n","authors":["Arkadiy Saakyan","Shreyas Kulkarni","Tuhin Chakrabarty","Smaranda Muresan"],"pdf_url":"https://arxiv.org/pdf/2405.01474v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11167v2","updated":"2024-10-16T15:40:51Z","published":"2024-02-17T02:25:57Z","title":"ToBlend: Token-Level Blending With an Ensemble of LLMs to Attack\n  AI-Generated Text Detection","summary":"  The robustness of AI-content detection models against sophisticated\nadversarial strategies, such as paraphrasing or word switching, is a rising\nconcern in natural language generation (NLG) applications. This study proposes\nToBlend, a novel token-level ensemble text generation method to challenge the\nrobustness of current AI-content detection approaches by utilizing multiple\nsets of candidate generative large language models (LLMs). By randomly sampling\ntoken(s) from candidate LLMs sets, we find ToBlend significantly drops the\nperformance of most mainstream AI-content detection methods. We evaluate the\ntext quality produced under different ToBlend settings based on annotations\nfrom experienced human experts. We proposed a fine-tuned Llama3.1 model to\ndistinguish the ToBlend generated text more accurately. Our findings underscore\nour proposed text generation approach's great potential in deceiving and\nimproving detection models. Our datasets, codes, and annotations are\nopen-sourced.\n","authors":["Fan Huang","Haewoon Kwak","Jisun An"],"pdf_url":"https://arxiv.org/pdf/2402.11167v2.pdf","comment":"Submitted to ARR Oct-2024 Cycle"},{"id":"http://arxiv.org/abs/2402.07204v4","updated":"2024-10-16T15:28:18Z","published":"2024-02-11T13:30:53Z","title":"ITINERA: Integrating Spatial Optimization with Large Language Models for\n  Open-domain Urban Itinerary Planning","summary":"  Citywalk, a recently popular form of urban travel, requires genuine\npersonalization and understanding of fine-grained requests compared to\ntraditional itinerary planning. In this paper, we introduce the novel task of\nOpen-domain Urban Itinerary Planning (OUIP), which generates personalized urban\nitineraries from user requests in natural language. We then present ITINERA, an\nOUIP system that integrates spatial optimization with large language models to\nprovide customized urban itineraries based on user needs. This involves\ndecomposing user requests, selecting candidate points of interest (POIs),\nordering the POIs based on cluster-aware spatial optimization, and generating\nthe itinerary. Experiments on real-world datasets and the performance of the\ndeployed system demonstrate our system's capacity to deliver personalized and\nspatially coherent itineraries compared to current solutions. Source codes of\nITINERA are available at https://github.com/YihongT/ITINERA.\n","authors":["Yihong Tang","Zhaokai Wang","Ao Qu","Yihao Yan","Zhaofeng Wu","Dingyi Zhuang","Jushi Kai","Kebing Hou","Xiaotong Guo","Jinhua Zhao","Zhan Zhao","Wei Ma"],"pdf_url":"https://arxiv.org/pdf/2402.07204v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12662v1","updated":"2024-10-16T15:20:08Z","published":"2024-10-16T15:20:08Z","title":"Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models","summary":"  Vision-language alignment in Large Vision-Language Models (LVLMs)\nsuccessfully enables LLMs to understand visual input. However, we find that\nexisting vision-language alignment methods fail to transfer the existing safety\nmechanism for text in LLMs to vision, which leads to vulnerabilities in toxic\nimage. To explore the cause of this problem, we give the insightful explanation\nof where and how the safety mechanism of LVLMs operates and conduct comparative\nanalysis between text and vision. We find that the hidden states at the\nspecific transformer layers play a crucial role in the successful activation of\nsafety mechanism, while the vision-language alignment at hidden states level in\ncurrent methods is insufficient. This results in a semantic shift for input\nimages compared to text in hidden states, therefore misleads the safety\nmechanism. To address this, we propose a novel Text-Guided vision-language\nAlignment method (TGA) for LVLMs. TGA retrieves the texts related to input\nvision and uses them to guide the projection of vision into the hidden states\nspace in LLMs. Experiments show that TGA not only successfully transfers the\nsafety mechanism for text in basic LLMs to vision in vision-language alignment\nfor LVLMs without any safety fine-tuning on the visual modality but also\nmaintains the general performance on various vision tasks (Safe and Good).\n","authors":["Shicheng Xu","Liang Pang","Yunchang Zhu","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.12662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12656v1","updated":"2024-10-16T15:17:20Z","published":"2024-10-16T15:17:20Z","title":"Evaluating Morphological Compositional Generalization in Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated significant progress in\nvarious natural language generation and understanding tasks. However, their\nlinguistic generalization capabilities remain questionable, raising doubts\nabout whether these models learn language similarly to humans. While humans\nexhibit compositional generalization and linguistic creativity in language use,\nthe extent to which LLMs replicate these abilities, particularly in morphology,\nis under-explored. In this work, we systematically investigate the\nmorphological generalization abilities of LLMs through the lens of\ncompositionality. We define morphemes as compositional primitives and design a\nnovel suite of generative and discriminative tasks to assess morphological\nproductivity and systematicity. Focusing on agglutinative languages such as\nTurkish and Finnish, we evaluate several state-of-the-art instruction-finetuned\nmultilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs\nstruggle with morphological compositional generalization particularly when\napplied to novel word roots, with performance declining sharply as\nmorphological complexity increases. While models can identify individual\nmorphological combinations better than chance, their performance lacks\nsystematicity, leading to significant accuracy gaps compared to humans.\n","authors":["Mete Ismayilzada","Defne Circi","Jonne Sälevä","Hale Sirin","Abdullatif Köksal","Bhuwan Dhingra","Antoine Bosselut","Lonneke van der Plas","Duygu Ataman"],"pdf_url":"https://arxiv.org/pdf/2410.12656v1.pdf","comment":"33 pages"},{"id":"http://arxiv.org/abs/2406.11785v2","updated":"2024-10-16T15:15:44Z","published":"2024-06-17T17:39:10Z","title":"CELL your Model: Contrastive Explanations for Large Language Models","summary":"  The advent of black-box deep neural network classification models has sparked\nthe need to explain their decisions. However, in the case of generative AI,\nsuch as large language models (LLMs), there is no class prediction to explain.\nRather, one can ask why an LLM output a particular response to a given prompt.\nIn this paper, we answer this question by proposing, to the best of our\nknowledge, the first contrastive explanation methods requiring simply\nblack-box/query access. Our explanations suggest that an LLM outputs a reply to\na given prompt because if the prompt was slightly modified, the LLM would have\ngiven a different response that is either less preferable or contradicts the\noriginal response. The key insight is that contrastive explanations simply\nrequire a scoring function that has meaning to the user and not necessarily a\nspecific real valued quantity (viz. class label). We offer two algorithms for\nfinding contrastive explanations: i) A myopic algorithm, which although\neffective in creating contrasts, requires many model calls and ii) A budgeted\nalgorithm, our main algorithmic contribution, which intelligently creates\ncontrasts adhering to a query budget, necessary for longer contexts. We show\nthe efficacy of these methods on diverse natural language tasks such as\nopen-text generation, automated red teaming, and explaining conversational\ndegradation.\n","authors":["Ronny Luss","Erik Miehling","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2406.11785v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20366v4","updated":"2024-10-16T15:09:14Z","published":"2024-09-30T15:04:17Z","title":"Disentangling Singlish Discourse Particles with Task-Driven\n  Representation","summary":"  Singlish, or formally Colloquial Singapore English, is an English-based\ncreole language originating from the SouthEast Asian country Singapore. The\nlanguage contains influences from Sinitic languages such as Chinese dialects,\nMalay, Tamil and so forth. A fundamental task to understanding Singlish is to\nfirst understand the pragmatic functions of its discourse particles, upon which\nSinglish relies heavily to convey meaning. This work offers a preliminary\neffort to disentangle the Singlish discourse particles (lah, meh and hor) with\ntask-driven representation learning. After disentanglement, we cluster these\ndiscourse particles to differentiate their pragmatic functions, and perform\nSinglish-to-English machine translation. Our work provides a computational\nmethod to understanding Singlish discourse particles, and opens avenues towards\na deeper comprehension of the language and its usage.\n","authors":["Linus Tze En Foo","Lynnette Hui Xian Ng"],"pdf_url":"https://arxiv.org/pdf/2409.20366v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13745v4","updated":"2024-10-16T15:07:41Z","published":"2024-08-25T07:10:36Z","title":"DOCE: Finding the Sweet Spot for Execution-Based Code Generation","summary":"  Recently, a diverse set of decoding and reranking procedures have been shown\neffective for LLM-based code generation. However, a comprehensive framework\nthat links and experimentally compares these methods is missing. We address\nthis by proposing Decoding Objectives for Code Execution, a comprehensive\nframework that includes candidate generation, $n$-best reranking, minimum Bayes\nrisk (MBR) decoding, and self-debugging as the core components. We then study\nthe contributions of these components through execution-based evaluation\nmetrics. Our findings highlight the importance of execution-based methods and\nthe difference gap between execution-based and execution-free methods.\nFurthermore, we assess the impact of filtering based on trial unit tests, a\nsimple and effective strategy that has been often overlooked in prior works. We\nalso propose self-debugging on multiple candidates, obtaining state-of-the-art\nperformance on reranking for code generation. We expect our framework to\nprovide a solid guideline for future research on code generation.\n","authors":["Haau-Sing Li","Patrick Fernandes","Iryna Gurevych","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2408.13745v4.pdf","comment":"10 pages (32 including appendix), 5 figures, 25 tables. Prompts are\n  provided in the GitHub repository to avoid potential text overlap with other\n  papers"},{"id":"http://arxiv.org/abs/2409.13832v3","updated":"2024-10-16T14:56:59Z","published":"2024-09-20T18:18:14Z","title":"GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music\n  Scores for All Singing Tasks","summary":"  The scarcity of high-quality and multi-task singing datasets significantly\nhinders the development of diverse controllable and personalized singing tasks,\nas existing singing datasets suffer from low quality, limited diversity of\nlanguages and singers, absence of multi-technique information and realistic\nmusic scores, and poor task suitability. To tackle these problems, we present\nGTSinger, a large global, multi-technique, free-to-use, high-quality singing\ncorpus with realistic music scores, designed for all singing tasks, along with\nits benchmarks. Particularly, (1) we collect 80.59 hours of high-quality\nsinging voices, forming the largest recorded singing dataset; (2) 20\nprofessional singers across nine widely spoken languages offer diverse timbres\nand styles; (3) we provide controlled comparison and phoneme-level annotations\nof six commonly used singing techniques, helping technique modeling and\ncontrol; (4) GTSinger offers realistic music scores, assisting real-world\nmusical composition; (5) singing voices are accompanied by manual\nphoneme-to-audio alignments, global style labels, and 16.16 hours of paired\nspeech for various singing tasks. Moreover, to facilitate the use of GTSinger,\nwe conduct four benchmark experiments: technique-controllable singing voice\nsynthesis, technique recognition, style transfer, and speech-to-singing\nconversion. The corpus and demos can be found at http://gtsinger.github.io. We\nprovide the dataset and the code for processing data and conducting benchmarks\nat https://huggingface.co/datasets/GTSinger/GTSinger and\nhttps://github.com/GTSinger/GTSinger.\n","authors":["Yu Zhang","Changhao Pan","Wenxiang Guo","Ruiqi Li","Zhiyuan Zhu","Jialei Wang","Wenhao Xu","Jingyu Lu","Zhiqing Hong","Chuxin Wang","LiChao Zhang","Jinzheng He","Ziyue Jiang","Yuxin Chen","Chen Yang","Jiecheng Zhou","Xinyu Cheng","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.13832v3.pdf","comment":"Accepted by NeurIPS 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2409.15360v3","updated":"2024-10-16T14:56:15Z","published":"2024-09-18T02:35:41Z","title":"Reward-Robust RLHF in LLMs","summary":"  As Large Language Models (LLMs) continue to progress toward more advanced\nforms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is\nincreasingly seen as a key pathway toward achieving Artificial General\nIntelligence (AGI). However, the reliance on reward-model-based (RM-based)\nalignment methods introduces significant challenges due to the inherent\ninstability and imperfections of Reward Models (RMs), which can lead to\ncritical issues such as reward hacking and misalignment with human intentions.\nIn this paper, we introduce a reward-robust RLHF framework aimed at addressing\nthese fundamental challenges, paving the way for more reliable and resilient\nlearning in LLMs. Our approach introduces a novel optimization objective that\ncarefully balances performance and robustness by incorporating Bayesian Reward\nModel Ensembles (BRME) to model the uncertainty set of reward functions. This\nallows the framework to integrate both nominal performance and minimum reward\nsignals, ensuring more stable learning even with imperfect RMs. Empirical\nresults demonstrate that our framework consistently outperforms baselines\nacross diverse benchmarks, showing improved accuracy and long-term stability.\nWe also provide a theoretical analysis, demonstrating that reward-robust RLHF\napproaches the stability of constant reward settings, which proves to be\nacceptable even in a stochastic-case analysis. Together, these contributions\nhighlight the framework potential to enhance both the performance and stability\nof LLM alignment.\n","authors":["Yuzi Yan","Xingzhou Lou","Jialian Li","Yiping Zhang","Jian Xie","Chao Yu","Yu Wang","Dong Yan","Yuan Shen"],"pdf_url":"https://arxiv.org/pdf/2409.15360v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13300v2","updated":"2024-10-16T14:52:16Z","published":"2024-07-18T09:05:49Z","title":"Robust ASR Error Correction with Conservative Data Filtering","summary":"  Error correction (EC) based on large language models is an emerging\ntechnology to enhance the performance of automatic speech recognition (ASR)\nsystems. Generally, training data for EC are collected by automatically pairing\na large set of ASR hypotheses (as sources) and their gold references (as\ntargets). However, the quality of such pairs is not guaranteed, and we observed\nvarious types of noise which can make the EC models brittle, e.g. inducing\novercorrection in out-of-domain (OOD) settings. In this work, we propose two\nfundamental criteria that EC training data should satisfy: namely, EC targets\nshould (1) improve linguistic acceptability over sources and (2) be inferable\nfrom the available context (e.g. source phonemes). Through these criteria, we\nidentify low-quality EC pairs and train the models not to make any correction\nin such cases, the process we refer to as conservative data filtering. In our\nexperiments, we focus on Japanese ASR using a strong Conformer-CTC as the\nbaseline and finetune Japanese LLMs for EC. Through our evaluation on a suite\nof 21 internal benchmarks, we demonstrate that our approach can significantly\nreduce overcorrection and improve both the accuracy and quality of ASR results\nin the challenging OOD settings.\n","authors":["Takuma Udagawa","Masayuki Suzuki","Masayasu Muraoka","Gakuto Kurata"],"pdf_url":"https://arxiv.org/pdf/2407.13300v2.pdf","comment":"Accepted to EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2406.09988v2","updated":"2024-10-16T14:48:38Z","published":"2024-06-14T12:52:42Z","title":"Details Make a Difference: Object State-Sensitive Neurorobotic Task\n  Planning","summary":"  The state of an object reflects its current status or condition and is\nimportant for a robot's task planning and manipulation. However, detecting an\nobject's state and generating a state-sensitive plan for robots is challenging.\nRecently, pre-trained Large Language Models (LLMs) and Vision-Language Models\n(VLMs) have shown impressive capabilities in generating plans. However, to the\nbest of our knowledge, there is hardly any investigation on whether LLMs or\nVLMs can also generate object state-sensitive plans. To study this, we\nintroduce an Object State-Sensitive Agent (OSSA), a task-planning agent\nempowered by pre-trained neural networks. We propose two methods for OSSA: (i)\na modular model consisting of a pre-trained vision processing module (dense\ncaptioning model, DCM) and a natural language processing model (LLM), and (ii)\na monolithic model consisting only of a VLM. To quantitatively evaluate the\nperformances of the two methods, we use tabletop scenarios where the task is to\nclear the table. We contribute a multimodal benchmark dataset that takes object\nstates into consideration. Our results show that both methods can be used for\nobject state-sensitive tasks, but the monolithic approach outperforms the\nmodular approach. The code for OSSA is available at\nhttps://github.com/Xiao-wen-Sun/OSSA\n","authors":["Xiaowen Sun","Xufeng Zhao","Jae Hee Lee","Wenhao Lu","Matthias Kerzel","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2406.09988v2.pdf","comment":"ICANN24, Switzerland"},{"id":"http://arxiv.org/abs/2410.12622v1","updated":"2024-10-16T14:42:23Z","published":"2024-10-16T14:42:23Z","title":"From Measurement Instruments to Training Data: Leveraging Theory-Driven\n  Synthetic Training Data for Measuring Social Constructs","summary":"  Computational text classification is a challenging task, especially for\nmulti-dimensional social constructs. Recently, there has been increasing\ndiscussion that synthetic training data could enhance classification by\noffering examples of how these constructs are represented in texts. In this\npaper, we systematically examine the potential of theory-driven synthetic\ntraining data for improving the measurement of social constructs. In\nparticular, we explore how researchers can transfer established knowledge from\nmeasurement instruments in the social sciences, such as survey scales or\nannotation codebooks, into theory-driven generation of synthetic data. Using\ntwo studies on measuring sexism and political topics, we assess the added value\nof synthetic training data for fine-tuning text classification models. Although\nthe results of the sexism study were less promising, our findings demonstrate\nthat synthetic data can be highly effective in reducing the need for labeled\ndata in political topic classification. With only a minimal drop in\nperformance, synthetic data allows for substituting large amounts of labeled\ndata. Furthermore, theory-driven synthetic data performed markedly better than\ndata generated without conceptual information in mind.\n","authors":["Lukas Birkenmaier","Matthias Roth","Indira Sen"],"pdf_url":"https://arxiv.org/pdf/2410.12622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12621v1","updated":"2024-10-16T14:40:32Z","published":"2024-10-16T14:40:32Z","title":"Weak-to-Strong Generalization beyond Accuracy: a Pilot Study in Safety,\n  Toxicity, and Legal Reasoning","summary":"  As large language models (LLMs) continue to advance, ensuring their alignment\nwith human values becomes increasingly critical. Traditional alignment methods\nheavily rely on human feedback to fine-tune models. With the emergence of\nsuperhuman models whose outputs may surpass human understanding, evaluating and\naligning these models using human judgments poses significant challenges. To\naddress the challenges, recent works use weak supervisors to elicit knowledge\nfrom much stronger models. However, there are important disanalogies between\nthe empirical setup in the existing works and the genuine goal of alignment. We\nremark that existing works investigate the phenomenon of weak-to-strong\ngeneration in analogous setup (i.e., binary classification), rather than\npractical alignment-relevant tasks (e.g., safety). In this paper, we bridge\nthis gap by extending weak-to-strong generation to the context of practical\nalignment. We empirically demonstrate the widespread phenomenon of\nweak-to-strong generation in three complicated alignment tasks: safety,\ntoxicity, and legal reasoning}. Furthermore, we explore efficient strategies\nfor improving alignment performance to enhance the quality of model outcomes.\nLastly, we summarize and analyze the challenges and potential solutions in\nregard to specific alignment tasks, which we hope to catalyze the research\nprogress on the topic of weak-to-strong generalization. Our code is released at\nhttps://github.com/yeruimeng/WTS.git.\n","authors":["Ruimeng Ye","Yang Xiao","Bo Hui"],"pdf_url":"https://arxiv.org/pdf/2410.12621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12617v1","updated":"2024-10-16T14:34:30Z","published":"2024-10-16T14:34:30Z","title":"Parsing Akkadian Verbs with Prolog","summary":"  This paper describes a parsing/generation system for finite verbal forms in\nAkkadian, with the possible addition of suffixes, implemented in Prolog. The\nwork described provides the framework and engine to interpret the D, N, and G\nstems along with accusative, dative and ventive endings.\n","authors":["Aaron Macks"],"pdf_url":"https://arxiv.org/pdf/2410.12617v1.pdf","comment":"6 pages, 9 figures, presented at ACL-02 the Association of\n  Computational Linguistics, 2002"},{"id":"http://arxiv.org/abs/2410.12613v1","updated":"2024-10-16T14:29:29Z","published":"2024-10-16T14:29:29Z","title":"Exploring Model Kinship for Merging Large Language Models","summary":"  Model merging has become one of the key technologies for enhancing the\ncapabilities and efficiency of Large Language Models (LLMs). However, our\nunderstanding of the expected performance gains and principles when merging any\ntwo models remains limited. In this work, we introduce model kinship, the\ndegree of similarity or relatedness between LLMs, analogous to biological\nevolution. With comprehensive empirical analysis, we find that there is a\ncertain relationship between model kinship and the performance gains after\nmodel merging, which can help guide our selection of candidate models. Inspired\nby this, we propose a new model merging strategy: Top-k Greedy Merging with\nModel Kinship, which can yield better performance on benchmark datasets.\nSpecifically, we discover that using model kinship as a criterion can assist us\nin continuously performing model merging, alleviating the degradation (local\noptima) in model evolution, whereas model kinship can serve as a guide to\nescape these traps. Code is available at\nhttps://github.com/zjunlp/ModelKinship.\n","authors":["Yedi Hu","Yunzhi Yao","Ningyu Zhang","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12613v1.pdf","comment":"Ongoing work"},{"id":"http://arxiv.org/abs/2305.14463v4","updated":"2024-10-16T14:27:49Z","published":"2023-05-23T18:37:30Z","title":"ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain\n  Readability Assessment","summary":"  We present a comprehensive evaluation of large language models for\nmultilingual readability assessment. Existing evaluation resources lack domain\nand language diversity, limiting the ability for cross-domain and cross-lingual\nanalyses. This paper introduces ReadMe++, a multilingual multi-domain dataset\nwith human annotations of 9757 sentences in Arabic, English, French, Hindi, and\nRussian, collected from 112 different data sources. This benchmark will\nencourage research on developing robust multilingual readability assessment\nmethods. Using ReadMe++, we benchmark multilingual and monolingual language\nmodels in the supervised, unsupervised, and few-shot prompting settings. The\ndomain and language diversity in ReadMe++ enable us to test more effective\nfew-shot prompting, and identify shortcomings in state-of-the-art unsupervised\nmethods. Our experiments also reveal exciting results of superior domain\ngeneralization and enhanced cross-lingual transfer capabilities by models\ntrained on ReadMe++. We will make our data publicly available and release a\npython package tool for multilingual sentence readability prediction using our\ntrained models at: https://github.com/tareknaous/readme\n","authors":["Tarek Naous","Michael J. Ryan","Anton Lavrouk","Mohit Chandra","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2305.14463v4.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.12608v1","updated":"2024-10-16T14:24:55Z","published":"2024-10-16T14:24:55Z","title":"Not All Votes Count! Programs as Verifiers Improve Self-Consistency of\n  Language Models for Math Reasoning","summary":"  Large language models (LLMs) have shown increasing proficiency in solving\nmathematical reasoning problems. However, many current open-source LLMs often\nstill make calculation and semantic understanding errors in their intermediate\nreasoning steps. In this work, we propose PROVE, a simple yet effective\nframework that uses program-based verification as a heuristic to filter out\npotentially incorrect reasoning paths before aggregating the final answers.\nInstead of relying on vanilla majority voting, our approach rejects solutions\nwhose corresponding program outputs are inconsistent with the generated\nsolution, aggregating only those validated by Python programs. We conducted\nextensive experiments on 13 open-source LLMs from various model families and\nsizes, ranging from 0.5B to 13B parameters, across seven math benchmarks. We\ndemonstrate that PROVE consistently outperforms vanilla majority voting as a\nheuristic for solving mathematical reasoning tasks across all datasets and\nmodel sizes. Notably, PROVE increases accuracy on the GSM8K benchmark from\n48.85% to 53.83% for Qwen2-0.5B-Instruct, from 65.66% to 73.01% for\nLlama-3.2-1B-Instruct, from 73.39% to 79.61% for Gemma-2-2b-it, and from 41.32%\nto 59.51% for Llama-2-7B-chat. Our codes are available at\nhttps://github.com/declare-lab/prove.\n","authors":["Vernon Y. H. Toh","Deepanway Ghosal","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2410.12608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12601v1","updated":"2024-10-16T14:21:52Z","published":"2024-10-16T14:21:52Z","title":"CCSBench: Evaluating Compositional Controllability in LLMs for\n  Scientific Document Summarization","summary":"  To broaden the dissemination of scientific knowledge to diverse audiences,\nscientific document summarization must simultaneously control multiple\nattributes such as length and empirical focus. However, existing research\ntypically focuses on controlling single attributes, leaving the compositional\ncontrol of multiple attributes underexplored. To address this gap, we introduce\nCCSBench, a benchmark for compositional controllable summarization in the\nscientific domain. Our benchmark enables fine-grained control over both\nexplicit attributes (e.g., length), which are objective and straightforward,\nand implicit attributes (e.g., empirical focus), which are more subjective and\nconceptual. We conduct extensive experiments on GPT-4, LLaMA2, and other\npopular LLMs under various settings. Our findings reveal significant\nlimitations in large language models' ability to balance trade-offs between\ncontrol attributes, especially implicit ones that require deeper understanding\nand abstract reasoning.\n","authors":["Yixi Ding","Jiaying Wu","Tongyao Zhu","Yanxia Qin","Qian Liu","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2410.12601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12600v1","updated":"2024-10-16T14:17:53Z","published":"2024-10-16T14:17:53Z","title":"On the Risk of Evidence Pollution for Malicious Social Text Detection in\n  the Era of LLMs","summary":"  Evidence-enhanced detectors present remarkable abilities in identifying\nmalicious social text with related evidence. However, the rise of large\nlanguage models (LLMs) brings potential risks of evidence pollution to confuse\ndetectors. This paper explores how to manipulate evidence, simulating potential\nmisuse scenarios including basic pollution, and rephrasing or generating\nevidence by LLMs. To mitigate its negative impact, we propose three defense\nstrategies from both the data and model sides, including machine-generated text\ndetection, a mixture of experts, and parameter updating. Extensive experiments\non four malicious social text detection tasks with ten datasets present that\nevidence pollution, especially the generate strategy, significantly compromises\nexisting detectors. On the other hand, the defense strategies could mitigate\nevidence pollution, but they faced limitations for practical employment, such\nas the need for annotated data and huge inference costs. Further analysis\nillustrates that polluted evidence is of high quality, would compromise the\nmodel calibration, and could ensemble to amplify the negative impact.\n","authors":["Herun Wan","Minnan Luo","Zhixiong Su","Guang Dai","Xiang Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.12600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11894v4","updated":"2024-10-16T14:14:27Z","published":"2024-03-18T15:53:33Z","title":"From Explainable to Interpretable Deep Learning for Natural Language\n  Processing in Healthcare: How Far from Reality?","summary":"  Deep learning (DL) has substantially enhanced natural language processing\n(NLP) in healthcare research. However, the increasing complexity of DL-based\nNLP necessitates transparent model interpretability, or at least\nexplainability, for reliable decision-making. This work presents a thorough\nscoping review of explainable and interpretable DL in healthcare NLP. The term\n\"eXplainable and Interpretable Artificial Intelligence\" (XIAI) is introduced to\ndistinguish XAI from IAI. Different models are further categorized based on\ntheir functionality (model-, input-, output-based) and scope (local, global).\nOur analysis shows that attention mechanisms are the most prevalent emerging\nIAI technique. The use of IAI is growing, distinguishing it from XAI. The major\nchallenges identified are that most XIAI does not explore \"global\" modelling\nprocesses, the lack of best practices, and the lack of systematic evaluation\nand benchmarks. One important opportunity is to use attention mechanisms to\nenhance multi-modal XIAI for personalized medicine. Additionally, combining DL\nwith causal logic holds promise. Our discussion encourages the integration of\nXIAI in Large Language Models (LLMs) and domain-specific smaller models. In\nconclusion, XIAI adoption in healthcare requires dedicated in-house expertise.\nCollaboration with domain experts, end-users, and policymakers can lead to\nready-to-use XIAI methods across NLP and medical tasks. While challenges exist,\nXIAI techniques offer a valuable foundation for interpretable NLP algorithms in\nhealthcare.\n","authors":["Guangming Huang","Yingya Li","Shoaib Jameel","Yunfei Long","Giorgos Papanastasiou"],"pdf_url":"https://arxiv.org/pdf/2403.11894v4.pdf","comment":"This paper has been accepted by Computational and Structural\n  Biotechnology Journal"},{"id":"http://arxiv.org/abs/2402.19350v6","updated":"2024-10-16T14:12:47Z","published":"2024-02-29T16:56:36Z","title":"Prompting Explicit and Implicit Knowledge for Multi-hop Question\n  Answering Based on Human Reading Process","summary":"  Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to\nsimulate human reasoning and inference processes, achieving proficient\nperformance in multi-hop QA. However, a gap persists between PLMs' reasoning\nabilities and those of humans when tackling complex problems. Psychological\nstudies suggest a vital connection between explicit information in passages and\nhuman prior knowledge during reading. Nevertheless, current research has given\ninsufficient attention to linking input passages and PLMs' pre-training-based\nknowledge from the perspective of human cognition studies. In this study, we\nintroduce a Prompting Explicit and Implicit knowledge (PEI) framework, which\nuses prompts to connect explicit and implicit knowledge, aligning with human\nreading process for multi-hop QA. We consider the input passages as explicit\nknowledge, employing them to elicit implicit knowledge through unified prompt\nreasoning. Furthermore, our model incorporates type-specific reasoning via\nprompts, a form of implicit knowledge. Experimental results show that PEI\nperforms comparably to the state-of-the-art on HotpotQA. Ablation studies\nconfirm the efficacy of our model in bridging and integrating explicit and\nimplicit knowledge.\n","authors":["Guangming Huang","Yunfei Long","Cunjin Luo","Jiaxing Shen","Xia Sun"],"pdf_url":"https://arxiv.org/pdf/2402.19350v6.pdf","comment":"This paper has been accepted at COLING 2024"},{"id":"http://arxiv.org/abs/2410.03293v3","updated":"2024-10-16T14:11:21Z","published":"2024-10-04T10:06:55Z","title":"Five Years of COVID-19 Discourse on Instagram: A Labeled Instagram\n  Dataset of Over Half a Million Posts for Multilingual Sentiment Analysis","summary":"  The work presented in this paper makes three scientific contributions with a\nspecific focus on mining and analysis of COVID-19-related posts on Instagram.\nFirst, it presents a multilingual dataset of 500,153 Instagram posts about\nCOVID-19 published between January 2020 and September 2024. This dataset,\navailable at https://dx.doi.org/10.21227/d46p-v480, contains Instagram posts in\n161 different languages as well as 535,021 distinct hashtags. After the\ndevelopment of this dataset, multilingual sentiment analysis was performed,\nwhich involved classifying each post as positive, negative, or neutral. The\nresults of sentiment analysis are presented as a separate attribute in this\ndataset. Second, it presents the results of performing sentiment analysis per\nyear from 2020 to 2024. The findings revealed the trends in sentiment related\nto COVID-19 on Instagram since the beginning of the pandemic. For instance,\nbetween 2020 and 2024, the sentiment trends show a notable shift, with positive\nsentiment decreasing from 38.35% to 28.69%, while neutral sentiment rising from\n44.19% to 58.34%. Finally, the paper also presents findings of\nlanguage-specific sentiment analysis. This analysis highlighted similar and\ncontrasting trends of sentiment across posts published in different languages\non Instagram. For instance, out of all English posts, 49.68% were positive,\n14.84% were negative, and 35.48% were neutral. In contrast, among Hindi posts,\n4.40% were positive, 57.04% were negative, and 38.56% were neutral, reflecting\ndistinct differences in the sentiment distribution between these two languages.\n","authors":["Nirmalya Thakur"],"pdf_url":"https://arxiv.org/pdf/2410.03293v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08469v2","updated":"2024-10-16T14:09:14Z","published":"2024-10-11T02:42:13Z","title":"Semantic Token Reweighting for Interpretable and Controllable Text\n  Embeddings in CLIP","summary":"  A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial\nrole in translating textual input into an embedding space shared with images,\nthereby facilitating the interpretative analysis of vision tasks through\nnatural language. Despite the varying significance of different textual\nelements within a sentence depending on the context, efforts to account for\nvariation of importance in constructing text embeddings have been lacking. We\npropose a framework of Semantic Token Reweighting to build Interpretable text\nembeddings (SToRI), which incorporates controllability as well. SToRI refines\nthe text encoding process in CLIP by differentially weighting semantic elements\nbased on contextual importance, enabling finer control over emphasis responsive\nto data-driven insights and user preferences. The efficacy of SToRI is\ndemonstrated through comprehensive experiments on few-shot image classification\nand image retrieval tailored to user preferences.\n","authors":["Eunji Kim","Kyuhong Shim","Simyung Chang","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2410.08469v2.pdf","comment":"Accepted at EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.12586v1","updated":"2024-10-16T14:04:26Z","published":"2024-10-16T14:04:26Z","title":"Can We Reverse In-Context Knowledge Edits?","summary":"  In-context knowledge editing (IKE) enables efficient modification of large\nlanguage model (LLM) outputs without parameter changes and at zero-cost.\nHowever, it can be misused to manipulate responses opaquely, e.g., insert\nmisinformation or offensive content. Such malicious interventions could be\nincorporated into high-level wrapped APIs where the final input prompt is not\nshown to end-users. To address this issue, we investigate the detection and\nreversal of IKE-edits. First, we demonstrate that IKE-edits can be detected\nwith high accuracy (F1 > 80\\%) using only the top-10 output probabilities of\nthe next token, even in a black-box setting, e.g. proprietary LLMs with limited\noutput information. Further, we introduce the novel task of reversing IKE-edits\nusing specially tuned reversal tokens. We explore using both continuous and\ndiscrete reversal tokens, achieving over 80\\% accuracy in recovering original,\nunedited outputs across multiple LLMs. Our continuous reversal tokens prove\nparticularly effective, with minimal impact on unedited prompts. Through\nanalysis of output distributions, attention patterns, and token rankings, we\nprovide insights into IKE's effects on LLMs and how reversal tokens mitigate\nthem. This work represents a significant step towards enhancing LLM resilience\nagainst potential misuse of in-context editing, improving their transparency\nand trustworthiness.\n","authors":["Paul Youssef","Zhixue Zhao","Jörg Schlötterer","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2410.12586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12583v1","updated":"2024-10-16T14:01:22Z","published":"2024-10-16T14:01:22Z","title":"STRUX: An LLM for Decision-Making with Structured Explanations","summary":"  Countless decisions shape our daily lives, and it is paramount to understand\nthe how and why behind these choices. In this paper, we introduce a new LLM\ndecision-making framework called STRUX, which enhances LLM decision-making by\nproviding structured explanations. These include favorable and adverse facts\nrelated to the decision, along with their respective strengths. STRUX begins by\ndistilling lengthy information into a concise table of key facts. It then\nemploys a series of self-reflection steps to determine which of these facts are\npivotal, categorizing them as either favorable or adverse in relation to a\nspecific decision. Lastly, we fine-tune an LLM to identify and prioritize these\nkey facts to optimize decision-making. STRUX has been evaluated on the\nchallenging task of forecasting stock investment decisions based on earnings\ncall transcripts and demonstrated superior performance against strong\nbaselines. It enhances decision transparency by allowing users to understand\nthe impact of different factors, representing a meaningful step towards\npractical decision-making with LLMs.\n","authors":["Yiming Lu","Yebowen Hu","Hassan Foroosh","Wei Jin","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.12583v1.pdf","comment":"10 pages, 7 figures, submitted to NAACL 2025"},{"id":"http://arxiv.org/abs/2406.12593v2","updated":"2024-10-16T13:45:54Z","published":"2024-06-18T13:25:18Z","title":"PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval","summary":"  Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)\nfor efficient document retrieval without relying on external indexes. However,\nDSI needs full re-training to handle updates in dynamic corpora, causing\nsignificant computational inefficiencies. We introduce PromptDSI, a\nprompt-based rehearsal-free approach for instance-wise incremental learning\ndocument retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of\nDSI, leveraging its powerful representation to efficiently index new corpora\nwhile maintaining a balance between stability and plasticity. We eliminate the\ninitial forward pass of prompt-based continual learning methods that doubles\ntraining and inference time. Moreover, we propose a topic-aware prompt pool\nthat employs neural topic embeddings as fixed keys. This strategy ensures\ndiverse and effective prompt usage, addressing the challenge of parameter\nunderutilization caused by the collapse of the query-key matching mechanism.\nOur empirical evaluations demonstrate that BERT-based PromptDSI matches IncDSI\nin managing forgetting while improving new corpora performance by more than 4%\nHits@10 on NQ320k and upto 3% MRR@10 on MS MARCO 300k.\n","authors":["Tuan-Luc Huynh","Thuy-Trang Vu","Weiqing Wang","Yinwei Wei","Trung Le","Dragan Gasevic","Yuan-Fang Li","Thanh-Toan Do"],"pdf_url":"https://arxiv.org/pdf/2406.12593v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2406.12319v3","updated":"2024-10-16T13:39:49Z","published":"2024-06-18T06:43:04Z","title":"The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences\n  of LLM Evaluators","summary":"  As large language models (LLMs) are increasingly used as evaluators for\nnatural language generation tasks, ensuring unbiased assessments is essential.\nHowever, LLM evaluators often display biased preferences, such as favoring\nverbosity and authoritative tones. Our empirical analysis reveals that these\nbiases are exacerbated in pairwise evaluation, where LLMs directly compare two\noutputs and easily prioritize superficial attributes. In contrast, pointwise\nevaluation, which assesses outputs independently, is less susceptible to such\nbias because each output is judged in isolation. To address the limitations of\nthe pairwise evaluation, we introduce a novel evaluation method, PRePair, which\nintegrates pointwise reasoning within a pairwise framework. PRePair effectively\nalleviates biased preference, improving performance on the adversarial\nbenchmark (LLMBar) while outperforming pointwise evaluation on the standard\nbenchmark (MT-Bench).\n","authors":["Hawon Jeong","ChaeHun Park","Jimin Hong","Hojoon Lee","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2406.12319v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12558v1","updated":"2024-10-16T13:34:51Z","published":"2024-10-16T13:34:51Z","title":"A Claim Decomposition Benchmark for Long-form Answer Verification","summary":"  The advancement of LLMs has significantly boosted the performance of complex\nlong-form question answering tasks. However, one prominent issue of LLMs is the\ngenerated \"hallucination\" responses that are not factual. Consequently,\nattribution for each claim in responses becomes a common solution to improve\nthe factuality and verifiability. Existing researches mainly focus on how to\nprovide accurate citations for the response, which largely overlook the\nimportance of identifying the claims or statements for each response. To bridge\nthis gap, we introduce a new claim decomposition benchmark, which requires\nbuilding system that can identify atomic and checkworthy claims for LLM\nresponses. Specifically, we present the Chinese Atomic Claim Decomposition\nDataset (CACDD), which builds on the WebCPM dataset with additional expert\nannotations to ensure high data quality. The CACDD encompasses a collection of\n500 human-annotated question-answer pairs, including a total of 4956 atomic\nclaims. We further propose a new pipeline for human annotation and describe the\nchallenges of this task. In addition, we provide experiment results on\nzero-shot, few-shot and fine-tuned LLMs as baselines. The results show that the\nclaim decomposition is highly challenging and requires further explorations.\nAll code and data are publicly available at\n\\url{https://github.com/FBzzh/CACDD}.\n","authors":["Zhihao Zhang","Yixing Fan","Ruqing Zhang","Jiafeng Guo"],"pdf_url":"https://arxiv.org/pdf/2410.12558v1.pdf","comment":"Accepted by CCIR 2024"},{"id":"http://arxiv.org/abs/2406.18221v3","updated":"2024-10-16T13:31:05Z","published":"2024-06-26T10:08:47Z","title":"Enhancing Data Privacy in Large Language Models through Private\n  Association Editing","summary":"  Large language models (LLMs) require a significant redesign in solutions to\npreserve privacy in data-intensive applications due to their text-generation\ncapabilities. Indeed, LLMs tend to memorize and emit private information when\nmaliciously prompted. In this paper, we introduce Private Association Editing\n(PAE) as a novel defense approach for private data leakage. PAE is designed to\neffectively remove Personally Identifiable Information (PII) without retraining\nthe model. Experimental results demonstrate the effectiveness of PAE with\nrespect to alternative baseline methods. We believe PAE will serve as a\ncritical tool in the ongoing effort to protect data privacy in LLMs,\nencouraging the development of safer models for real-world applications.\n","authors":["Davide Venditti","Elena Sofia Ruzzetti","Giancarlo A. Xompero","Cristina Giannone","Andrea Favalli","Raniero Romagnoli","Fabio Massimo Zanzotto"],"pdf_url":"https://arxiv.org/pdf/2406.18221v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12543v1","updated":"2024-10-16T13:21:46Z","published":"2024-10-16T13:21:46Z","title":"LLM-based Translation Inference with Iterative Bilingual Understanding","summary":"  The remarkable understanding and generation capabilities of large language\nmodels (LLMs) have greatly improved translation performance. However, incorrect\nunderstanding of the sentence to be translated can degrade translation quality.\nTo address this issue, we proposed a novel Iterative Bilingual Understanding\nTranslation (IBUT) method based on the cross-lingual capabilities of LLMs and\nthe dual characteristics of translation tasks. The cross-lingual capability of\nLLMs enables the generation of contextual understanding for both the source and\ntarget languages separately. Furthermore, the dual characteristics allow IBUT\nto generate effective cross-lingual feedback, iteratively refining contextual\nunderstanding, thereby reducing errors and improving translation performance.\nExperimental results showed that the proposed IBUT outperforms several strong\ncomparison methods, especially being generalized to multiple domains (e.g.,\nnews, commonsense, and cultural translation benchmarks).\n","authors":["Andong Chen","Kehai Chen","Yang Xiang","Xuefeng Bai","Muyun Yang","Tiejun Zhao","Min zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12543v1.pdf","comment":"work in process"},{"id":"http://arxiv.org/abs/2406.14162v3","updated":"2024-10-16T13:16:25Z","published":"2024-06-20T10:04:09Z","title":"DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval\n  Augmented Generation","summary":"  Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information when answering queries that need an integrated analysis\nof information (e.g., Tell me good news in the stock market today.)? To address\nthese concerns, RAG developers need to annotate information retrieval (IR) data\nfor their domain of interest, which is challenging because (1) domain-specific\nqueries usually need nuanced definitions of relevance beyond shallow semantic\nrelevance; and (2) human or GPT-4 annotation is costly and cannot cover all\n(query, document) pairs (i.e., annotation selection bias), thus harming the\neffectiveness in evaluating IR recall. To address these challenges, we propose\nDIRAS (Domain-specific Information Retrieval Annotation with Scalability), a\nmanual-annotation-free schema that fine-tunes open-sourced LLMs to consider\nnuanced relevance definition and annotate (partial) relevance labels with\ncalibrated relevance scores. Extensive evaluation shows that DIRAS enables\nsmaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking\nunseen (query, document) pairs, and is helpful for real-world RAG development.\nAll code, LLM generations, and human annotations can be found in\n\\url{https://github.com/EdisonNi-hku/DIRAS}.\n","authors":["Jingwei Ni","Tobias Schimanski","Meihong Lin","Mrinmaya Sachan","Elliott Ash","Markus Leippold"],"pdf_url":"https://arxiv.org/pdf/2406.14162v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12532v1","updated":"2024-10-16T13:10:27Z","published":"2024-10-16T13:10:27Z","title":"MedAide: Towards an Omni Medical Aide via Specialized LLM-based\n  Multi-Agent Collaboration","summary":"  Large Language Model (LLM)-driven interactive systems currently show\npotential promise in healthcare domains. Despite their remarkable capabilities,\nLLMs typically lack personalized recommendations and diagnosis analysis in\nsophisticated medical applications, causing hallucinations and performance\nbottlenecks. To address these challenges, this paper proposes MedAide, an\nLLM-based omni medical multi-agent collaboration framework for specialized\nhealthcare services. Specifically, MedAide first performs query rewriting\nthrough retrieval-augmented generation to accomplish accurate medical intent\nunderstanding. Immediately, we devise a contextual encoder to obtain intent\nprototype embeddings, which are used to recognize fine-grained intents by\nsimilarity matching. According to the intent relevance, the activated agents\ncollaborate effectively to provide integrated decision analysis. Extensive\nexperiments are conducted on four medical benchmarks with composite intents.\nExperimental results from automated metrics and expert doctor evaluations show\nthat MedAide outperforms current LLMs and improves their medical proficiency\nand strategic reasoning.\n","authors":["Jinjie Wei","Dingkang Yang","Yanshu Li","Qingyao Xu","Zhaoyu Chen","Mingcheng Li","Yue Jiang","Xiaolu Hou","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12532v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2407.09879v3","updated":"2024-10-16T12:57:56Z","published":"2024-07-13T13:03:45Z","title":"sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through\n  N-shot Guided Prompting","summary":"  Despite the remarkable success of LLMs in English, there is a significant gap\nin performance in non-English languages. In order to address this, we introduce\na novel recipe for creating a multilingual synthetic instruction tuning\ndataset, sPhinX, which is created by selectively translating instruction\nresponse pairs from English into 50 languages. We test the effectiveness of\nsPhinx by using it to fine-tune two state-of-the-art models, Mistral-7B and\nPhi-Small and then evaluating them across a comprehensive suite of multilingual\nbenchmarks that test reasoning, question answering, reading comprehension and\nmachine translation. Our results show that Mistral-7B and Phi-Small fine-tuned\nwith sPhinX perform better on an average by 5%pt for both the models when\ncompared to the base variants of these models. We also devise a strategy to\nincorporate N-shot examples in each fine-tuning sample which further boosts the\nperformance of these models by 9%pt and 4%pt respectively respectively compared\nto vanilla fine-tuning. To show efficacy of our data curation approach, we also\ndirectly translate our original dataset to the target languages, and observe an\nincrease of 7%pt and 4%pt on both the models respectively. sPhinX outperforms\nother multilingual instruction tuning datasets in both efficiency and\ndiversity, reducing dataset creation costs. It also maintains strong\nperformance on standard English LLM benchmarks, with minimal regression.\n","authors":["Sanchit Ahuja","Kumar Tanmay","Hardik Hansrajbhai Chauhan","Barun Patra","Kriti Aggarwal","Luciano Del Corro","Arindam Mitra","Tejas Indulal Dhamecha","Ahmed Awadallah","Monojit Choudhary","Vishrav Chaudhary","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2407.09879v3.pdf","comment":"20 pages, 12 tables, 5 figures"},{"id":"http://arxiv.org/abs/2409.19014v3","updated":"2024-10-16T12:55:55Z","published":"2024-09-24T01:40:50Z","title":"FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark","summary":"  Text-to-SQL systems have become crucial for translating natural language into\nSQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, the Execution Accuracy\n(EX), the most prevalent evaluation metric, still shows many false positives\nand negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our metric improves\nagreement with human experts (from 62 to 87.04 in Cohen's kappa) with\ncomprehensive context and sophisticated criteria. Our extensive experiments\nyield several key insights: (1) Models' performance increases by over 2.6\npoints on average, substantially affecting rankings on Spider and BIRD\nbenchmarks; (2) The underestimation of models in EX primarily stems from\nannotation quality issues; and (3) Model performance on particularly\nchallenging questions tends to be overestimated. This work contributes to a\nmore accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field.\n","authors":["Heegyu Kim","Taeyang Jeon","Seunghwan Choi","Seungtaek Choi","Hyunsouk Cho"],"pdf_url":"https://arxiv.org/pdf/2409.19014v3.pdf","comment":"preprint, under review"},{"id":"http://arxiv.org/abs/2410.12513v1","updated":"2024-10-16T12:45:35Z","published":"2024-10-16T12:45:35Z","title":"FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction","summary":"  Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments.\n","authors":["Akriti Jain","Saransh Sharma","Koyel Mukherjee","Soumyabrata Pal"],"pdf_url":"https://arxiv.org/pdf/2410.12513v1.pdf","comment":"17 pages, 6 figures, Submitted to ICLR 2025"},{"id":"http://arxiv.org/abs/2410.12511v1","updated":"2024-10-16T12:38:58Z","published":"2024-10-16T12:38:58Z","title":"Advancing Fairness in Natural Language Processing: From Traditional\n  Methods to Explainability","summary":"  The burgeoning field of Natural Language Processing (NLP) stands at a\ncritical juncture where the integration of fairness within its frameworks has\nbecome an imperative. This PhD thesis addresses the need for equity and\ntransparency in NLP systems, recognizing that fairness in NLP is not merely a\ntechnical challenge but a moral and ethical necessity, requiring a rigorous\nexamination of how these technologies interact with and impact diverse human\npopulations. Through this lens, this thesis undertakes a thorough investigation\ninto the development of equitable NLP methodologies and the evaluation of\nbiases that prevail in current systems.\n  First, it introduces an innovative algorithm to mitigate biases in\nmulti-class classifiers, tailored for high-risk NLP applications, surpassing\ntraditional methods in both bias mitigation and prediction accuracy. Then, an\nanalysis of the Bios dataset reveals the impact of dataset size on\ndiscriminatory biases and the limitations of standard fairness metrics. This\nawareness has led to explorations in the field of explainable AI, aiming for a\nmore complete understanding of biases where traditional metrics are limited.\nConsequently, the thesis presents COCKATIEL, a model-agnostic explainability\nmethod that identifies and ranks concepts in Transformer models, outperforming\nprevious approaches in sentiment analysis tasks. Finally, the thesis\ncontributes to bridging the gap between fairness and explainability by\nintroducing TaCo, a novel method to neutralize bias in Transformer model\nembeddings.\n  In conclusion, this thesis constitutes a significant interdisciplinary\nendeavor that intertwines explicability and fairness to challenge and reshape\ncurrent NLP paradigms. The methodologies and critiques presented contribute to\nthe ongoing discourse on fairness in machine learning, offering actionable\nsolutions for more equitable and responsible AI systems.\n","authors":["Fanny Jourdan"],"pdf_url":"https://arxiv.org/pdf/2410.12511v1.pdf","comment":"PhD Thesis, Toulouse University"},{"id":"http://arxiv.org/abs/2410.10114v2","updated":"2024-10-16T12:30:53Z","published":"2024-10-14T03:05:12Z","title":"Mixture of Experts Made Personalized: Federated Prompt Learning for\n  Vision-Language Models","summary":"  Prompt learning for pre-trained Vision-Language Models (VLMs) like CLIP has\ndemonstrated potent applicability across diverse downstream tasks. This\nlightweight approach has quickly gained traction from federated learning (FL)\nresearchers who seek to efficiently adapt VLMs to heterogeneous scenarios.\nHowever, current federated prompt learning methods are habitually restricted to\nthe traditional FL paradigm, where the participating clients are generally only\nallowed to download a single globally aggregated model from the server. While\njustifiable for training full-sized models under federated settings, in this\nwork, we argue that this paradigm is ill-suited for lightweight prompts. By\nfacilitating the clients to download multiple pre-aggregated prompts as fixed\nnon-local experts, we propose Personalized Federated Mixture of Adaptive\nPrompts (pFedMoAP), a novel FL framework that personalizes the prompt learning\nprocess through the lens of Mixture of Experts (MoE). pFedMoAP implements a\nlocal attention-based gating network that learns to generate enhanced text\nfeatures for better alignment with local image data on the client, benefiting\nfrom both local and downloaded non-local adaptive prompt experts. The non-local\nexperts are sparsely selected from a server-maintained pool, fostering\ncollaborative learning across clients. To evaluate the proposed algorithm, we\nconduct extensive experiments across 9 datasets under various heterogeneous\nfederated settings. The results show that pFedMoAP consistently outperforms the\nstate-of-the-art alternatives, underscoring its efficacy in personalizing\nprompt learning for CLIP within the federated learning paradigm.\n","authors":["Jun Luo","Chen Chen","Shandong Wu"],"pdf_url":"https://arxiv.org/pdf/2410.10114v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.12499v1","updated":"2024-10-16T12:22:47Z","published":"2024-10-16T12:22:47Z","title":"With a Grain of SALT: Are LLMs Fair Across Social Dimensions?","summary":"  This paper presents an analysis of biases in open-source Large Language\nModels (LLMs) across various genders, religions, and races. We introduce a\nmethodology for generating a bias detection dataset using seven bias triggers:\nGeneral Debate, Positioned Debate, Career Advice, Story Generation,\nProblem-Solving, Cover-Letter Writing, and CV Generation. We use GPT-4o to\ngenerate a diverse set of prompts for each trigger across various genders,\nreligious and racial groups. We evaluate models from Llama and Gemma family on\nthe generated dataset. We anonymise the LLM-generated text associated with each\ngroup using GPT-4o-mini and do a pairwise comparison using GPT-4o-as-a-Judge.\nTo quantify bias in the LLM-generated text we use the number of wins and losses\nin the pairwise comparison. Our analysis spans three languages, English,\nGerman, and Arabic to explore how language influences bias manifestation. Our\nfindings reveal that LLMs exhibit strong polarization toward certain groups\nacross each category, with a notable consistency observed across models.\nHowever, when switching languages, variations and anomalies emerge, often\nattributable to cultural cues and contextual differences.\n","authors":["Samee Arif","Zohaib Khan","Agha Ali Raza","Awais Athar"],"pdf_url":"https://arxiv.org/pdf/2410.12499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05770v2","updated":"2024-10-16T12:18:20Z","published":"2024-10-08T07:52:35Z","title":"Efficient Few-shot Learning for Multi-label Classification of Scientific\n  Documents with Many Classes","summary":"  Scientific document classification is a critical task and often involves many\nclasses. However, collecting human-labeled data for many classes is expensive\nand usually leads to label-scarce scenarios. Moreover, recent work has shown\nthat sentence embedding model fine-tuning for few-shot classification is\nefficient, robust, and effective. In this work, we propose FusionSent\n(Fusion-based Sentence Embedding Fine-tuning), an efficient and prompt-free\napproach for few-shot classification of scientific documents with many classes.\nFusionSent uses available training examples and their respective label texts to\ncontrastively fine-tune two different sentence embedding models. Afterward, the\nparameters of both fine-tuned models are fused to combine the complementary\nknowledge from the separate fine-tuning steps into a single model. Finally, the\nresulting sentence embedding model is frozen to embed the training instances,\nwhich are then used as input features to train a classification head. Our\nexperiments show that FusionSent significantly outperforms strong baselines by\nan average of $6.0$ $F_{1}$ points across multiple scientific document\nclassification datasets. In addition, we introduce a new dataset for\nmulti-label classification of scientific documents, which contains 203,961\nscientific articles and 130 classes from the arXiv category taxonomy. Code and\ndata are available at https://github.com/sebischair/FusionSent.\n","authors":["Tim Schopf","Alexander Blatzheim","Nektarios Machner","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2410.05770v2.pdf","comment":"Accepted to the 7th International Conference on Natural Language and\n  Speech Processing (ICNLSP 2024)"},{"id":"http://arxiv.org/abs/2408.09945v2","updated":"2024-10-16T12:15:39Z","published":"2024-08-19T12:34:31Z","title":"Benchmarking LLMs for Translating Classical Chinese Poetry:Evaluating\n  Adequacy, Fluency, and Elegance","summary":"  Large language models (LLMs) have shown remarkable performance in general\ntranslation tasks. However, the increasing demand for high-quality translations\nthat are not only adequate but also fluent and elegant. To assess the extent to\nwhich current LLMs can meet these demands, we introduce a suitable benchmark\nfor translating classical Chinese poetry into English. This task requires not\nonly adequacy in translating culturally and historically significant content\nbut also a strict adherence to linguistic fluency and poetic elegance. Our\nstudy reveals that existing LLMs fall short of this task. To address these\nissues, we propose RAT, a \\textbf{R}etrieval-\\textbf{A}ugmented machine\n\\textbf{T}ranslation method that enhances the translation process by\nincorporating knowledge related to classical poetry. Additionally, we propose\nan automatic evaluation metric based on GPT-4, which better assesses\ntranslation quality in terms of adequacy, fluency, and elegance, overcoming the\nlimitations of traditional metrics. Our dataset and code will be made\navailable.\n","authors":["Andong Chen","Lianzhang Lou","Kehai Chen","Xuefeng Bai","Yang Xiang","Muyun Yang","Tiejun Zhao","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.09945v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2408.08688v4","updated":"2024-10-16T12:15:19Z","published":"2024-08-16T12:01:55Z","title":"The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic\n  Preference Optimization Dataset Generation","summary":"  This paper presents a novel methodology for generating synthetic Preference\nOptimization (PO) datasets using multi-agent workflows. We evaluate the\neffectiveness and potential of these workflows in automating and enhancing the\ndataset generation process. PO dataset generation requires two modules: (1)\nresponse evaluation, and (2) response generation. In the response evaluation\nmodule, the responses from Large Language Models (LLMs) are evaluated and\nranked - a task typically carried out by human annotators that we automate\nusing LLMs. We assess the response evaluation module in a 2 step process. In\nstep 1, we assess LLMs as evaluators using three distinct prompting strategies.\nIn step 2, we apply the winning prompting strategy to compare the performance\nof LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. Our evaluation shows that\nGPT-4o-as-a-Judge is more consistent across all datasets. For the response\ngeneration module, we use the identified LLM evaluator configuration and\ncompare different configurations of the LLM Feedback Loop. We use the win rate\nto determine the best multi-agent configuration for generation. Experimenting\nwith various configurations, we find that the LLM Feedback Loop, with Llama as\nthe generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win\nrate over single-agent Llama and Gemma, respectively. After identifying the\nbest configurations for both modules, we generate our PO datasets using the\nabove pipeline.\n","authors":["Samee Arif","Sualeha Farid","Abdul Hameed Azeemi","Awais Athar","Agha Ali Raza"],"pdf_url":"https://arxiv.org/pdf/2408.08688v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12492v1","updated":"2024-10-16T12:14:29Z","published":"2024-10-16T12:14:29Z","title":"End-to-end Planner Training for Language Modeling","summary":"  Through end-to-end training to predict the next token, LLMs have become\nvaluable tools for various tasks. Enhancing their core training in language\nmodeling can improve numerous downstream applications. A successful approach to\nenhance language modeling uses a separate planning module to predict abstract\nlabels of future sentences and conditions the LM on these predictions. However,\nthis method is non-differentiable, preventing joint end-to-end tuning of the\nplanner with the LM. We propose an effective method to improve this approach by\nenabling joint fine-tuning of the planner and the LM. We show that a naive way\nof approximating the gradient of selecting a label via the straight-through\nestimator is not effective. Instead, we propose to use the predicted label\nprobabilities as mixing weights to condition the LM on a weighted average of\nlabel embeddings in a differentiable manner. This not only enables joint\nfine-tuning of the planner and the LM, but also allows the LM to draw on the\nfull label distribution predicted by the planner, retaining more information.\nOur experimental results show consistent improvements in perplexity.\n","authors":["Nathan Cornille","Florian Mai","Jingyuan Sun","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2410.12492v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2410.12491v1","updated":"2024-10-16T12:14:25Z","published":"2024-10-16T12:14:25Z","title":"Insights from the Inverse: Reconstructing LLM Training Goals Through\n  Inverse RL","summary":"  Large language models (LLMs) trained with Reinforcement Learning from Human\nFeedback (RLHF) have demonstrated remarkable capabilities, but their underlying\nreward functions and decision-making processes remain opaque. This paper\nintroduces a novel approach to interpreting LLMs by applying inverse\nreinforcement learning (IRL) to recover their implicit reward functions. We\nconduct experiments on toxicity-aligned LLMs of varying sizes, extracting\nreward models that achieve up to 80.40% accuracy in predicting human\npreferences. Our analysis reveals key insights into the non-identifiability of\nreward functions, the relationship between model size and interpretability, and\npotential pitfalls in the RLHF process. We demonstrate that IRL-derived reward\nmodels can be used to fine-tune new LLMs, resulting in comparable or improved\nperformance on toxicity benchmarks. This work provides a new lens for\nunderstanding and improving LLM alignment, with implications for the\nresponsible development and deployment of these powerful systems.\n","authors":["Jared Joselowitz","Arjun Jagota","Satyapriya Krishna","Sonali Parbhoo"],"pdf_url":"https://arxiv.org/pdf/2410.12491v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.16535v2","updated":"2024-10-16T12:00:46Z","published":"2024-06-24T11:16:26Z","title":"Token-based Decision Criteria Are Suboptimal in In-context Learning","summary":"  In-Context Learning (ICL) typically utilizes classification criteria from\noutput probabilities of manually selected label tokens. However, we argue that\nsuch token-based classification criteria lead to suboptimal decision\nboundaries, despite delicate calibrations through translation and constrained\nrotation applied. To address this problem, we propose Hidden Calibration, which\nrenounces token probabilities and uses the nearest centroid classifier on the\nLM's last hidden states. In detail, we assign the label of the nearest centroid\npreviously estimated from a calibration set to the test sample as the predicted\nlabel. Our experiments on 6 models and 10 classification datasets indicate that\nHidden Calibration consistently outperforms current token-based baselines by\nabout 20%~50%, achieving a strong state-of-the-art in ICL. Our further analysis\ndemonstrates that Hidden Calibration finds better classification criteria with\nless inter-class overlap, and LMs provide linearly separable intra-class\nclusters with the help of demonstrations, which supports Hidden Calibration and\ngives new insights into the principle of ICL.\n","authors":["Hakaze Cho","Yoshihiro Sakai","Mariko Kato","Kenshiro Tanaka","Akira Ishii","Naoya Inoue"],"pdf_url":"https://arxiv.org/pdf/2406.16535v2.pdf","comment":"24 pages, 15 figures, 13 tables"},{"id":"http://arxiv.org/abs/2402.10052v2","updated":"2024-10-16T11:50:27Z","published":"2024-02-15T16:21:14Z","title":"UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in\n  Large Language Models","summary":"  Mitigating the retention of sensitive or private information in large\nlanguage models is essential for enhancing privacy and safety. Existing\nunlearning methods, like Gradient Ascent and Negative Preference Optimization,\ndirectly tune models to remove unwanted information. However, these methods\noften become unstable because they fine-tune by maximizing cross-entropy loss,\nwhich is the opposite of traditional loss minimization in learning. This\nreversal creates instability, especially on larger datasets, as the model\nstruggles to balance unlearning with maintaining language capacity, leading to\nover-unlearning. In this paper, we introduce UnDIAL (Unlearning via\nSelf-Distillation on Adjusted Logits), a novel and robust unlearning method.\nOur approach leverages self-distillation to adjust logits and selectively\nreduce the influence of targeted tokens. This technique ensures smooth\nconvergence and avoids catastrophic forgetting, even in challenging unlearning\ntasks with large datasets and sequential unlearning requests. Extensive\nexperiments show that UnDIAL can achieve both robustness in unlearning and\nscalability while maintaining stable training dynamics and resilience to\nhyperparameter tuning.\n","authors":["Yijiang River Dong","Hongzhou Lin","Mikhail Belkin","Ramon Huerta","Ivan Vulić"],"pdf_url":"https://arxiv.org/pdf/2402.10052v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12480v1","updated":"2024-10-16T11:50:02Z","published":"2024-10-16T11:50:02Z","title":"KcMF: A Knowledge-compliant Framework for Schema and Entity Matching\n  with Fine-tuning-free LLMs","summary":"  Schema and entity matching tasks are crucial for data integration and\nmanagement. While large language models (LLMs) have shown promising results in\nthese tasks, they suffer from hallucinations and confusion about task\ninstructions. In this paper, we present the Knowledge-Compliant Matching\nFramework (KcMF), an LLM-based approach that addresses these issues without the\nneed for domain-specific fine-tuning. KcMF employs a pseudo-code-based task\ndecomposition strategy to adopt task-specific natural language statements that\nguide LLM reasoning and reduce confusion. We also propose two mechanisms,\nDataset as Knowledge (DaK) and Example as Knowledge (EaK), to build domain\nknowledge sets when unstructured domain knowledge is lacking. Additionally, we\nintroduce a result-ensembling strategy to leverage multiple knowledge sources\nand suppress poorly formatted outputs. Comprehensive evaluations on schema and\nentity matching tasks demonstrate that KcMF outperforms previous non-LLM\nstate-of-the-art (SOTA) methods by an average F1 score of 22.9% and competes\neffectively with SOTA fine-tuned LLMs. Moreover, KcMF generalizes well across\ndifferent LLMs.\n","authors":["Yongqin Xu","Huan Li","Ke Chen","Lidan Shou"],"pdf_url":"https://arxiv.org/pdf/2410.12480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08440v3","updated":"2024-10-16T11:49:48Z","published":"2024-07-11T12:26:55Z","title":"Beyond Instruction Following: Evaluating Inferential Rule Following of\n  Large Language Models","summary":"  Although Large Language Models (LLMs) have demonstrated strong ability, they\nare further supposed to be controlled and guided by in real-world scenarios to\nbe safe, accurate, and intelligent. This demands the possession of capability\nof LLMs. However, no prior work has made a clear evaluation of the inferential\nrule-following capability of LLMs. Previous studies that try to evaluate the\ninferential rule-following capability of LLMs fail to distinguish the\ninferential rule-following scenarios from the instruction-following scenarios.\nTherefore, this paper first clarifies the concept of inferential rule-following\nand proposes a comprehensive benchmark, RuleBench, to evaluate a diversified\nrange of inferential rule-following abilities. Our experimental results on a\nvariety of LLMs show that they are still limited in following rules. Our\nanalysis based on the evaluation results provides insights into the\nimprovements for LLMs toward a better inferential rule-following intelligent\nagent. We further propose Inferential Rule-Following Tuning (IRFT). The\nexperimental results show that through IRFT, LLMs can learn abstract\nrule-following abilities from purely synthetic data and then generalize to\nRuleBench. The data and code can be found at:\nhttps://anonymous.4open.science/r/llm-rule-following-B3E3/\n","authors":["Wangtao Sun","Chenxiang Zhang","XueYou Zhang","Xuanqing Yu","Ziyang Huang","Pei Chen","Haotian Xu","Shizhu He","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.08440v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12478v1","updated":"2024-10-16T11:46:55Z","published":"2024-10-16T11:46:55Z","title":"MlingConf: A Comprehensive Study of Multilingual Confidence Estimation\n  on Large Language Models","summary":"  The tendency of Large Language Models (LLMs) to generate hallucinations\nraises concerns regarding their reliability. Therefore, confidence estimations\nindicating the extent of trustworthiness of the generations become essential.\nHowever, current LLM confidence estimations in languages other than English\nremain underexplored. This paper addresses this gap by introducing a\ncomprehensive investigation of Multilingual Confidence estimation (MlingConf)\non LLMs, focusing on both language-agnostic (LA) and language-specific (LS)\ntasks to explore the performance and language dominance effects of multilingual\nconfidence estimations on different tasks. The benchmark comprises four\nmeticulously checked and human-evaluate high-quality multilingual datasets for\nLA tasks and one for the LS task tailored to specific social, cultural, and\ngeographical contexts of a language. Our experiments reveal that on LA tasks\nEnglish exhibits notable linguistic dominance in confidence estimations than\nother languages, while on LS tasks, using question-related language to prompt\nLLMs demonstrates better linguistic dominance in multilingual confidence\nestimations. The phenomena inspire a simple yet effective native-tone prompting\nstrategy by employing language-specific prompts for LS tasks, effectively\nimproving LLMs' reliability and accuracy on LS tasks.\n","authors":["Boyang Xue","Hongru Wang","Rui Wang","Sheng Wang","Zezhong Wang","Yiming Du","Bin Liang","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2410.12478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12476v1","updated":"2024-10-16T11:46:32Z","published":"2024-10-16T11:46:32Z","title":"Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial\n  Generation","summary":"  Machine learning (ML) exhibits promise in the clinical domain. However, it is\nconstrained by data scarcity and ethical considerations, as the generation of\nclinical trials presents significant challenges due to stringent privacy\nregulations, high costs, and the extended duration required for conducting\nstudies with human participants. Despite the advancements of large language\nmodels (LLMs) in general generation tasks, their potential in facilitating the\ngeneration of synthetic clinical trials is under-explored. To address this gap,\nwe introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs\nto generate artificial yet realistic and diverse clinical trials with binary\nsuccess/failure labels. Experiments conducted on real clinical trials from the\n\\url{ClinicalTrials.gov} database demonstrate that our synthetic data can\neffectively augment real datasets. Furthermore, by fine-tuning a pre-trained\nmodel as a binary classifier on synthetic clinical trial datasets, we\ndemonstrate that this augmentation enhances model training for downstream tasks\nsuch as trial outcome prediction. Our findings suggest that LLMs for synthetic\nclinical trial generation hold promise for accelerating clinical research and\nupholding ethical standards for patient privacy. The code is publicly available\nat\nhttps://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.\n","authors":["Zerui Xu","Fang Wu","Tianfan Fu","Yue Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.12476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12470v1","updated":"2024-10-16T11:34:33Z","published":"2024-10-16T11:34:33Z","title":"Learning to Predict Usage Options of Product Reviews with LLM-Generated\n  Labels","summary":"  Annotating large datasets can be challenging. However, crowd-sourcing is\noften expensive and can lack quality, especially for non-trivial tasks. We\npropose a method of using LLMs as few-shot learners for annotating data in a\ncomplex natural language task where we learn a standalone model to predict\nusage options for products from customer reviews. We also propose a new\nevaluation metric for this scenario, HAMS4, that can be used to compare a set\nof strings with multiple reference sets. Learning a custom model offers\nindividual control over energy efficiency and privacy measures compared to\nusing the LLM directly for the sequence-to-sequence task. We compare this data\nannotation approach with other traditional methods and demonstrate how LLMs can\nenable considerable cost savings. We find that the quality of the resulting\ndata exceeds the level attained by third-party vendor services and that\nGPT-4-generated labels even reach the level of domain experts. We make the code\nand generated labels publicly available.\n","authors":["Leo Kohlenberg","Leonard Horns","Frederic Sadrieh","Nils Kiele","Matthis Clausen","Konstantin Ketterer","Avetis Navasardyan","Tamara Czinczoll","Gerard de Melo","Ralf Herbrich"],"pdf_url":"https://arxiv.org/pdf/2410.12470v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2410.12462v1","updated":"2024-10-16T11:23:03Z","published":"2024-10-16T11:23:03Z","title":"Bridging the Language Gaps in Large Language Models with Inference-Time\n  Cross-Lingual Intervention","summary":"  Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage processing but exhibit significant performance gaps among different\nlanguages. Most existing approaches to address these disparities rely on\npretraining or fine-tuning, which are resource-intensive. To overcome these\nlimitations without incurring significant costs, we propose Inference-Time\nCross-Lingual Intervention (INCLINE), a novel framework that enhances LLM\nperformance on low-performing (source) languages by aligning their internal\nrepresentations with those of high-performing (target) languages during\ninference. INCLINE initially learns alignment matrices using parallel sentences\nfrom source and target languages through a Least-Squares optimization, and then\napplies these matrices during inference to transform the low-performing\nlanguage representations toward the high-performing language space. Extensive\nexperiments on nine benchmarks with five LLMs demonstrate that INCLINE\nsignificantly improves performance across diverse tasks and languages, compared\nto recent strong baselines. Our analysis demonstrates that INCLINE is highly\ncost-effective and applicable to a wide range of applications. In addition, we\nrelease the code to foster research along this line:\nhttps://github.com/weixuan-wang123/INCLINE.\n","authors":["Weixuan Wang","Minghao Wu","Barry Haddow","Alexandra Birch"],"pdf_url":"https://arxiv.org/pdf/2410.12462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12458v1","updated":"2024-10-16T11:16:34Z","published":"2024-10-16T11:16:34Z","title":"The Best of Both Worlds: Bridging Quality and Diversity in Data\n  Selection with Bipartite Graph","summary":"  The performance of large language models (LLMs) in natural language\nprocessing (NLP) tasks is significantly influenced by the quality and diversity\nof data used for supervised fine-tuning (SFT). Current data selection methods\noften focus solely on quality or diversity, leading to underperforming models\ndue to suboptimal training data. In this paper, we introduce GraphFilter, a\nnovel method that represents the dataset as a bipartite graph, linking\nsentences to their constituent n-grams. This representation effectively\ncaptures the relationships between sentences and linguistic patterns,\nfacilitating the selection of sentences that enhance n-gram diversity. To\nbalance quality and diversity during selection, we propose a priority function\nthat combines the quality metric with the diversity metric in a multiplicative\nmanner. GraphFilter iteratively selects high-priority sentences, updates the\nbipartite graph by removing covered n-grams, and re-calculates priorities to\nreflect the evolving data landscape. We conduct extensive experiments using\nthree model backbones across six widely used benchmarks. The results\ndemonstrate that GraphFilter outperforms all nine baseline approaches,\nachieving superior model performance and computational efficiency. Our analyses\nvalidate the effectiveness of our design choices, examine the subsets selected\nby GraphFilter and other methods, highlight the importance of instruction\ndiversity, and explore the role of quality and diversity in relation to subset\nsizes. GraphFilter establishes a new foundation for effective data selection\nstrategies, encouraging further research in data selection for LLMs.\n","authors":["Minghao Wu","Thuy-Trang Vu","Lizhen Qu","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2410.12458v1.pdf","comment":"19 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2406.07411v2","updated":"2024-10-16T10:56:24Z","published":"2024-06-11T16:15:06Z","title":"VersiCode: Towards Version-controllable Code Generation","summary":"  Large Language Models (LLMs) have made tremendous strides in code generation,\nbut existing research fails to account for the dynamic nature of software\ndevelopment, marked by frequent library updates. This gap significantly limits\nLLMs' deployment in realistic settings. In this paper, we propose two novel\ntasks aimed at bridging this gap: version-specific code completion (VSCC) and\nversion-aware code migration (VACM). In conjunction, we introduce VersiCode, a\ncomprehensive Python dataset specifically designed to evaluate LLMs on these\ntwo tasks, together with a novel evaluation metric, Critical Diff Check\n(CDC@1), which assesses code generation against evolving API requirements. We\nconduct an extensive evaluation on VersiCode, which reveals that\nversion-controllable code generation is indeed a significant challenge, even\nfor GPT-4o and other strong frontier models. We believe the novel tasks,\ndataset, and metric open up a new, important research direction that will\nfurther enhance LLMs' real-world applicability. The code and resources can be\nfound at https://github.com/wutong8023/VersiCode.\n","authors":["Tongtong Wu","Weigang Wu","Xingyu Wang","Kang Xu","Suyu Ma","Bo Jiang","Ping Yang","Zhenchang Xing","Yuan-Fang Li","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2406.07411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12445v1","updated":"2024-10-16T10:49:22Z","published":"2024-10-16T10:49:22Z","title":"Open Ko-LLM Leaderboard2: Bridging Foundational and Practical Evaluation\n  for Korean LLMs","summary":"  The Open Ko-LLM Leaderboard has been instrumental in benchmarking Korean\nLarge Language Models (LLMs), yet it has certain limitations. Notably, the\ndisconnect between quantitative improvements on the overly academic leaderboard\nbenchmarks and the qualitative impact of the models should be addressed.\nFurthermore, the benchmark suite is largely composed of translated versions of\ntheir English counterparts, which may not fully capture the intricacies of the\nKorean language. To address these issues, we propose Open Ko-LLM Leaderboard2,\nan improved version of the earlier Open Ko-LLM Leaderboard. The original\nbenchmarks are entirely replaced with new tasks that are more closely aligned\nwith real-world capabilities. Additionally, four new native Korean benchmarks\nare introduced to better reflect the distinct characteristics of the Korean\nlanguage. Through these refinements, Open Ko-LLM Leaderboard2 seeks to provide\na more meaningful evaluation for advancing Korean LLMs.\n","authors":["Hyeonwoo Kim","Dahyun Kim","Jihoo Kim","Sukyung Lee","Yungi Kim","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2410.12445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12444v1","updated":"2024-10-16T10:48:14Z","published":"2024-10-16T10:48:14Z","title":"Expanding Chatbot Knowledge in Customer Service: Context-Aware Similar\n  Question Generation Using Large Language Models","summary":"  Reliable responses of service chatbots are often achieved by employing\nretrieval-based methods that restrict answers to a knowledge base comprising\npredefined question-answer pairs (QA pairs). To accommodate potential\nvariations in how a customer's query may be expressed, it emerges as the\nfavored solution to augment these QA pairs with similar questions that are\npossibly diverse while remaining semantic consistency. This augmentation task\nis known as Similar Question Generation (SQG). Traditional methods that heavily\nrely on human efforts or rule-based techniques suffer from limited diversity or\nsignificant semantic deviation from the source question, only capable of\nproducing a finite number of useful questions.\n  To address these limitations, we propose an SQG approach based on Large\nLanguage Models (LLMs), capable of producing a substantial number of diverse\nquestions while maintaining semantic consistency to the source QA pair. This is\nachieved by leveraging LLMs' natural language understanding capability through\nfine-tuning with specially designed prompts. The experiments conducted on a\nreal customer-service dataset demonstrate that our method surpasses baseline\nmethods by a significant margin in terms of semantic diversity. Human\nevaluation further confirms that integrating the answer that reflects the\ncustomer's intention is crucial for increasing the number of generated\nquestions that meet business requirements.\n","authors":["Mengze Hong","Yuanfeng Song","Di Jiang","Lu Wang","Zichang Guo","Chen Jason Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11507v2","updated":"2024-10-16T10:36:18Z","published":"2024-10-15T11:20:42Z","title":"Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic\n  Evaluation Framework for LLMs","summary":"  While various vertical domain large language models (LLMs) have been\ndeveloped, the challenge of automatically evaluating their performance across\ndifferent domains remains significant. Current benchmark-based evaluation\nmethods exhibit rigid, aimless interactions and rely on pre-collected static\ndatasets that are costly to build, inflexible across domains, and misaligned\nwith practical user needs. To address this issue, we revisit the evaluation\ncomponents and introduce two concepts: Benchmark+, which extends traditional\nquestion-answer benchmark into a more flexible \"strategy-criterion\" format; and\nAssessment+, which enhances the interaction process, enabling deeper\nexploration and supporting both quantitative metrics and qualitative insights.\nThese concepts capture the nuanced behaviors of LLMs through richer, multi-turn\ninteractions. We propose an agent-based evaluation framework called TestAgent,\nwhich implements these concepts through retrieval augmented generation and\nreinforcement learning. Experiments on tasks ranging from constructing vertical\ndomain evaluation to activating existing benchmarks demonstrate the\neffectiveness of TestAgent across various scenarios. We believe this work\noffers an interesting perspective on automatic evaluation for LLMs.\n","authors":["Wanying Wang","Zeyu Ma","Pengfei Liu","Mingang Chen"],"pdf_url":"https://arxiv.org/pdf/2410.11507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16264v2","updated":"2024-10-16T10:19:45Z","published":"2024-08-29T05:02:52Z","title":"LoraMap: Harnessing the Power of LoRA Connections","summary":"  Fact-checking techniques can mitigate hallucinations in Large Language Models\n(LLMs), a prominent issue in specialized domains. As parameter-efficient\ntechniques such as Low-Rank Adaptation (LoRA) can overcome substantial\ncomputational overhead, some studies have explored the integration of multiple\nLoRAs. While previous studies focus on parallel integration, this paper\ninvestigates methods to establish connections among multiple LoRAs. We create\nthree reasoning datasets tailored to fact-checking and fine-tune individual\nLoRAs, allowing them to view and reason from diverse perspectives. Then, we\nexplore strategies for allocating these reasoning LoRAs and introduce LoraMap,\nan approach to map connections between them. The results of the fact-checking\ntask demonstrate that the performance of LoraMap is superior to LoraHub, an\nexisting method for integrating LoRAs. LoraMap also outperforms with\nsignificantly fewer trainable parameters than LoraConcat, which concatenates\nLoRAs and further fine-tunes them.\n","authors":["Hyeryun Park","Jeongwon Kwak","Dongsuk Jang","Sumin Park","Jinwook Choi"],"pdf_url":"https://arxiv.org/pdf/2408.16264v2.pdf","comment":"17 pages, 12 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.12428v1","updated":"2024-10-16T10:16:34Z","published":"2024-10-16T10:16:34Z","title":"Conformity in Large Language Models","summary":"  The conformity effect describes the tendency of individuals to align their\nresponses with the majority. Studying this bias in large language models (LLMs)\nis crucial, as LLMs are increasingly used in various information-seeking and\ndecision-making tasks as conversation partners to improve productivity. Thus,\nconformity to incorrect responses can compromise their effectiveness. In this\npaper, we adapt psychological experiments to examine the extent of conformity\nin state-of-the-art LLMs. Our findings reveal that all models tested exhibit\nvarying levels of conformity toward the majority, regardless of their initial\nchoice or correctness, across different knowledge domains. Notably, we are the\nfirst to show that LLMs are more likely to conform when they are more uncertain\nin their own prediction. We further explore factors that influence conformity,\nsuch as training paradigms and input characteristics, finding that\ninstruction-tuned models are less susceptible to conformity, while increasing\nthe naturalness of majority tones amplifies conformity. Finally, we propose two\ninterventions--Devil's Advocate and Question Distillation--to mitigate\nconformity, providing insights into building more robust language models.\n","authors":["Xiaochen Zhu","Caiqi Zhang","Tom Stafford","Nigel Collier","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2410.12428v1.pdf","comment":"16 pages (8 pages main body), 14 figures"},{"id":"http://arxiv.org/abs/2410.07129v2","updated":"2024-10-16T10:14:54Z","published":"2024-10-09T17:51:55Z","title":"Mental Disorders Detection in the Era of Large Language Models","summary":"  This paper compares the effectiveness of traditional machine learning\nmethods, encoder-based models, and large language models (LLMs) on the task of\ndetecting depression and anxiety. Five datasets were considered, each differing\nin format and the method used to define the target pathology class. We tested\nAutoML models based on linguistic features, several variations of encoder-based\nTransformers such as BERT, and state-of-the-art LLMs as pathology\nclassification models. The results demonstrated that LLMs outperform\ntraditional methods, particularly on noisy and small datasets where training\nexamples vary significantly in text length and genre. However, psycholinguistic\nfeatures and encoder-based models can achieve performance comparable to\nlanguage models when trained on texts from individuals with clinically\nconfirmed depression, highlighting their potential effectiveness in targeted\nclinical applications.\n","authors":["Gleb Kuzmin","Petr Strepetov","Maksim Stankevich","Artem Shelmanov","Ivan Smirnov"],"pdf_url":"https://arxiv.org/pdf/2410.07129v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11288v2","updated":"2024-10-16T09:59:17Z","published":"2024-06-17T07:51:44Z","title":"MFC-Bench: Benchmarking Multimodal Fact-Checking with Large\n  Vision-Language Models","summary":"  Large vision-language models (LVLMs) have significantly improved multimodal\nreasoning tasks, such as visual question answering and image captioning. These\nmodels embed multimodal facts within their parameters, rather than relying on\nexternal knowledge bases to store factual information explicitly. However, the\ncontent discerned by LVLMs may deviate from actual facts due to inherent bias\nor incorrect inference. To address this issue, we introduce MFC-Bench, a\nrigorous and comprehensive benchmark designed to evaluate the factual accuracy\nof LVLMs across three stages of verdict prediction for MFC: Manipulation,\nOut-of-Context, and Veracity Classification. Through our evaluation on\nMFC-Bench, we benchmarked a dozen diverse and representative LVLMs, uncovering\nthat current models still fall short in multimodal fact-checking and\ndemonstrate insensitivity to various forms of manipulated content. We hope that\nMFC-Bench could raise attention to the trustworthy AI potentially assisted by\nLVLMs in the future. The MFC-Bench and accompanying resources are publicly\naccessible at https://github.com/wskbest/MFC-Bench, contributing to ongoing\nresearch in the multimodal fact-checking field.\n","authors":["Shengkang Wang","Hongzhan Lin","Ziyang Luo","Zhen Ye","Guang Chen","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2406.11288v2.pdf","comment":"28 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.12413v1","updated":"2024-10-16T09:56:01Z","published":"2024-10-16T09:56:01Z","title":"Theoretical Analysis of Hierarchical Language Recognition and Generation\n  by Transformers without Positional Encoding","summary":"  In this study, we provide constructive proof that Transformers can recognize\nand generate hierarchical language efficiently with respect to model size, even\nwithout the need for a specific positional encoding. Specifically, we show that\ncausal masking and a starting token enable Transformers to compute positional\ninformation and depth within hierarchical structures. We demonstrate that\nTransformers without positional encoding can generate hierarchical languages.\nFurthermore, we suggest that explicit positional encoding might have a\ndetrimental effect on generalization with respect to sequence length.\n","authors":["Daichi Hayakawa","Issei Sato"],"pdf_url":"https://arxiv.org/pdf/2410.12413v1.pdf","comment":"55 pages, 11 figures"},{"id":"http://arxiv.org/abs/2409.07891v2","updated":"2024-10-16T09:53:40Z","published":"2024-09-12T09:51:56Z","title":"A corpus-based investigation of pitch contours of monosyllabic words in\n  conversational Taiwan Mandarin","summary":"  In Mandarin, the tonal contours of monosyllabic words produced in isolation\nor in careful speech are characterized by four lexical tones: a high-level tone\n(T1), a rising tone (T2), a dipping tone (T3) and a falling tone (T4). However,\nin spontaneous speech, the actual tonal realization of monosyllabic words can\ndeviate significantly from these canonical tones due to intra-syllabic\nco-articulation and inter-syllabic co-articulation with adjacent tones. In\naddition, Chuang et al. (2024) recently reported that the tonal contours of\ndisyllabic Mandarin words with T2-T4 tone pattern are co-determined by their\nmeanings. Following up on their research, we present a corpus-based\ninvestigation of how the pitch contours of monosyllabic words are realized in\nspontaneous conversational Mandarin, focusing on the effects of contextual\npredictors on the one hand, and the way in words' meanings co-determine pitch\ncontours on the other hand. We analyze the F0 contours of 3824 tokens of 63\ndifferent word types in a spontaneous Taiwan Mandarin corpus, using the\ngeneralized additive (mixed) model to decompose a given observed pitch contour\ninto a set of component pitch contours. We show that the tonal context\nsubstantially modify a word's canonical tone. Once the effect of tonal context\nis controlled for, T2 and T3 emerge as low flat tones, contrasting with T1 as a\nhigh tone, and with T4 as a high-to-mid falling tone. The neutral tone (T0),\nwhich in standard descriptions, is realized based on the preceding tone,\nemerges as a low tone in its own right, modified by the other predictors in the\nsame way as the standard tones T1, T2, T3, and T4. We also show that word, and\neven more so, word sense, co-determine words' F0 contours. Analyses of variable\nimportance using random forests further supported the substantial effect of\ntonal context and an effect of word sense.\n","authors":["Xiaoyun Jin","Mirjam Ernestus","R. Harald Baayen"],"pdf_url":"https://arxiv.org/pdf/2409.07891v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12409v1","updated":"2024-10-16T09:44:38Z","published":"2024-10-16T09:44:38Z","title":"Revealing the Barriers of Language Agents in Planning","summary":"  Autonomous planning has been an ongoing pursuit since the inception of\nartificial intelligence. Based on curated problem solvers, early planning\nagents could deliver precise solutions for specific tasks but lacked\ngeneralization. The emergence of large language models (LLMs) and their\npowerful reasoning capabilities has reignited interest in autonomous planning\nby automatically generating reasonable solutions for given tasks. However,\nprior research and our experiments show that current language agents still lack\nhuman-level planning abilities. Even the state-of-the-art reasoning model,\nOpenAI o1, achieves only 15.6% on one of the complex real-world planning\nbenchmarks. This highlights a critical question: What hinders language agents\nfrom achieving human-level planning? Although existing studies have highlighted\nweak performance in agent planning, the deeper underlying issues and the\nmechanisms and limitations of the strategies proposed to address them remain\ninsufficiently understood. In this work, we apply the feature attribution study\nand identify two key factors that hinder agent planning: the limited role of\nconstraints and the diminishing influence of questions. We also find that\nalthough current strategies help mitigate these challenges, they do not fully\nresolve them, indicating that agents still have a long way to go before\nreaching human-level intelligence.\n","authors":["Jian Xie","Kexun Zhang","Jiangjie Chen","Siyu Yuan","Kai Zhang","Yikai Zhang","Lei Li","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.12409v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2410.12407v1","updated":"2024-10-16T09:42:29Z","published":"2024-10-16T09:42:29Z","title":"Beyond Coarse-Grained Matching in Video-Text Retrieval","summary":"  Video-text retrieval has seen significant advancements, yet the ability of\nmodels to discern subtle differences in captions still requires verification.\nIn this paper, we introduce a new approach for fine-grained evaluation. Our\napproach can be applied to existing datasets by automatically generating hard\nnegative test captions with subtle single-word variations across nouns, verbs,\nadjectives, adverbs, and prepositions. We perform comprehensive experiments\nusing four state-of-the-art models across two standard benchmarks (MSR-VTT and\nVATEX) and two specially curated datasets enriched with detailed descriptions\n(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our\nanalyses show that the current evaluation benchmarks fall short in detecting a\nmodel's ability to perceive subtle single-word differences, 2) our fine-grained\nevaluation highlights the difficulty models face in distinguishing such subtle\nvariations. To enhance fine-grained understanding, we propose a new baseline\nthat can be easily combined with current methods. Experiments on our\nfine-grained evaluations demonstrate that this approach enhances a model's\nability to understand fine-grained differences.\n","authors":["Aozhu Chen","Hazel Doughty","Xirong Li","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12407v1.pdf","comment":"Accepted to ACCV 2024"},{"id":"http://arxiv.org/abs/2410.12406v1","updated":"2024-10-16T09:41:48Z","published":"2024-10-16T09:41:48Z","title":"Nominal Class Assignment in Swahili: A Computational Account","summary":"  We discuss the open question of the relation between semantics and nominal\nclass assignment in Swahili. We approach the problem from a computational\nperspective, aiming first to quantify the extent of this relation, and then to\nexplicate its nature, taking extra care to suppress morphosyntactic confounds.\nOur results are the first of their kind, providing a quantitative evaluation of\nthe semantic cohesion of each nominal class, as well as a nuanced taxonomic\ndescription of its semantic content.\n","authors":["Giada Palmieri","Konstantinos Kogkalidis"],"pdf_url":"https://arxiv.org/pdf/2410.12406v1.pdf","comment":"Tenth Italian Conference on Computational Linguistics (CliC-it-2024)"},{"id":"http://arxiv.org/abs/2410.12405v1","updated":"2024-10-16T09:38:13Z","published":"2024-10-16T09:38:13Z","title":"ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs","summary":"  Large language models (LLMs) have demonstrated impressive capabilities across\nvarious tasks, but their performance is highly sensitive to the prompts\nutilized. This variability poses challenges for accurate assessment and user\nsatisfaction. Current research frequently overlooks instance-level prompt\nvariations and their implications on subjective evaluations. To address these\nshortcomings, we introduce ProSA, a framework designed to evaluate and\ncomprehend prompt sensitivity in LLMs. ProSA incorporates a novel sensitivity\nmetric, PromptSensiScore, and leverages decoding confidence to elucidate\nunderlying mechanisms. Our extensive study, spanning multiple tasks, uncovers\nthat prompt sensitivity fluctuates across datasets and models, with larger\nmodels exhibiting enhanced robustness. We observe that few-shot examples can\nalleviate this sensitivity issue, and subjective evaluations are also\nsusceptible to prompt sensitivities, particularly in complex,\nreasoning-oriented tasks. Furthermore, our findings indicate that higher model\nconfidence correlates with increased prompt robustness. We believe this work\nwill serve as a helpful tool in studying prompt sensitivity of LLMs. The\nproject is released at: https://github.com/open-compass/ProSA .\n","authors":["Jingming Zhuo","Songyang Zhang","Xinyu Fang","Haodong Duan","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12405v1.pdf","comment":"EMNLP 2024, Findings"},{"id":"http://arxiv.org/abs/2308.01472v2","updated":"2024-10-16T09:37:11Z","published":"2023-08-02T23:39:29Z","title":"Reverse Stable Diffusion: What prompt was used to generate this image?","summary":"  Text-to-image diffusion models have recently attracted the interest of many\nresearchers, and inverting the diffusion process can play an important role in\nbetter understanding the generative process and how to engineer prompts in\norder to obtain the desired images. To this end, we study the task of\npredicting the prompt embedding given an image generated by a generative\ndiffusion model. We consider a series of white-box and black-box models (with\nand without access to the weights of the diffusion network) to deal with the\nproposed task. We propose a novel learning framework comprising a joint prompt\nregression and multi-label vocabulary classification objective that generates\nimproved prompts. To further improve our method, we employ a curriculum\nlearning procedure that promotes the learning of image-prompt pairs with lower\nlabeling noise (i.e. that are better aligned). We conduct experiments on the\nDiffusionDB data set, predicting text prompts from images generated by Stable\nDiffusion. In addition, we make an interesting discovery: training a diffusion\nmodel on the prompt generation task can make the model generate images that are\nmuch better aligned with the input prompts, when the model is directly reused\nfor text-to-image generation. Our code is publicly available for download at\nhttps://github.com/CroitoruAlin/Reverse-Stable-Diffusion.\n","authors":["Florinel-Alin Croitoru","Vlad Hondru","Radu Tudor Ionescu","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2308.01472v2.pdf","comment":"Accepted for publication in Computer Vision and Image Understanding"},{"id":"http://arxiv.org/abs/2405.00716v4","updated":"2024-10-16T09:18:58Z","published":"2024-04-25T15:51:06Z","title":"Large Language Models in the Clinic: A Comprehensive Benchmark","summary":"  The adoption of large language models (LLMs) to assist clinicians has\nattracted remarkable attention. Existing works mainly adopt the close-ended\nquestion-answering (QA) task with answer options for evaluation. However, many\nclinical decisions involve answering open-ended questions without pre-set\noptions. To better understand LLMs in the clinic, we construct a benchmark\nClinicBench. We first collect eleven existing datasets covering diverse\nclinical language generation, understanding, and reasoning tasks. Furthermore,\nwe construct six novel datasets and clinical tasks that are complex but common\nin real-world practice, e.g., open-ended decision-making, long document\nprocessing, and emerging drug analysis. We conduct an extensive evaluation of\ntwenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite\nmedical experts to evaluate the clinical usefulness of LLMs. The benchmark data\nis available at https://github.com/AI-in-Health/ClinicBench.\n","authors":["Fenglin Liu","Zheng Li","Hongjian Zhou","Qingyu Yin","Jingfeng Yang","Xianfeng Tang","Chen Luo","Ming Zeng","Haoming Jiang","Yifan Gao","Priyanka Nigam","Sreyashi Nag","Bing Yin","Yining Hua","Xuan Zhou","Omid Rohanian","Anshul Thakur","Lei Clifton","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2405.00716v4.pdf","comment":"Accepted at EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.12391v1","updated":"2024-10-16T09:18:39Z","published":"2024-10-16T09:18:39Z","title":"Tracking Universal Features Through Fine-Tuning and Model Merging","summary":"  We study how features emerge, disappear, and persist across models fine-tuned\non different domains of text. More specifically, we start from a base one-layer\nTransformer language model that is trained on a combination of the BabyLM\ncorpus, and a collection of Python code from The Stack. This base model is\nadapted to two new domains of text: TinyStories, and the Lua programming\nlanguage, respectively; and then these two models are merged using these two\nmodels using spherical linear interpolation. Our exploration aims to provide\ndeeper insights into the stability and transformation of features across\ntypical transfer-learning scenarios using small-scale models and sparse\nauto-encoders.\n","authors":["Niels Horn","Desmond Elliott"],"pdf_url":"https://arxiv.org/pdf/2410.12391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12388v1","updated":"2024-10-16T09:13:23Z","published":"2024-10-16T09:13:23Z","title":"Prompt Compression for Large Language Models: A Survey","summary":"  Leveraging large language models (LLMs) for complex natural language tasks\ntypically requires long-form prompts to convey detailed requirements and\ninformation, which results in increased memory usage and inference costs. To\nmitigate these challenges, multiple efficient methods have been proposed, with\nprompt compression gaining significant research interest. This survey provides\nan overview of prompt compression techniques, categorized into hard prompt\nmethods and soft prompt methods. First, the technical approaches of these\nmethods are compared, followed by an exploration of various ways to understand\ntheir mechanisms, including the perspectives of attention optimization,\nParameter-Efficient Fine-Tuning (PEFT), modality fusion, and new synthetic\nlanguage. We also examine the downstream adaptations of various prompt\ncompression techniques. Finally, the limitations of current prompt compression\nmethods are analyzed, and several future directions are outlined, such as\noptimizing the compression encoder, combining hard and soft prompts methods,\nand leveraging insights from multimodality.\n","authors":["Zongqian Li","Yinhong Liu","Yixuan Su","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2410.12388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03514v2","updated":"2024-10-16T08:59:22Z","published":"2024-03-06T07:43:43Z","title":"CLongEval: A Chinese Benchmark for Evaluating Long-Context Large\n  Language Models","summary":"  Developing Large Language Models (LLMs) with robust long-context capabilities\nhas been the recent research focus, resulting in the emergence of long-context\nLLMs proficient in Chinese. However, the evaluation of these models remains\nunderdeveloped due to a lack of benchmarks. To address this gap, we present\nCLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs.\nCLongEval is characterized by three key features: (1) Sufficient data volume,\ncomprising 7 distinct tasks and 7,267 examples; (2) Broad applicability,\naccommodating to models with context windows size from 1K to 100K; (3) High\nquality, with over 2,000 manually annotated question-answer pairs in addition\nto the automatically constructed labels. With CLongEval, we undertake a\ncomprehensive assessment of 6 open-source long-context LLMs and 2 leading\ncommercial counterparts that feature both long-context abilities and\nproficiency in Chinese. We also provide in-depth analysis based on the\nempirical results, trying to shed light on the critical capabilities that\npresent challenges in long-context settings. The dataset, evaluation scripts,\nand model outputs are released.\n","authors":["Zexuan Qiu","Jingjing Li","Shijue Huang","Xiaoqi Jiao","Wanjun Zhong","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2403.03514v2.pdf","comment":"Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.12380v1","updated":"2024-10-16T08:55:49Z","published":"2024-10-16T08:55:49Z","title":"Evaluation of Attribution Bias in Retrieval-Augmented Large Language\n  Models","summary":"  Attributing answers to source documents is an approach used to enhance the\nverifiability of a model's output in retrieval augmented generation (RAG).\nPrior work has mainly focused on improving and evaluating the attribution\nquality of large language models (LLMs) in RAG, but this may come at the\nexpense of inducing biases in the attribution of answers. We define and examine\ntwo aspects in the evaluation of LLMs in RAG pipelines, namely attribution\nsensitivity and bias with respect to authorship information. We explicitly\ninform an LLM about the authors of source documents, instruct it to attribute\nits answers, and analyze (i) how sensitive the LLM's output is to the author of\nsource documents, and (ii) whether the LLM exhibits a bias towards\nhuman-written or AI-generated source documents. We design an experimental setup\nin which we use counterfactual evaluation to study three LLMs in terms of their\nattribution sensitivity and bias in RAG pipelines. Our results show that adding\nauthorship information to source documents can significantly change the\nattribution quality of LLMs by 3% to 18%. Moreover, we show that LLMs can have\nan attribution bias towards explicit human authorship, which can serve as a\ncompeting hypothesis for findings of prior work that shows that LLM-generated\ncontent may be preferred over human-written contents. Our findings indicate\nthat metadata of source documents can influence LLMs' trust, and how they\nattribute their answers. Furthermore, our research highlights attribution bias\nand sensitivity as a novel aspect of brittleness in LLMs.\n","authors":["Amin Abolghasemi","Leif Azzopardi","Seyyed Hadi Hashemi","Maarten de Rijke","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2410.12380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06499v4","updated":"2024-10-16T08:53:23Z","published":"2023-12-11T16:22:37Z","title":"TaCo: Targeted Concept Erasure Prevents Non-Linear Classifiers From\n  Detecting Protected Attributes","summary":"  Ensuring fairness in NLP models is crucial, as they often encode sensitive\nattributes like gender and ethnicity, leading to biased outcomes. Current\nconcept erasure methods attempt to mitigate this by modifying final latent\nrepresentations to remove sensitive information without retraining the entire\nmodel. However, these methods typically rely on linear classifiers, which leave\nmodels vulnerable to non-linear adversaries capable of recovering sensitive\ninformation.\n  We introduce Targeted Concept Erasure (TaCo), a novel approach that removes\nsensitive information from final latent representations, ensuring fairness even\nagainst non-linear classifiers. Our experiments show that TaCo outperforms\nstate-of-the-art methods, achieving greater reductions in the prediction\naccuracy of sensitive attributes by non-linear classifier while preserving\noverall task performance. Code is available on\nhttps://github.com/fanny-jourdan/TaCo.\n","authors":["Fanny Jourdan","Louis Béthune","Agustin Picard","Laurent Risser","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2312.06499v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12377v1","updated":"2024-10-16T08:49:17Z","published":"2024-10-16T08:49:17Z","title":"HerO at AVeriTeC: The Herd of Open Large Language Models for Verifying\n  Real-World Claims","summary":"  To tackle the AVeriTeC shared task hosted by the FEVER-24, we introduce a\nsystem that only employs publicly available large language models (LLMs) for\neach step of automated fact-checking, dubbed the Herd of Open LLMs for\nverifying real-world claims (HerO). HerO employs multiple LLMs for each step of\nautomated fact-checking. For evidence retrieval, a language model is used to\nenhance a query by generating hypothetical fact-checking documents. We prompt\npretrained and fine-tuned LLMs for question generation and veracity prediction\nby crafting prompts with retrieved in-context samples. HerO achieved 2nd place\non the leaderboard with the AVeriTeC score of 0.57, suggesting the potential of\nopen LLMs for verifying real-world claims. For future research, we make our\ncode publicly available at https://github.com/ssu-humane/HerO.\n","authors":["Yejun Yoon","Jaeyoon Jung","Seunghyun Yoon","Kunwoo Park"],"pdf_url":"https://arxiv.org/pdf/2410.12377v1.pdf","comment":"A system description paper for the AVeriTeC shared task, hosted by\n  the seventh FEVER workshop (co-located with EMNLP 2024)"},{"id":"http://arxiv.org/abs/2410.12375v1","updated":"2024-10-16T08:46:26Z","published":"2024-10-16T08:46:26Z","title":"PRefLexOR: Preference-based Recursive Language Modeling for Exploratory\n  Optimization of Reasoning and Agentic Thinking","summary":"  PRefLexOR (Preference-based Recursive Language Modeling for Exploratory\nOptimization of Reasoning) combines preference optimization with concepts from\nReinforcement Learning to enable models to self-teach through iterative\nreasoning improvements. We propose a recursive learning approach that engages\nthe model in multi-step reasoning, revisiting, and refining intermediate steps\nbefore producing a final output in training and inference phases. Through\nmultiple training stages, the model first learns to align its reasoning with\naccurate decision paths by optimizing the log odds between preferred and\nnon-preferred responses. During this process, PRefLexOR builds a dynamic\nknowledge graph by generating questions from random text chunks and\nretrieval-augmentation to contextualize relevant details from the entire\ntraining corpus. In the second stage, preference optimization enhances model\nperformance by using rejection sampling to fine-tune reasoning quality by\ncontinually producing in-situ training data while masking the reasoning steps.\nRecursive optimization within a thinking token framework introduces iterative\nfeedback loops, where the model refines reasoning, achieving deeper coherence,\nconsistency, and adaptability. Implemented in small language models with only 3\nbillion parameters, we should that even tiny models can iteratively teach\nthemselves to reason with greater depth and reflectivity. Our implementation is\nstraightforward and can be incorporated into any existing pretrained LLM. We\nfocus our examples on applications in biological materials science and\ndemonstrate the method in a variety of case studies that range from in-domain\nto cross-domain applications. Using reasoning strategies that include thinking\nand reflection modalities we build a multi-agent recursive self-improving\ninference approach to successively improve responses via repeated sampling in\ninference time.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2410.12375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12361v1","updated":"2024-10-16T08:24:09Z","published":"2024-10-16T08:24:09Z","title":"Proactive Agent: Shifting LLM Agents from Reactive Responses to Active\n  Assistance","summary":"  Agents powered by large language models have shown remarkable abilities in\nsolving complex tasks. However, most agent systems remain reactive, limiting\ntheir effectiveness in scenarios requiring foresight and autonomous\ndecision-making. In this paper, we tackle the challenge of developing proactive\nagents capable of anticipating and initiating tasks without explicit human\ninstructions. We propose a novel data-driven approach for this problem.\nFirstly, we collect real-world human activities to generate proactive task\npredictions. These predictions are then labeled by human annotators as either\naccepted or rejected. The labeled data is used to train a reward model that\nsimulates human judgment and serves as an automatic evaluator of the\nproactiveness of LLM agents. Building on this, we develop a comprehensive data\ngeneration pipeline to create a diverse dataset, ProactiveBench, containing\n6,790 events. Finally, we demonstrate that fine-tuning models with the proposed\nProactiveBench can significantly elicit the proactiveness of LLM agents.\nExperimental results show that our fine-tuned model achieves an F1-Score of\n66.47% in proactively offering assistance, outperforming all open-source and\nclose-source models. These results highlight the potential of our method in\ncreating more proactive and effective agent systems, paving the way for future\nadvancements in human-agent collaboration.\n","authors":["Yaxi Lu","Shenzhi Yang","Cheng Qian","Guirong Chen","Qinyu Luo","Yesai Wu","Huadong Wang","Xin Cong","Zhong Zhang","Yankai Lin","Weiwen Liu","Yasheng Wang","Zhiyuan Liu","Fangming Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.12361v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.09822v2","updated":"2024-10-16T08:20:43Z","published":"2024-09-15T18:43:11Z","title":"Causal Inference with Large Language Model: A Survey","summary":"  Causal inference has been a pivotal challenge across diverse domains such as\nmedicine and economics, demanding a complicated integration of human knowledge,\nmathematical reasoning, and data mining capabilities. Recent advancements in\nnatural language processing (NLP), particularly with the advent of large\nlanguage models (LLMs), have introduced promising opportunities for traditional\ncausal inference tasks. This paper reviews recent progress in applying LLMs to\ncausal inference, encompassing various tasks spanning different levels of\ncausation. We summarize the main causal problems and approaches, and present a\ncomparison of their evaluation results in different causal scenarios.\nFurthermore, we discuss key findings and outline directions for future\nresearch, underscoring the potential implications of integrating LLMs in\nadvancing causal inference methodologies.\n","authors":["Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2409.09822v2.pdf","comment":"12 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2312.03003v3","updated":"2024-10-16T08:15:53Z","published":"2023-12-04T06:13:35Z","title":"Explore, Select, Derive, and Recall: Augmenting LLM with Human-like\n  Memory for Mobile Task Automation","summary":"  The advent of large language models (LLMs) has opened up new opportunities in\nthe field of mobile task automation. Their superior language understanding and\nreasoning capabilities allow users to automate complex and repetitive tasks.\nHowever, due to the inherent unreliability and high operational cost of LLMs,\ntheir practical applicability is quite limited. To address these issues, this\npaper introduces MobileGPT, an innovative LLM-based mobile task automator\nequipped with a human-like app memory. MobileGPT emulates the cognitive process\nof humans interacting with a mobile app -- explore, select, derive, and recall.\nThis approach allows for a more precise and efficient learning of a task's\nprocedure by breaking it down into smaller, modular sub-tasks that can be\nre-used, re-arranged, and adapted for various objectives. We implement\nMobileGPT using online LLMs services (GPT-3.5 and GPT-4) and evaluate its\nperformance on a dataset of 185 tasks across 18 mobile apps. The results\nindicate that MobileGPT can automate and learn new tasks with 82.7% accuracy,\nand is able to adapt them to different contexts with near perfect (98.75%)\naccuracy while reducing both latency and cost by 62.5% and 68.8%, respectively,\ncompared to the GPT-4 powered baseline.\n","authors":["Sunjae Lee","Junyoung Choi","Jungjae Lee","Munim Hasan Wasi","Hojun Choi","Steven Y. Ko","Sangeun Oh","Insik Shin"],"pdf_url":"https://arxiv.org/pdf/2312.03003v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12350v1","updated":"2024-10-16T08:13:54Z","published":"2024-10-16T08:13:54Z","title":"GECTurk WEB: An Explainable Online Platform for Turkish Grammatical\n  Error Detection and Correction","summary":"  Sophisticated grammatical error detection/correction tools are available for\na small set of languages such as English and Chinese. However, it is not\nstraightforward -- if not impossible -- to adapt them to morphologically rich\nlanguages with complex writing rules like Turkish which has more than 80\nmillion speakers. Even though several tools exist for Turkish, they primarily\nfocus on spelling errors rather than grammatical errors and lack features such\nas web interfaces, error explanations and feedback mechanisms. To fill this\ngap, we introduce GECTurk WEB, a light, open-source, and flexible web-based\nsystem that can detect and correct the most common forms of Turkish writing\nerrors, such as the misuse of diacritics, compound and foreign words, pronouns,\nlight verbs along with spelling mistakes. Our system provides native speakers\nand second language learners an easily accessible tool to detect/correct such\nmistakes and also to learn from their mistakes by showing the explanation for\nthe violated rule(s). The proposed system achieves 88,3 system usability score,\nand is shown to help learn/remember a grammatical rule (confirmed by 80% of the\nparticipants). The GECTurk WEB is available both as an offline tool at\nhttps://github.com/GGLAB-KU/gecturkweb or online at www.gecturk.net.\n","authors":["Ali Gebeşçe","Gözde Gül Şahin"],"pdf_url":"https://arxiv.org/pdf/2410.12350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01308v2","updated":"2024-10-16T08:08:51Z","published":"2024-08-02T15:00:05Z","title":"Reconsidering Degeneration of Token Embeddings with Definitions for\n  Encoder-based Pre-trained Language Models","summary":"  Learning token embeddings based on token co-occurrence statistics has proven\neffective for both pre-training and fine-tuning in natural language processing.\nHowever, recent studies have pointed out that the distribution of learned\nembeddings degenerates into anisotropy (i.e., non-uniform distribution), and\neven pre-trained language models (PLMs) suffer from a loss of semantics-related\ninformation in embeddings for low-frequency tokens. This study first analyzes\nthe fine-tuning dynamics of encoder-based PLMs and demonstrates their\nrobustness against degeneration. On the basis of this analysis, we propose\nDefinitionEMB, a method that utilizes definitions to re-construct isotropically\ndistributed and semantics-related token embeddings for encoder-based PLMs while\nmaintaining original robustness during fine-tuning. Our experiments demonstrate\nthe effectiveness of leveraging definitions from Wiktionary to re-construct\nsuch embeddings for two encoder-based PLMs: RoBERTa-base and BART-large.\nFurthermore, the re-constructed embeddings for low-frequency tokens improve the\nperformance of these models across various GLUE and four text summarization\ndatasets.\n","authors":["Ying Zhang","Dongyuan Li","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2408.01308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07109v2","updated":"2024-10-16T08:06:22Z","published":"2024-10-09T17:45:47Z","title":"I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in\n  Multi-Agent Settings with Social Hierarchy","summary":"  As Large Language Model (LLM)-based agents become increasingly autonomous and\nwill more freely interact with each other, studying interactions between them\nbecomes crucial to anticipate emergent phenomena and potential risks. Drawing\ninspiration from the widely popular Stanford Prison Experiment, we contribute\nto this line of research by studying interaction patterns of LLM agents in a\ncontext characterized by strict social hierarchy. We do so by specifically\nstudying two types of phenomena: persuasion and anti-social behavior in\nsimulated scenarios involving a guard and a prisoner agent who seeks to achieve\na specific goal (i.e., obtaining additional yard time or escape from prison).\nLeveraging 200 experimental scenarios for a total of 2,000 machine-machine\nconversations across five different popular LLMs, we provide a set of\nnoteworthy findings. We first document how some models consistently fail in\ncarrying out a conversation in our multi-agent setup where power dynamics are\nat play. Then, for the models that were able to engage in successful\ninteractions, we empirically show how the goal that an agent is set to achieve\nimpacts primarily its persuasiveness, while having a negligible effect with\nrespect to the agent's anti-social behavior. Third, we highlight how agents'\npersonas, and particularly the guard's personality, drive both the likelihood\nof successful persuasion from the prisoner and the emergence of anti-social\nbehaviors. Fourth, we show that even without explicitly prompting for specific\npersonalities, anti-social behavior emerges by simply assigning agents' roles.\nThese results bear implications for the development of interactive LLM agents\nas well as the debate on their societal impact.\n","authors":["Gian Maria Campedelli","Nicolò Penzo","Massimo Stefan","Roberto Dessì","Marco Guerini","Bruno Lepri","Jacopo Staiano"],"pdf_url":"https://arxiv.org/pdf/2410.07109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15729v3","updated":"2024-10-16T08:04:46Z","published":"2024-02-24T05:40:01Z","title":"How Do Humans Write Code? Large Models Do It the Same Way Too","summary":"  Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought\n(CoT) as the most popular method in Large Language Models (LLMs) mathematical\nreasoning tasks by utilizing external tool calls to circumvent computational\nerrors. However, our evaluation of the GPT-4 and Llama series reveals that\nusing PoT introduces more reasoning errors, such as incorrect formulas or\nflawed logic, compared to CoT. To address this issue, we propose Human-Think\nLanguage (HTL), which leverages a suite of strategies that help integrate PoT\nand CoT, encompassing: (1) a new generation paradigm that uses full CoT\nreasoning to control code generation. (2) Focus Attention, that directs model\nattention to the CoT reasoning during PoT to generate more logical code. (3)\nreinforcement learning that utilizes the accuracy of both CoT and PoT responses\nas rewards to prevent repetitive reasoning steps in LLMs when solving difficult\nmath problems. Our method achieves an average improvement of 6.5% on the\nLlama-Base model and 4.3% on the Mistral-Base model across 8 mathematical\ncalculation datasets. It also shows significant effectiveness on five\nout-of-domain datasets by controlling the model's information flow, exhibiting\nstrong transferability. Additionally, HTL shows the most significant\nimprovement in non-mathematical natural language inference task, contributing\nto a unified reasoning task framework\n","authors":["Long Li","Xuzheng He","Haozhe Wang","Linlin Wang","Liang He"],"pdf_url":"https://arxiv.org/pdf/2402.15729v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12341v1","updated":"2024-10-16T08:02:48Z","published":"2024-10-16T08:02:48Z","title":"A linguistic analysis of undesirable outcomes in the era of generative\n  AI","summary":"  Recent research has focused on the medium and long-term impacts of generative\nAI, posing scientific and societal challenges mainly due to the detection and\nreliability of machine-generated information, which is projected to form the\nmajor content on the Web soon. Prior studies show that LLMs exhibit a lower\nperformance in generation tasks (model collapse) as they undergo a fine-tuning\nprocess across multiple generations on their own generated content\n(self-consuming loop). In this paper, we present a comprehensive simulation\nframework built upon the chat version of LLama2, focusing particularly on the\nlinguistic aspects of the generated content, which has not been fully examined\nin existing studies. Our results show that the model produces less lexical rich\ncontent across generations, reducing diversity. The lexical richness has been\nmeasured using the linguistic measures of entropy and TTR as well as\ncalculating the POSTags frequency. The generated content has also been examined\nwith an $n$-gram analysis, which takes into account the word order, and\nsemantic networks, which consider the relation between different words. These\nfindings suggest that the model collapse occurs not only by decreasing the\ncontent diversity but also by distorting the underlying linguistic patterns of\nthe generated text, which both highlight the critical importance of carefully\nchoosing and curating the initial input text, which can alleviate the model\ncollapse problem. Furthermore, we conduct a qualitative analysis of the\nfine-tuned models of the pipeline to compare their performances on generic NLP\ntasks to the original model. We find that autophagy transforms the initial\nmodel into a more creative, doubtful and confused one, which might provide\ninaccurate answers and include conspiracy theories in the model responses,\nspreading false and biased information on the Web.\n","authors":["Daniele Gambetta","Gizem Gezici","Fosca Giannotti","Dino Pedreschi","Alistair Knott","Luca Pappalardo"],"pdf_url":"https://arxiv.org/pdf/2410.12341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17753v2","updated":"2024-10-16T07:59:39Z","published":"2024-06-25T17:40:47Z","title":"Measuring and Benchmarking Large Language Models' Capabilities to\n  Generate Persuasive Language","summary":"  We are exposed to much information trying to influence us, such as teaser\nmessages, debates, politically framed news, and propaganda - all of which use\npersuasive language. With the recent interest in Large Language Models (LLMs),\nwe study the ability of LLMs to produce persuasive text. As opposed to prior\nwork which focuses on particular domains or types of persuasion, we conduct a\ngeneral study across various domains to measure and benchmark to what degree\nLLMs produce persuasive language - both when explicitly instructed to rewrite\ntext to be more or less persuasive and when only instructed to paraphrase. We\nconstruct the new dataset Persuasive-Pairs of pairs of a short text and its\nrewrite by an LLM to amplify or diminish persuasive language. We multi-annotate\nthe pairs on a relative scale for persuasive language: a valuable resource in\nitself, and for training a regression model to score and benchmark persuasive\nlanguage, including for new LLMs across domains. In our analysis, we find that\ndifferent 'personas' in LLaMA3's system prompt change persuasive language\nsubstantially, even when only instructed to paraphrase.\n","authors":["Amalie Brogaard Pauli","Isabelle Augenstein","Ira Assent"],"pdf_url":"https://arxiv.org/pdf/2406.17753v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12329v1","updated":"2024-10-16T07:49:13Z","published":"2024-10-16T07:49:13Z","title":"Understanding the Role of LLMs in Multimodal Evaluation Benchmarks","summary":"  The rapid advancement of Multimodal Large Language Models (MLLMs) has been\naccompanied by the development of various benchmarks to evaluate their\ncapabilities. However, the true nature of these evaluations and the extent to\nwhich they assess multimodal reasoning versus merely leveraging the underlying\nLarge Language Model (LLM) backbone remain unclear. This paper presents a\ncomprehensive investigation into the role of LLM backbones in MLLM evaluation,\nfocusing on two critical aspects: the degree to which current benchmarks truly\nassess multimodal reasoning and the influence of LLM prior knowledge on\nperformance. Specifically, we introduce a modified evaluation protocol to\ndisentangle the contributions of the LLM backbone from multimodal integration,\nand an automatic knowledge identification technique for diagnosing whether LLMs\nequip the necessary knowledge for corresponding multimodal questions. Our study\nencompasses four diverse MLLM benchmarks and eight state-of-the-art MLLMs. Key\nfindings reveal that some benchmarks allow high performance even without visual\ninputs and up to 50\\% of error rates can be attributed to insufficient world\nknowledge in the LLM backbone, indicating a heavy reliance on language\ncapabilities. To address knowledge deficiencies, we propose a knowledge\naugmentation pipeline that achieves significant performance gains, with\nimprovements of up to 60\\% on certain datasets, resulting in a approximately 4x\nincrease in performance. Our work provides crucial insights into the role of\nthe LLM backbone in MLLMs, and highlights the need for more nuanced\nbenchmarking approaches.\n","authors":["Botian Jiang","Lei Li","Xiaonan Li","Zhaowei Li","Xiachong Feng","Lingpeng Kong","Qi Liu","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.12329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12327v1","updated":"2024-10-16T07:47:45Z","published":"2024-10-16T07:47:45Z","title":"Neuron-based Personality Trait Induction in Large Language Models","summary":"  Large language models (LLMs) have become increasingly proficient at\nsimulating various personality traits, an important capability for supporting\nrelated applications (e.g., role-playing). To further improve this capacity, in\nthis paper, we present a neuron-based approach for personality trait induction\nin LLMs, with three major technical contributions. First, we construct\nPersonalityBench, a large-scale dataset for identifying and evaluating\npersonality traits in LLMs. This dataset is grounded in the Big Five\npersonality traits from psychology and is designed to assess the generative\ncapabilities of LLMs towards specific personality traits. Second, by leveraging\nPersonalityBench, we propose an efficient method for identifying\npersonality-related neurons within LLMs by examining the opposite aspects of a\ngiven trait. Third, we develop a simple yet effective induction method that\nmanipulates the values of these identified personality-related neurons. This\nmethod enables fine-grained control over the traits exhibited by LLMs without\ntraining and modifying model parameters. Extensive experiments validate the\nefficacy of our neuron identification and trait induction methods. Notably, our\napproach achieves comparable performance as fine-tuned models, offering a more\nefficient and flexible solution for personality trait induction in LLMs. We\nprovide access to all the mentioned resources at\nhttps://github.com/RUCAIBox/NPTI.\n","authors":["Jia Deng","Tianyi Tang","Yanbin Yin","Wenhao Yang","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2410.12327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12325v1","updated":"2024-10-16T07:45:56Z","published":"2024-10-16T07:45:56Z","title":"Optimizing Low-Resource Language Model Training: Comprehensive Analysis\n  of Multi-Epoch, Multi-Lingual, and Two-Stage Approaches","summary":"  In this paper, we address the challenge of optimizing training setups for\nLarge Language Models (LLMs) of low-resource language with a limited amount of\ncorpus. Existing works adopt multi-epoch, multi-lingual, and two-stage training\nto utilize the limited target language corpus efficiently. However, there is\nstill a lack of understanding about the optimal hyperparameter setups for\ncombining these three approaches to train LLMs. We exhaustively explore\ntraining setups for low-resource language LLM, combining these three\napproaches, and found the following insights for efficiently reducing the cost\nof hyperparameter search: (1) As the amount of target language corpus\ndecreases, the optimal training approach shifts from monolingual single-stage\ntraining to multi-lingual two-stage training at a compute budget dependent\nthreshold. (2) The optimal model scale remains stable regardless of the amount\nof target language corpus, allowing the use of the compute-optimal scale of\nmonolingual training. (3) The optimal number of epochs can be extrapolated from\nsmaller-scale experiments to larger scale using our proposed model. Also, we\nprovide evidence that, in single-stage training, the target language validation\nloss follows a power law with respect to the target language ratio, with an\nexponent independent of the amount of data, model scale, and language pair.\n","authors":["Kosuke Akimoto","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2410.12325v1.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.12323v1","updated":"2024-10-16T07:44:28Z","published":"2024-10-16T07:44:28Z","title":"Reversal of Thought: Enhancing Large Language Models with\n  Preference-Guided Reverse Reasoning Warm-up","summary":"  Large language models (LLMs) have shown remarkable performance in reasoning\ntasks but face limitations in mathematical and complex logical reasoning.\nExisting methods to improve LLMs' logical capabilities either involve traceable\nor verifiable logical sequences that generate more reliable responses by\nconstructing logical structures yet increase computational costs, or introduces\nrigid logic template rules, reducing flexibility. In this paper, we propose\nReversal of Thought (RoT), a novel framework aimed at enhancing the logical\nreasoning abilities of LLMs. RoT utilizes a Preference-Guided Reverse Reasoning\nwarm-up strategy, which integrates logical symbols for pseudocode planning\nthrough meta-cognitive mechanisms and pairwise preference self-evaluation to\ngenerate task-specific prompts solely through demonstrations, aligning with\nLLMs' cognitive preferences shaped by Reinforcement Learning with Human\nFeedback (RLHF). Through reverse reasoning, we ultilize a Cognitive Preference\nManager to assess knowledge boundaries and further expand LLMs' reasoning\ncapabilities by aggregating solution logic for known tasks and stylistic\ntemplates for unknown tasks. Experiments across various tasks demonstrate that\nRoT surpasses existing baselines in both reasoning accuracy and efficiency.\n","authors":["Jiahao Yuan","Dehui Du","Hao Zhang","Zixiang Di","Usman Naseem"],"pdf_url":"https://arxiv.org/pdf/2410.12323v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12311v1","updated":"2024-10-16T07:24:28Z","published":"2024-10-16T07:24:28Z","title":"Open Domain Question Answering with Conflicting Contexts","summary":"  Open domain question answering systems frequently rely on information\nretrieved from large collections of text (such as the Web) to answer questions.\nHowever, such collections of text often contain conflicting information, and\nindiscriminately depending on this information may result in untruthful and\ninaccurate answers. To understand the gravity of this problem, we collect a\nhuman-annotated dataset, Question Answering with Conflicting Contexts (QACC),\nand find that as much as 25% of unambiguous, open domain questions can lead to\nconflicting contexts when retrieved using Google Search. We evaluate and\nbenchmark three powerful Large Language Models (LLMs) with our dataset QACC and\ndemonstrate their limitations in effectively addressing questions with\nconflicting information. To explore how humans reason through conflicting\ncontexts, we request our annotators to provide explanations for their\nselections of correct answers. We demonstrate that by finetuning LLMs to\nexplain their answers, we can introduce richer information into their training\nthat guide them through the process of reasoning with conflicting contexts.\n","authors":["Siyi Liu","Qiang Ning","Kishaloy Halder","Wei Xiao","Zheng Qi","Phu Mon Htut","Yi Zhang","Neha Anna John","Bonan Min","Yassine Benajiba","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2410.12311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07167v2","updated":"2024-10-16T07:23:03Z","published":"2024-10-09T17:59:04Z","title":"Deciphering Cross-Modal Alignment in Large Vision-Language Models with\n  Modality Integration Rate","summary":"  We present the Modality Integration Rate (MIR), an effective, robust, and\ngeneralized metric to indicate the multi-modal pre-training quality of Large\nVision Language Models (LVLMs). Large-scale pre-training plays a critical role\nin building capable LVLMs, while evaluating its training quality without the\ncostly supervised fine-tuning stage is under-explored. Loss, perplexity, and\nin-context evaluation results are commonly used pre-training metrics for Large\nLanguage Models (LLMs), while we observed that these metrics are less\nindicative when aligning a well-trained LLM with a new modality. Due to the\nlack of proper metrics, the research of LVLMs in the critical pre-training\nstage is hindered greatly, including the training data choice, efficient module\ndesign, etc. In this paper, we propose evaluating the pre-training quality from\nthe inter-modal distribution distance perspective and present MIR, the Modality\nIntegration Rate, which is 1) \\textbf{Effective} to represent the pre-training\nquality and show a positive relation with the benchmark performance after\nsupervised fine-tuning. 2) \\textbf{Robust} toward different training/evaluation\ndata. 3) \\textbf{Generalize} across training configurations and architecture\nchoices. We conduct a series of pre-training experiments to explore the\neffectiveness of MIR and observe satisfactory results that MIR is indicative\nabout training data selection, training strategy schedule, and model\narchitecture design to get better pre-training results. We hope MIR could be a\nhelpful metric for building capable LVLMs and inspire the following research\nabout modality alignment in different areas. Our code is at:\nhttps://github.com/shikiw/Modality-Integration-Rate.\n","authors":["Qidong Huang","Xiaoyi Dong","Pan Zhang","Yuhang Zang","Yuhang Cao","Jiaqi Wang","Dahua Lin","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.07167v2.pdf","comment":"Project page: https://github.com/shikiw/Modality-Integration-Rate"},{"id":"http://arxiv.org/abs/2406.13993v2","updated":"2024-10-16T07:14:20Z","published":"2024-06-20T04:44:20Z","title":"Exploring Changes in Nation Perception with Nationality-Assigned\n  Personas in LLMs","summary":"  Persona assignment has become a common strategy for customizing LLM use to\nparticular tasks and contexts. In this study, we explore how evaluation of\ndifferent nations change when LLMs are assigned specific nationality personas.\nWe assign 193 different nationality personas (e.g., an American person) to four\nLLMs and examine how the LLM evaluations (or ''perceptions'')of countries\nchange. We find that all LLM-persona combinations tend to favor Western\nEuropean nations, though nation-personas push LLM behaviors to focus more on\nand treat the nation-persona's own region more favorably. Eastern European,\nLatin American, and African nations are treated more negatively by different\nnationality personas. We additionally find that evaluations by nation-persona\nLLMs of other nations correlate with human survey responses but fail to match\nthe values closely. Our study provides insight into how biases and stereotypes\nare realized within LLMs when adopting different national personas. In line\nwith the ''Blueprint for an AI Bill of Rights'', our findings underscore the\ncritical need for developing mechanisms to ensure that LLM outputs promote\nfairness and avoid over-generalization.\n","authors":["Mahammed Kamruzzaman","Gene Louis Kim"],"pdf_url":"https://arxiv.org/pdf/2406.13993v2.pdf","comment":"Pre-print, Under review"},{"id":"http://arxiv.org/abs/2410.05581v2","updated":"2024-10-16T07:07:20Z","published":"2024-10-08T00:37:16Z","title":"Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes\n  Fail to Improve?","summary":"  In the last decade, the generalization and adaptation abilities of deep\nlearning models were typically evaluated on fixed training and test\ndistributions. Contrary to traditional deep learning, large language models\n(LLMs) are (i) even more overparameterized, (ii) trained on unlabeled text\ncorpora curated from the Internet with minimal human intervention, and (iii)\ntrained in an online fashion. These stark contrasts prevent researchers from\ntransferring lessons learned on model generalization and adaptation in deep\nlearning contexts to LLMs. To this end, our short paper introduces empirical\nobservations that aim to shed light on further training of already pretrained\nlanguage models. Specifically, we demonstrate that training a model on a text\ndomain could degrade its perplexity on the test portion of the same domain. We\nobserve with our subsequent analysis that the performance degradation is\npositively correlated with the similarity between the additional and the\noriginal pretraining dataset of the LLM. Our further token-level perplexity\nobservations reveals that the perplexity degradation is due to a handful of\ntokens that are not informative about the domain. We hope these findings will\nguide us in determining when to adapt a model vs when to rely on its\nfoundational capabilities.\n","authors":["Fırat Öncel","Matthias Bethge","Beyza Ermis","Mirco Ravanelli","Cem Subakan","Çağatay Yıldız"],"pdf_url":"https://arxiv.org/pdf/2410.05581v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.12299v1","updated":"2024-10-16T06:58:49Z","published":"2024-10-16T06:58:49Z","title":"Semantics-Adaptive Activation Intervention for LLMs via Dynamic Steering\n  Vectors","summary":"  Large language models (LLMs) have achieved remarkable performance across many\ntasks, yet aligning them with desired behaviors remains challenging. Activation\nintervention has emerged as an effective and economical method to modify the\nbehavior of LLMs. Despite considerable interest in this area, current\nintervention methods exclusively employ a fixed steering vector to modify model\nactivations, lacking adaptability to diverse input semantics. To address this\nlimitation, we propose Semantics-Adaptive Dynamic Intervention (SADI), a novel\nmethod that constructs a dynamic steering vector to intervene model activations\nat inference time. More specifically, SADI utilizes activation differences in\ncontrastive pairs to precisely identify critical elements of an LLM (i.e.,\nattention heads, hidden states, and neurons) for targeted intervention. During\ninference, SADI dynamically steers model behavior by scaling element-wise\nactivations based on the directions of input semantics. Experimental results\nshow that SADI outperforms established baselines by substantial margins,\nimproving task performance without training. SADI's cost-effectiveness and\ngeneralizability across various LLM backbones and tasks highlight its potential\nas a versatile alignment technique. In addition, we release the code to foster\nresearch along this line:https://github.com/weixuan-wang123/SADI.\n","authors":["Weixuan Wang","Jingyuan Yang","Wei Peng"],"pdf_url":"https://arxiv.org/pdf/2410.12299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12298v1","updated":"2024-10-16T06:57:18Z","published":"2024-10-16T06:57:18Z","title":"Pyramid-Driven Alignment: Pyramid Principle Guided Integration of Large\n  Language Models and Knowledge Graphs","summary":"  Large Language Models (LLMs) possess impressive reasoning abilities but are\nprone to generating incorrect information, often referred to as hallucinations.\nWhile incorporating external Knowledge Graphs (KGs) can partially mitigate this\nissue, existing methods primarily treat KGs as static knowledge repositories,\noverlooking the critical disparity between KG and LLM knowledge, and failing to\nfully exploit the reasoning capabilities inherent in KGs. To address these\nlimitations, we propose Pyramid-Driven Alignment (PDA), a novel framework for\nseamlessly integrating LLMs with KGs. PDA utilizes Pyramid Principle analysis\nto construct a hierarchical pyramid structure. This structure is designed to\nreflect the input question and generate more validated deductive knowledge,\nthereby enhancing the alignment of LLMs and KGs and ensuring more cohesive\nintegration. Furthermore, PDA employs a recursive mechanism to harness the\nunderlying reasoning abilities of KGs, resulting in more accurate knowledge\nretrieval for question-answering tasks. Our experimental results reveal a\nsubstantial performance advantage of PDA over state-of-the-art baselines, with\nimprovements reaching 26.70% and 26.78%.\n","authors":["Lei Sun","Xinchen Wang","Youdi Li"],"pdf_url":"https://arxiv.org/pdf/2410.12298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12294v1","updated":"2024-10-16T06:51:09Z","published":"2024-10-16T06:51:09Z","title":"Towards LLM-based Cognitive Models of Students with Misconceptions","summary":"  Accurately modeling student cognition is crucial for developing effective\nAI-driven educational technologies. A key challenge is creating realistic\nstudent models that satisfy two essential properties: (1) accurately\nreplicating specific misconceptions, and (2) correctly solving problems where\nthese misconceptions are not applicable. This dual requirement reflects the\ncomplex nature of student understanding, where misconceptions coexist with\ncorrect knowledge. This paper investigates whether Large Language Models (LLMs)\ncan be instruction-tuned to meet this dual requirement and effectively simulate\nstudent thinking in algebra. We introduce MalAlgoPy, a novel Python library\nthat generates datasets reflecting authentic student solution patterns through\na graph-based representation of algebraic problem-solving. Utilizing MalAlgoPy,\nwe define and examine Cognitive Student Models (CSMs) - LLMs instruction tuned\nto faithfully emulate realistic student behavior. Our findings reveal that LLMs\ntrained on misconception examples can efficiently learn to replicate errors.\nHowever, the training diminishes the model's ability to solve problems\ncorrectly, particularly for problem types where the misconceptions are not\napplicable, thus failing to satisfy second property of CSMs. We demonstrate\nthat by carefully calibrating the ratio of correct to misconception examples in\nthe training data - sometimes as low as 0.25 - it is possible to develop CSMs\nthat satisfy both properties. Our insights enhance our understanding of\nAI-based student models and pave the way for effective adaptive learning\nsystems.\n","authors":["Shashank Sonkar","Xinghe Chen","Naiming Liu","Richard G. Baraniuk","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.12294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12292v1","updated":"2024-10-16T06:49:54Z","published":"2024-10-16T06:49:54Z","title":"How much do contextualized representations encode long-range context?","summary":"  We analyze contextual representations in neural autoregressive language\nmodels, emphasizing long-range contexts that span several thousand tokens. Our\nmethodology employs a perturbation setup and the metric\n\\emph{Anisotropy-Calibrated Cosine Similarity}, to capture the degree of\ncontextualization of long-range patterns from the perspective of representation\ngeometry. We begin the analysis with a case study on standard decoder-only\nTransformers, demonstrating that similar perplexity can exhibit markedly\ndifferent downstream task performance, which can be explained by the difference\nin contextualization of long-range content. Next, we extend the analysis to\nother models, covering recent novel architectural designs and various training\nconfigurations. The representation-level results illustrate a reduced capacity\nfor high-complexity (i.e., less compressible) sequences across architectures,\nand that fully recurrent models rely heavily on local context, whereas hybrid\nmodels more effectively encode the entire sequence structure. Finally,\npreliminary analysis of model size and training configurations on the encoding\nof long-range context suggest potential directions for improving existing\nlanguage models.\n","authors":["Simeng Sun","Cheng-Ping Hsieh"],"pdf_url":"https://arxiv.org/pdf/2410.12292v1.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.12288v1","updated":"2024-10-16T06:47:18Z","published":"2024-10-16T06:47:18Z","title":"A Prompt-Based Knowledge Graph Foundation Model for Universal In-Context\n  Reasoning","summary":"  Extensive knowledge graphs (KGs) have been constructed to facilitate\nknowledge-driven tasks across various scenarios. However, existing work usually\ndevelops separate reasoning models for different KGs, lacking the ability to\ngeneralize and transfer knowledge across diverse KGs and reasoning settings. In\nthis paper, we propose a prompt-based KG foundation model via in-context\nlearning, namely KG-ICL, to achieve a universal reasoning ability.\nSpecifically, we introduce a prompt graph centered with a query-related example\nfact as context to understand the query relation. To encode prompt graphs with\nthe generalization ability to unseen entities and relations in queries, we\nfirst propose a unified tokenizer that maps entities and relations in prompt\ngraphs to predefined tokens. Then, we propose two message passing neural\nnetworks to perform prompt encoding and KG reasoning, respectively. We conduct\nevaluation on 43 different KGs in both transductive and inductive settings.\nResults indicate that the proposed KG-ICL outperforms baselines on most\ndatasets, showcasing its outstanding generalization and universal reasoning\ncapabilities. The source code is accessible on GitHub:\nhttps://github.com/nju-websoft/KG-ICL.\n","authors":["Yuanning Cui","Zequn Sun","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2410.12288v1.pdf","comment":"Accepted in the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.12284v1","updated":"2024-10-16T06:43:02Z","published":"2024-10-16T06:43:02Z","title":"Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical\n  Decision-Support Setting","summary":"  The growing capabilities of AI models are leading to their wider use,\nincluding in safety-critical domains. Explainable AI (XAI) aims to make these\nmodels safer to use by making their inference process more transparent.\nHowever, current explainability methods are seldom evaluated in the way they\nare intended to be used: by real-world end users. To address this, we conducted\na large-scale user study with 85 healthcare practitioners in the context of\nhuman-AI collaborative chest X-ray analysis. We evaluated three types of\nexplanations: visual explanations (saliency maps), natural language\nexplanations, and a combination of both modalities. We specifically examined\nhow different explanation types influence users depending on whether the AI\nadvice and explanations are factually correct. We find that text-based\nexplanations lead to significant over-reliance, which is alleviated by\ncombining them with saliency maps. We also observe that the quality of\nexplanations, that is, how much factually correct information they entail, and\nhow much this aligns with AI correctness, significantly impacts the usefulness\nof the different explanation types.\n","authors":["Maxime Kayser","Bayar Menzat","Cornelius Emde","Bogdan Bercean","Alex Novak","Abdala Espinosa","Bartlomiej W. Papiez","Susanne Gaube","Thomas Lukasiewicz","Oana-Maria Camburu"],"pdf_url":"https://arxiv.org/pdf/2410.12284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12279v1","updated":"2024-10-16T06:35:56Z","published":"2024-10-16T06:35:56Z","title":"Beyond Oversmoothing: Evaluating DDPM and MSE for Scalable Speech\n  Synthesis in ASR","summary":"  Synthetically generated speech has rapidly approached human levels of\nnaturalness. However, the paradox remains that ASR systems, when trained on TTS\noutput that is judged as natural by humans, continue to perform badly on real\nspeech. In this work, we explore whether this phenomenon is due to the\noversmoothing behaviour of models commonly used in TTS, with a particular focus\non the behaviour of TTS-for-ASR as the amount of TTS training data is scaled\nup. We systematically compare Denoising Diffusion Probabilistic Models (DDPM)\nto Mean Squared Error (MSE) based models for TTS, when used for ASR model\ntraining. We test the scalability of the two approaches, varying both the\nnumber hours, and the number of different speakers. We find that for a given\nmodel size, DDPM can make better use of more data, and a more diverse set of\nspeakers, than MSE models. We achieve the best reported ratio between real and\nsynthetic speech WER to date (1.46), but also find that a large gap remains.\n","authors":["Christoph Minixhofer","Ondrej Klejch","Peter Bell"],"pdf_url":"https://arxiv.org/pdf/2410.12279v1.pdf","comment":"Under review at ICASSP 2025"},{"id":"http://arxiv.org/abs/2406.14036v2","updated":"2024-10-16T06:33:44Z","published":"2024-06-20T06:56:35Z","title":"Towards Infinite-Long Prefix in Transformer","summary":"  Prompting and context-based fine-tuning methods, which we call Prefix\nLearning, have been proposed to enhance the performance of language models on\nvarious downstream tasks. They are empirically efficient and effective,\nmatching the performance of full parameter fine-tuning, but the theoretical\nunderstandings are limited. In this paper, we aim to address this limitation by\nstudying their ability from the perspective of prefix length. In particular, we\nprovide a convergence guarantee for training an ultra-long prefix in a stylized\nsetting using the Neural Tangent Kernel (NTK) framework. Based on this strong\ntheoretical guarantee, we design and implement an algorithm that only needs to\nintroduce and fine-tune a few extra trainable parameters instead of an\ninfinite-long prefix in each layer of a transformer, and can approximate the\nprefix attention to a guaranteed polynomial-small error. Preliminary\nexperimental results on vision, natural language, and math data show that our\nmethod achieves superior or competitive performance compared to existing\nmethods like full parameters fine-tuning, P-Tuning V2, and LoRA. This\ndemonstrates our method is promising for parameter-efficient fine-tuning. Our\ncode can be found at\n\\url{https://github.com/ChristianYang37/chiwun/tree/main/src/NTK-Attention}.\n","authors":["Yingyu Liang","Zhenmei Shi","Zhao Song","Chiwun Yang"],"pdf_url":"https://arxiv.org/pdf/2406.14036v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11258v2","updated":"2024-10-16T06:32:50Z","published":"2024-06-17T06:48:31Z","title":"SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented\n  Generation","summary":"  Large Language Models (LLMs) have shown great potential in the biomedical\ndomain with the advancement of retrieval-augmented generation (RAG). However,\nexisting retrieval-augmented approaches face challenges in addressing diverse\nqueries and documents, particularly for medical knowledge queries, resulting in\nsub-optimal performance. To address these limitations, we propose a novel\nplug-and-play LLM-based retrieval method called Self-Rewarding Tree Search\n(SeRTS) based on Monte Carlo Tree Search (MCTS) and a self-rewarding paradigm.\nBy combining the reasoning capabilities of LLMs with the effectiveness of tree\nsearch, SeRTS boosts the zero-shot performance of retrieving high-quality and\ninformative results for RAG. We further enhance retrieval performance by\nfine-tuning LLMs with Proximal Policy Optimization (PPO) objectives using the\ntrajectories collected by SeRTS as feedback. Controlled experiments using the\nBioASQ-QA dataset with GPT-3.5-Turbo and LLama2-7b demonstrate that our method\nsignificantly improves the performance of the BM25 retriever and surpasses the\nstrong baseline of self-reflection in both efficiency and scalability.\nMoreover, SeRTS generates higher-quality feedback for PPO training than\nself-reflection. Our proposed method effectively adapts LLMs to document\nretrieval tasks, enhancing their ability to retrieve highly relevant documents\nfor RAG in the context of medical knowledge queries. This work presents a\nsignificant step forward in leveraging LLMs for accurate and comprehensive\nbiomedical question answering.\n","authors":["Minda Hu","Licheng Zong","Hongru Wang","Jingyan Zhou","Jingjing Li","Yichen Gao","Kam-Fai Wong","Yu Li","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2406.11258v2.pdf","comment":"This work has been accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.12278v1","updated":"2024-10-16T06:31:59Z","published":"2024-10-16T06:31:59Z","title":"Controlled Automatic Task-Specific Synthetic Data Generation for\n  Hallucination Detection","summary":"  We present a novel approach to automatically generate non-trivial\ntask-specific synthetic datasets for hallucination detection. Our approach\nfeatures a two-step generation-selection pipeline, using hallucination pattern\nguidance and a language style alignment during generation. Hallucination\npattern guidance leverages the most important task-specific hallucination\npatterns while language style alignment aligns the style of the synthetic\ndataset with benchmark text. To obtain robust supervised detectors from\nsynthetic datasets, we also adopt a data mixture strategy to improve\nperformance robustness and generalization. Our results on three datasets show\nthat our generated hallucination text is more closely aligned with\nnon-hallucinated text versus baselines, to train hallucination detectors with\nbetter generalization. Our hallucination detectors trained on synthetic\ndatasets outperform in-context-learning (ICL)-based detectors by a large margin\nof 32%. Our extensive experiments confirm the benefits of our approach with\ncross-task and cross-generator generalization. Our data-mixture-based training\nfurther improves the generalization and robustness of hallucination detection.\n","authors":["Yong Xie","Karan Aggarwal","Aitzaz Ahmad","Stephen Lau"],"pdf_url":"https://arxiv.org/pdf/2410.12278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11291v2","updated":"2024-10-16T06:25:57Z","published":"2024-10-15T05:26:57Z","title":"Enhancing Assamese NLP Capabilities: Introducing a Centralized Dataset\n  Repository","summary":"  This paper introduces a centralized, open-source dataset repository designed\nto advance NLP and NMT for Assamese, a low-resource language. The repository,\navailable at GitHub, supports various tasks like sentiment analysis, named\nentity recognition, and machine translation by providing both pre-training and\nfine-tuning corpora. We review existing datasets, highlighting the need for\nstandardized resources in Assamese NLP, and discuss potential applications in\nAI-driven research, such as LLMs, OCR, and chatbots. While promising,\nchallenges like data scarcity and linguistic diversity remain. The repository\naims to foster collaboration and innovation, promoting Assamese language\nresearch in the digital age.\n","authors":["S. Tamang","D. J. Bora"],"pdf_url":"https://arxiv.org/pdf/2410.11291v2.pdf","comment":"6 pages, 1 table, 1 figure"},{"id":"http://arxiv.org/abs/2407.12508v2","updated":"2024-10-16T06:25:50Z","published":"2024-07-17T11:45:02Z","title":"MERLIN: Multimodal Embedding Refinement via LLM-based Iterative\n  Navigation for Text-Video Retrieval-Rerank Pipeline","summary":"  The rapid expansion of multimedia content has made accurately retrieving\nrelevant videos from large collections increasingly challenging. Recent\nadvancements in text-video retrieval have focused on cross-modal interactions,\nlarge-scale foundation model training, and probabilistic modeling, yet often\nneglect the crucial user perspective, leading to discrepancies between user\nqueries and the content retrieved. To address this, we introduce MERLIN\n(Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel,\ntraining-free pipeline that leverages Large Language Models (LLMs) for\niterative feedback learning. MERLIN refines query embeddings from a user\nperspective, enhancing alignment between queries and video content through a\ndynamic question answering process. Experimental results on datasets like\nMSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improves\nRecall@1, outperforming existing systems and confirming the benefits of\nintegrating LLMs into multimodal retrieval systems for more responsive and\ncontext-aware multimedia retrieval.\n","authors":["Donghoon Han","Eunhwan Park","Gisang Lee","Adam Lee","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2407.12508v2.pdf","comment":"EMNLP 2024 Industry Track Accepted (Camera-Ready Version)"},{"id":"http://arxiv.org/abs/2410.12271v1","updated":"2024-10-16T06:16:30Z","published":"2024-10-16T06:16:30Z","title":"Kallini et al. (2024) do not compare impossible languages with\n  constituency-based ones","summary":"  A central goal of linguistic theory is to find a precise characterization of\nthe notion \"possible human language\", in the form of a computational device\nthat is capable of describing all and only the languages that can be acquired\nby a typically developing human child. The success of recent large language\nmodels (LLMs) in NLP applications arguably raises the possibility that LLMs\nmight be computational devices that meet this goal. This would only be the case\nif, in addition to succeeding in learning human languages, LLMs struggle to\nlearn \"impossible\" human languages. Kallini et al. (2024; \"Mission: Impossible\nLanguage Models\", Proc. ACL) conducted experiments aiming to test this by\ntraining GPT-2 on a variety of synthetic languages, and found that it learns\nsome more successfully than others. They present these asymmetries as support\nfor the idea that LLMs' inductive biases align with what is regarded as\n\"possible\" for human languages, but the most significant comparison has a\nconfound that makes this conclusion unwarranted. In this paper I explain the\nconfound and suggest some ways forward towards constructing a comparison that\nappropriately tests the underlying issue.\n","authors":["Tim Hunter"],"pdf_url":"https://arxiv.org/pdf/2410.12271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04070v3","updated":"2024-10-16T06:15:35Z","published":"2024-10-05T08:00:55Z","title":"PAD: Personalized Alignment at Decoding-Time","summary":"  Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.\n","authors":["Ruizhe Chen","Xiaotian Zhang","Meng Luo","Wenhao Chai","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2410.04070v3.pdf","comment":"This paper presents Personalized Alignment at Decoding-time (PAD), a\n  novel framework designed to align LLM outputs with diverse personalized\n  preferences during the inference phase"},{"id":"http://arxiv.org/abs/2410.12265v1","updated":"2024-10-16T06:06:06Z","published":"2024-10-16T06:06:06Z","title":"An Automatic and Cost-Efficient Peer-Review Framework for Language\n  Generation Evaluation","summary":"  With the rapid development of large language models (LLMs), how to\nefficiently evaluate them has become an important research question. Existing\nevaluation methods often suffer from high costs, limited test formats, the need\nof human references, and systematic evaluation biases. To address these\nlimitations, our study introduces the Auto-PRE, an automatic LLM evaluation\nframework based on peer review. In contrast to previous studies that rely on\nhuman annotations, Auto-PRE selects evaluator LLMs automatically based on their\ninherent traits including consistency, self-confidence, and pertinence. We\nconduct extensive experiments on three tasks: summary generation, non-factoid\nquestion-answering, and dialogue generation. Experimental results indicate our\nAuto-PRE achieves state-of-the-art performance at a lower cost. Moreover, our\nstudy highlights the impact of prompt strategies and evaluation formats on\nevaluation performance, offering guidance for method optimization in the\nfuture.\n","authors":["Junjie Chen","Weihang Su","Zhumin Chu","Haitao Li","Qinyao Ai","Yiqun Liu","Min Zhang","Shaoping Ma"],"pdf_url":"https://arxiv.org/pdf/2410.12265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10148v2","updated":"2024-10-16T05:59:33Z","published":"2024-10-14T04:29:57Z","title":"$α$-DPO: Adaptive Reward Margin is What Direct Preference\n  Optimization Needs","summary":"  Aligning large language models (LLMs) with human values and intentions is\ncrucial for their utility, honesty, and safety. Reinforcement learning from\nhuman feedback (RLHF) is a popular approach to achieve this alignment, but it\nfaces challenges in computational efficiency and training stability. Recent\nmethods like Direct Preference Optimization (DPO) and Simple Preference\nOptimization (SimPO) have proposed offline alternatives to RLHF, simplifying\nthe process by reparameterizing the reward function. However, DPO depends on a\npotentially suboptimal reference model, and SimPO's assumption of a fixed\ntarget reward margin may lead to suboptimal decisions in diverse data settings.\nIn this work, we propose $\\alpha$-DPO, an adaptive preference optimization\nalgorithm designed to address these limitations by introducing a dynamic reward\nmargin. Specifically, $\\alpha$-DPO employs an adaptive preference distribution,\nbalancing the policy model and the reference model to achieve personalized\nreward margins. We provide theoretical guarantees for $\\alpha$-DPO,\ndemonstrating its effectiveness as a surrogate optimization objective and its\nability to balance alignment and diversity through KL divergence control.\nEmpirical evaluations on AlpacaEval 2 and Arena-Hard show that $\\alpha$-DPO\nconsistently outperforms DPO and SimPO across various model settings,\nestablishing it as a robust approach for fine-tuning LLMs. Our method achieves\nsignificant improvements in win rates, highlighting its potential as a powerful\ntool for LLM alignment. The code is available at\nhttps://github.com/junkangwu/alpha-DPO\n","authors":["Junkang Wu","Xue Wang","Zhengyi Yang","Jiancan Wu","Jinyang Gao","Bolin Ding","Xiang Wang","Rong Jin","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2410.10148v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10792v6","updated":"2024-10-16T05:44:07Z","published":"2023-08-21T15:35:16Z","title":"Instruction Tuning for Large Language Models: A Survey","summary":"  This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of IT, the\nconstruction of IT datasets, the training of IT models, and applications to\ndifferent modalities, domains and applications, along with an analysis on\naspects that influence the outcome of IT (e.g., generation of instruction\noutputs, size of the instruction dataset, etc). We also review the potential\npitfalls of IT along with criticism against it, along with efforts pointing out\ncurrent deficiencies of existing strategies and suggest some avenues for\nfruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey\n","authors":["Shengyu Zhang","Linfeng Dong","Xiaoya Li","Sen Zhang","Xiaofei Sun","Shuhe Wang","Jiwei Li","Runyi Hu","Tianwei Zhang","Fei Wu","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2308.10792v6.pdf","comment":"V3; Last update: Oct 16, 2024"},{"id":"http://arxiv.org/abs/2406.11632v2","updated":"2024-10-16T05:22:53Z","published":"2024-06-17T15:13:52Z","title":"Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding\n  for Neural Machine Translation","summary":"  Maximum a posteriori decoding, a commonly used method for neural machine\ntranslation (NMT), aims to maximize the estimated posterior probability.\nHowever, high estimated probability does not always lead to high translation\nquality. Minimum Bayes Risk (MBR) decoding (\\citealp{kumar2004minimum}) offers\nan alternative by seeking hypotheses with the highest expected utility. In this\npaper, we show that Quality Estimation (QE) reranking\n(\\citealp{fernandes-etal-2022-quality}), which uses a QE model as a reranker,\ncan be viewed as a variant of MBR. Inspired by this, we propose source-based\nMBR (sMBR) decoding, a novel approach that utilizes synthetic sources\n(generated via back-translation or paraphrasing) as ``support hypotheses'' and\na reference-free quality estimation metric as the utility function, marking the\nfirst work to solely use sources in MBR decoding. Experiments show that sMBR\noutperforms QE reranking and the standard MBR decoding. Our findings suggest\nthat sMBR is a promising approach for NMT decoding.\n","authors":["Boxuan Lyu","Hidetaka Kamigaito","Kotaro Funakoshi","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2406.11632v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12248v1","updated":"2024-10-16T05:20:32Z","published":"2024-10-16T05:20:32Z","title":"CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for\n  Retrieval-Augmented Generation with Enhanced Data Diversity","summary":"  Retrieval-Augmented Generation (RAG) aims to enhance large language models\n(LLMs) to generate more accurate and reliable answers with the help of the\nretrieved context from external knowledge sources, thereby reducing the\nincidence of hallucinations. Despite the advancements, evaluating these systems\nremains a crucial research area due to the following issues: (1) Limited data\ndiversity: The insufficient diversity of knowledge sources and query types\nconstrains the applicability of RAG systems; (2) Obscure problems location:\nExisting evaluation methods have difficulty in locating the stage of the RAG\npipeline where problems occur; (3) Unstable retrieval evaluation: These methods\noften fail to effectively assess retrieval performance, particularly when the\nchunking strategy changes. To tackle these challenges, we propose a\nComprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough\nevaluation across the entire RAG pipeline, including chunking, retrieval,\nreranking, and generation. To effectively evaluate the first three phases, we\nintroduce multi-granularity keywords, including coarse-grained and fine-grained\nkeywords, to assess the retrieved context instead of relying on the annotation\nof golden chunks. Moreover, we release a holistic benchmark dataset tailored\nfor diverse data scenarios covering a wide range of document formats and query\ntypes. We demonstrate the utility of the CoFE-RAG framework by conducting\nexperiments to evaluate each stage of RAG systems. Our evaluation method\nprovides unique insights into the effectiveness of RAG systems in handling\ndiverse data scenarios, offering a more nuanced understanding of their\ncapabilities and limitations.\n","authors":["Jintao Liu","Ruixue Ding","Linhao Zhang","Pengjun Xie","Fie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.12248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12247v1","updated":"2024-10-16T05:17:49Z","published":"2024-10-16T05:17:49Z","title":"EPS-MoE: Expert Pipeline Scheduler for Cost-Efficient MoE Inference","summary":"  Large Language Model (LLM) has revolutionized the field of artificial\nintelligence, with their capabilities expanding rapidly due to advances in deep\nlearning and increased computational resources. The mixture-of-experts (MoE)\nmodel has emerged as a prominent architecture in the field of LLM, better\nbalancing the model performance and computational efficiency. MoE architecture\nallows for effective scaling and efficient parallel processing, but the GEMM\n(General Matrix Multiply) of MoE and the large parameters introduce challenges\nin terms of computation efficiency and communication overhead, which becomes\nthe throughput bottleneck during inference. Applying a single parallelism\nstrategy like EP, DP, PP, etc. to MoE architecture usually achieves sub-optimal\ninference throughput, the straightforward combinations of existing different\nparallelisms on MoE can not obtain optimal inference throughput yet. This paper\nintroduces EPS-MoE, a novel expert pipeline scheduler for MoE that goes beyond\nthe existing inference parallelism schemes. Our approach focuses on optimizing\nthe computation of MoE FFN (FeedForward Network) modules by dynamically\nselecting the best kernel implementation of GroupGemm and DenseGemm for\ndifferent loads and adaptively overlapping these computations with\n\\textit{all2all} communication, leading to a substantial increase in\nthroughput. Our experimental results demonstrate an average 21% improvement in\nprefill throughput over existing parallel inference methods. Specifically, we\nvalidated our method on DeepSeekV2, a highly optimized model claimed to achieve\na prefill throughput of 100K tokens per second. By applying EPS-MoE, we further\naccelerated it to at least 120K tokens per second.\n","authors":["Yulei Qian","Fengcun Li","Xiangyang Ji","Xiaoyu Zhao","Jianchao Tan","Kefeng Zhang","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2410.12247v1.pdf","comment":"13 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.06949v2","updated":"2024-10-16T05:04:45Z","published":"2024-10-09T14:45:45Z","title":"Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent\n  Approach","summary":"  In real world software development, improper or missing exception handling\ncan severely impact the robustness and reliability of code. Exception handling\nmechanisms require developers to detect, capture, and manage exceptions\naccording to high standards, but many developers struggle with these tasks,\nleading to fragile code. This problem is particularly evident in open source\nprojects and impacts the overall quality of the software ecosystem. To address\nthis challenge, we explore the use of large language models (LLMs) to improve\nexception handling in code. Through extensive analysis, we identify three key\nissues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception\nTypes, and Distorted Handling Solutions. These problems are widespread across\nreal world repositories, suggesting that robust exception handling practices\nare often overlooked or mishandled. In response, we propose Seeker, a multi\nagent framework inspired by expert developer strategies for exception handling.\nSeeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist\nLLMs in detecting, capturing, and resolving exceptions more effectively. Our\nwork is the first systematic study on leveraging LLMs to enhance exception\nhandling practices, providing valuable insights for future improvements in code\nreliability.\n","authors":["Xuanming Zhang","Yuxuan Chen","Yuan Yuan","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.06949v2.pdf","comment":"26 pages, 7 figures. Submitted ICLR 2025"},{"id":"http://arxiv.org/abs/2406.14867v2","updated":"2024-10-16T05:03:04Z","published":"2024-06-21T05:05:39Z","title":"Investigating the Transferability of Code Repair for Low-Resource\n  Programming Languages","summary":"  Large language models (LLMs) have shown remarkable performance on code\ngeneration tasks. A recent use case is iterative code repair, where an LLM\nfixes an incorrect program by rationalizing about errors and generating new\ncode. Recent works augment the code repair process by integrating modern\ntechniques such as chain-of-thought reasoning or distillation, but only study\ntheir benefits on high-resource languages like Python, and ignore low-resource\nlanguages like Perl. To address this gap of knowledge, we investigate the\nbenefits of distilling code repair for both high and low resource languages to\ndetermine if the techniques that are effective in a high resource setting are\nalso applicable in a low resource setting. Our evaluation shows that distilling\nthe ability to repair code has language dependent benefits. To explain this\nbehavior, we perform a further analysis and find that contrary to preexisting\nbeliefs, the correlation between reasoning ability and code correction ability\nis weak. We hypothesize this weak correlation is magnified in low-resource\nsettings where base models lack deep knowledge of a programming language,\nleading to wavering benefits of code repair.\n","authors":["Kyle Wong","Alfonso Amayuelas","Liangming Pan","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.14867v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06554v2","updated":"2024-10-16T04:48:08Z","published":"2024-10-09T05:17:08Z","title":"The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield\n  Better Language Models","summary":"  Reinforcement Learning from Human Feedback significantly enhances Natural\nLanguage Processing by aligning language models with human expectations. A\ncritical factor in this alignment is the strength of reward models used during\ntraining. This study explores whether stronger reward models invariably lead to\nbetter language models. In this paper, through experiments on relevance,\nfactuality, and completeness tasks using the QA-FEEDBACK dataset and reward\nmodels based on Longformer, we uncover a surprising paradox: language models\ntrained with moderately accurate reward models outperform those guided by\nhighly accurate ones. This challenges the widely held belief that stronger\nreward models always lead to better language models, and opens up new avenues\nfor future research into the key factors driving model performance and how to\nchoose the most suitable reward models. Code and additional details are\navailable at https://github.com/EIT-NLP/AccuracyParadox-RLHF.\n","authors":["Yanjun Chen","Dawei Zhu","Yirong Sun","Xinghao Chen","Wei Zhang","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2410.06554v2.pdf","comment":"10 pages, 27 figures (including 18 in the appendix), submitted to\n  EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.12228v1","updated":"2024-10-16T04:44:15Z","published":"2024-10-16T04:44:15Z","title":"Triple Modality Fusion: Aligning Visual, Textual, and Graph Data with\n  Large Language Models for Multi-Behavior Recommendations","summary":"  Integrating diverse data modalities is crucial for enhancing the performance\nof personalized recommendation systems. Traditional models, which often rely on\nsingular data sources, lack the depth needed to accurately capture the\nmultifaceted nature of item features and user behaviors. This paper introduces\na novel framework for multi-behavior recommendations, leveraging the fusion of\ntriple-modality, which is visual, textual, and graph data through alignment\nwith large language models (LLMs). By incorporating visual information, we\ncapture contextual and aesthetic item characteristics; textual data provides\ninsights into user interests and item features in detail; and graph data\nelucidates relationships within the item-behavior heterogeneous graphs. Our\nproposed model called Triple Modality Fusion (TMF) utilizes the power of LLMs\nto align and integrate these three modalities, achieving a comprehensive\nrepresentation of user behaviors. The LLM models the user's interactions\nincluding behaviors and item features in natural languages. Initially, the LLM\nis warmed up using only natural language-based prompts. We then devise the\nmodality fusion module based on cross-attention and self-attention mechanisms\nto integrate different modalities from other models into the same embedding\nspace and incorporate them into an LLM. Extensive experiments demonstrate the\neffectiveness of our approach in improving recommendation accuracy. Further\nablation studies validate the effectiveness of our model design and benefits of\nthe TMF.\n","authors":["Luyi Ma","Xiaohan Li","Zezhong Fan","Jianpeng Xu","Jason Cho","Praveen Kanumala","Kaushiki Nag","Sushant Kumar","Kannan Achan"],"pdf_url":"https://arxiv.org/pdf/2410.12228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12222v1","updated":"2024-10-16T04:36:17Z","published":"2024-10-16T04:36:17Z","title":"On A Scale From 1 to 5: Quantifying Hallucination in Faithfulness\n  Evaluation","summary":"  Hallucination has been a popular topic in natural language generation (NLG).\nIn real-world applications, unfaithful content can result in bad data quality\nor loss of trust from end users. Thus, it is crucial to fact-check before\nadopting NLG for production usage, which can be expensive if done manually. In\nthis paper, we investigate automated faithfulness evaluation in guided NLG. We\ndeveloped a rubrics template and use large language models (LLMs) to score the\ngeneration into quantifiable scales. We compared popular LLMs as well as the\nwidely adopted natural language inference (NLI) models in scoring quality and\nsensitivity. In addition, we developed methods to generation synthetic\nunfaithful data, as well as a heuristics to quantify the percentage of\nhallucination. Our results on 4 travel-domain industry dataset show that GPT-4\ncan provide accurate judgement and explanation on whether a source and a\ngeneration are factually consistent. Furthermore, we found that tuning NLI\nmodels on synthetic data can improve performance. Lastly, we present insights\non latency and cost for deploying such system.\n","authors":["Xiaonan Jing","Srinivas Billa","Danny Godbout"],"pdf_url":"https://arxiv.org/pdf/2410.12222v1.pdf","comment":"14 pages, 13 figures"},{"id":"http://arxiv.org/abs/2406.17232v2","updated":"2024-10-16T04:36:09Z","published":"2024-06-25T02:37:29Z","title":"Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human\n  Belief Networks","summary":"  Creating human-like large language model (LLM) agents is crucial for faithful\nsocial simulation. Having LLMs role-play based on demographic information\nsometimes improves human likeness but often does not. This study assessed\nwhether LLM alignment with human behavior can be improved by integrating\ninformation from empirically-derived human belief networks. Using data from a\nhuman survey, we estimated a belief network encompassing 64 topics loading on\nnine non-overlapping latent factors. We then seeded LLM-based agents with an\nopinion on one topic, and assessed the alignment of its expressed opinions on\nremaining test topics with corresponding human data. Role-playing based on\ndemographic information alone did not align LLM and human opinions, but seeding\nthe agent with a single belief greatly improved alignment for topics related in\nthe belief network, and not for topics outside the network. These results\nsuggest a novel path for human-LLM belief alignment in work seeking to simulate\nand understand patterns of belief distributions in society.\n","authors":["Yun-Shiuan Chuang","Krirk Nirunwiroj","Zach Studdiford","Agam Goyal","Vincent V. Frigo","Sijia Yang","Dhavan Shah","Junjie Hu","Timothy T. Rogers"],"pdf_url":"https://arxiv.org/pdf/2406.17232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12219v1","updated":"2024-10-16T04:29:46Z","published":"2024-10-16T04:29:46Z","title":"OmnixR: Evaluating Omni-modality Language Models on Reasoning across\n  Modalities","summary":"  We introduce OmnixR, an evaluation suite designed to benchmark SoTA\nOmni-modality Language Models, such as GPT-4o and Gemini. Evaluating OLMs,\nwhich integrate multiple modalities such as text, vision, and audio, presents\nunique challenges. Particularly, the user message might often consist of\nmultiple modalities, such that OLMs have to establish holistic understanding\nand reasoning across modalities to accomplish the task. Existing benchmarks are\nlimited to single modality or dual-modality tasks, overlooking comprehensive\nmulti-modal assessments of model reasoning. To address this, OmnixR offers two\nevaluation variants: (1)synthetic subset: a synthetic dataset generated\nautomatically by translating text into multiple modalities--audio, images,\nvideo, and hybrids (Omnify). (2)realistic subset: a real-world dataset,\nmanually curated and annotated by experts, for evaluating cross-modal reasoning\nin natural settings. OmnixR presents a unique evaluation towards assessing OLMs\nover a diverse mix of modalities, such as a question that involves video,\naudio, and text, providing a rigorous cross-modal reasoning testbed unlike any\nexisting benchmarks. Our experiments find that all state-of-the-art OLMs\nstruggle with OmnixR questions that require integrating information from\nmultiple modalities to answer. Further analysis highlights differences in\nreasoning behavior, underscoring the challenges of omni-modal AI alignment.\n","authors":["Lichang Chen","Hexiang Hu","Mingda Zhang","Yiwen Chen","Zifeng Wang","Yandong Li","Pranav Shyam","Tianyi Zhou","Heng Huang","Ming-Hsuan Yang","Boqing Gong"],"pdf_url":"https://arxiv.org/pdf/2410.12219v1.pdf","comment":"19 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2410.12217v1","updated":"2024-10-16T04:26:40Z","published":"2024-10-16T04:26:40Z","title":"Accurate and Data-Efficient Toxicity Prediction when Annotators Disagree","summary":"  When annotators disagree, predicting the labels given by individual\nannotators can capture nuances overlooked by traditional label aggregation. We\nintroduce three approaches to predicting individual annotator ratings on the\ntoxicity of text by incorporating individual annotator-specific information: a\nneural collaborative filtering (NCF) approach, an in-context learning (ICL)\napproach, and an intermediate embedding-based architecture. We also study the\nutility of demographic information for rating prediction. NCF showed limited\nutility; however, integrating annotator history, demographics, and survey\ninformation permits both the embedding-based architecture and ICL to\nsubstantially improve prediction accuracy, with the embedding-based\narchitecture outperforming the other methods. We also find that, if\ndemographics are predicted from survey information, using these imputed\ndemographics as features performs comparably to using true demographic data.\nThis suggests that demographics may not provide substantial information for\nmodeling ratings beyond what is captured in survey responses. Our findings\nraise considerations about the relative utility of different types of annotator\ninformation and provide new approaches for modeling annotators in subjective\nNLP tasks.\n","authors":["Harbani Jaggi","Kashyap Murali","Eve Fleisig","Erdem Bıyık"],"pdf_url":"https://arxiv.org/pdf/2410.12217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20246v5","updated":"2024-10-16T04:26:04Z","published":"2023-10-31T08:09:20Z","title":"Breaking Language Barriers in Multilingual Mathematical Reasoning:\n  Insights and Observations","summary":"  Existing research predominantly focuses on developing powerful language\nlearning models (LLMs) for mathematical reasoning within monolingual languages,\nwith few explorations in preserving efficacy in a multilingual context. To\nbridge this gap, this paper pioneers exploring and training powerful\nMultilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we\nconstruct the first multilingual math reasoning instruction dataset,\nMGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue\nof training data scarcity in xMR tasks. Based on the collected dataset, we\npropose different training strategies to build powerful xMR LLMs, named\nMathOctopus, notably outperform conventional open-source LLMs and exhibit\nsuperiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B\nreaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond\nremarkable results, we unearth several pivotal observations and insights from\nextensive experiments: (1) When extending the rejection sampling strategy to\nthe multilingual context, it proves effective for model performances, albeit\nlimited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT)\nacross multiple languages not only significantly enhances model performance\nmultilingually but also elevates their monolingual performance. This indicates\nthat crafting multilingual corpora can be regarded as a vital strategy for\nenhancing model performance in a specific language, especially in mathematical\nreasoning tasks. For instance, MathOctopus-7B improves its counterparts that\ntrained on English from 42.2% to 50.8% on GSM8K testset. Codes are available at\nhttps://github.com/microsoft/MathOctopus.\n","authors":["Nuo Chen","Zinan Zheng","Ning Wu","Ming Gong","Dongmei Zhang","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2310.20246v5.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2407.17487v3","updated":"2024-10-16T04:24:59Z","published":"2024-07-03T08:27:51Z","title":"Explainable Natural Language Processing for Corporate Sustainability\n  Analysis","summary":"  Sustainability commonly refers to entities, such as individuals, companies,\nand institutions, having a non-detrimental (or even positive) impact on the\nenvironment, society, and the economy. With sustainability becoming a synonym\nof acceptable and legitimate behaviour, it is being increasingly demanded and\nregulated. Several frameworks and standards have been proposed to measure the\nsustainability impact of corporations, including United Nations' sustainable\ndevelopment goals and the recently introduced global sustainability reporting\nframework, amongst others. However, the concept of corporate sustainability is\ncomplex due to the diverse and intricate nature of firm operations (i.e.\ngeography, size, business activities, interlinks with other stakeholders). As a\nresult, corporate sustainability assessments are plagued by subjectivity both\nwithin data that reflect corporate sustainability efforts (i.e. corporate\nsustainability disclosures) and the analysts evaluating them. This subjectivity\ncan be distilled into distinct challenges, such as incompleteness, ambiguity,\nunreliability and sophistication on the data dimension, as well as limited\nresources and potential bias on the analyst dimension. Put together,\nsubjectivity hinders effective cost attribution to entities non-compliant with\nprevailing sustainability expectations, potentially rendering sustainability\nefforts and its associated regulations futile. To this end, we argue that\nExplainable Natural Language Processing (XNLP) can significantly enhance\ncorporate sustainability analysis. Specifically, linguistic understanding\nalgorithms (lexical, semantic, syntactic), integrated with XAI capabilities\n(interpretability, explainability, faithfulness), can bridge gaps in analyst\nresources and mitigate subjectivity problems within data.\n","authors":["Keane Ong","Rui Mao","Ranjan Satapathy","Ricardo Shirota Filho","Erik Cambria","Johan Sulaeman","Gianmarco Mengaldo"],"pdf_url":"https://arxiv.org/pdf/2407.17487v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11086v2","updated":"2024-10-16T04:23:12Z","published":"2024-10-14T20:59:59Z","title":"JOOCI: a Framework for Learning Comprehensive Speech Representations","summary":"  Information in speech can be divided into two categories: what is being said\n(content) and how it is expressed (other). Current state-of-the-art (SOTA)\ntechniques model speech at fixed segments, usually 10-25 ms, using a single\nembedding. Given the orthogonal nature of other and content information,\nattempting to optimize both within a single embedding results in suboptimal\nsolutions. This approach divides the models capacity, limiting its ability to\nbuild complex hierarchical features effectively. In this work, we present an\nend-to-end speech representation learning framework designed to jointly\noptimize the other and content information (JOOCI) in speech. By using separate\nlearnable parameters, JOOCI addresses this optimization challenge by modeling\nother and content information independently. Our results show that JOOCI\nconsistently outperforms other SOTA models of similar size (100 million\nparameters) and pre-training data used (960 hours) by a significant margin when\nevaluated on a range of speech downstream tasks in the SUPERB benchmark, as\nshown in Table 1.\n","authors":["Hemant Yadav","Rajiv Ratn Shah","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2410.11086v2.pdf","comment":"Submitted to ICLR 2025"},{"id":"http://arxiv.org/abs/2410.10861v2","updated":"2024-10-16T03:53:40Z","published":"2024-10-07T16:54:18Z","title":"Translation Canvas: An Explainable Interface to Pinpoint and Analyze\n  Translation Systems","summary":"  With the rapid advancement of machine translation research, evaluation\ntoolkits have become essential for benchmarking system progress. Tools like\nCOMET and SacreBLEU offer single quality score assessments that are effective\nfor pairwise system comparisons. However, these tools provide limited insights\nfor fine-grained system-level comparisons and the analysis of instance-level\ndefects. To address these limitations, we introduce Translation Canvas, an\nexplainable interface designed to pinpoint and analyze translation systems'\nperformance: 1) Translation Canvas assists machine translation researchers in\ncomprehending system-level model performance by identifying common errors\n(their frequency and severity) and analyzing relationships between different\nsystems based on various evaluation metrics. 2) It supports fine-grained\nanalysis by highlighting error spans with explanations and selectively\ndisplaying systems' predictions. According to human evaluation, Translation\nCanvas demonstrates superior performance over COMET and SacreBLEU packages\nunder enjoyability and understandability criteria.\n","authors":["Chinmay Dandekar","Wenda Xu","Xi Xu","Siqi Ouyang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2410.10861v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.12997v2","updated":"2024-10-16T03:52:13Z","published":"2024-06-18T18:37:24Z","title":"Discovering Elementary Discourse Units in Textual Data Using Canonical\n  Correlation Analysis","summary":"  Canonical Correlation Analysis (CCA) has been exploited immensely for\nlearning latent representations in various fields. This study takes a step\nfurther by demonstrating the potential of CCA in identifying Elementary\nDiscourse Units(EDUs) that captures the latent information within the textual\ndata. The probabilistic interpretation of CCA discussed in this study utilizes\nthe two-view nature of textual data, i.e. the consecutive sentences in a\ndocument or turns in a dyadic conversation, and has a strong theoretical\nfoundation. Furthermore, this study proposes a model for Elementary Discourse\nUnit(EDU) segmentation that discovers EDUs in textual data without any\nsupervision. To validate the model, the EDUs are utilized as textual unit for\ncontent selection in textual similarity task. Empirical results on Semantic\nTextual Similarity(STSB) and Mohler datasets confirm that, despite represented\nas a unigram, the EDUs deliver competitive results and can even beat various\nsophisticated supervised techniques. The model is simple, linear, adaptable and\nlanguage independent making it an ideal baseline particularly when labeled\ntraining data is scarce or nonexistent.\n","authors":["Akanksha Mehndiratta","Krishna Asawa"],"pdf_url":"https://arxiv.org/pdf/2406.12997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17969v2","updated":"2024-10-16T03:35:22Z","published":"2024-05-28T08:56:33Z","title":"Knowledge Circuits in Pretrained Transformers","summary":"  The remarkable capabilities of modern large language models are rooted in\ntheir vast repositories of knowledge encoded within their parameters, enabling\nthem to perceive the world and engage in reasoning. The inner workings of how\nthese models store knowledge have long been a subject of intense interest and\ninvestigation among researchers. To date, most studies have concentrated on\nisolated components within these models, such as the Multilayer Perceptrons and\nattention head. In this paper, we delve into the computation graph of the\nlanguage model to uncover the knowledge circuits that are instrumental in\narticulating specific knowledge. The experiments, conducted with GPT2 and\nTinyLLAMA, have allowed us to observe how certain information heads, relation\nheads, and Multilayer Perceptrons collaboratively encode knowledge within the\nmodel. Moreover, we evaluate the impact of current knowledge editing techniques\non these knowledge circuits, providing deeper insights into the functioning and\nconstraints of these editing methodologies. Finally, we utilize knowledge\ncircuits to analyze and interpret language model behaviors such as\nhallucinations and in-context learning. We believe the knowledge circuits hold\npotential for advancing our understanding of Transformers and guiding the\nimproved design of knowledge editing. Code and data are available in\nhttps://github.com/zjunlp/KnowledgeCircuits.\n","authors":["Yunzhi Yao","Ningyu Zhang","Zekun Xi","Mengru Wang","Ziwen Xu","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.17969v2.pdf","comment":"NeurIPS 2024, 32 pages"},{"id":"http://arxiv.org/abs/2407.07321v2","updated":"2024-10-16T03:33:58Z","published":"2024-07-10T02:33:09Z","title":"Examining Long-Context Large Language Models for Environmental Review\n  Document Comprehension","summary":"  As LLMs become increasingly ubiquitous, researchers have tried various\ntechniques to augment the knowledge provided to these models. Long context and\nretrieval-augmented generation (RAG) are two such methods that have recently\ngained popularity. In this work, we examine the benefits of both of these\ntechniques by utilizing question answering (QA) task in a niche domain. While\nthe effectiveness of LLM-based QA systems has already been established at an\nacceptable level in popular domains such as trivia and literature, it has not\noften been established in niche domains that traditionally require specialized\nexpertise. We construct the NEPAQuAD1.0 benchmark to evaluate the performance\nof five long-context LLMs -- Claude Sonnet, Gemini, GPT-4, Llama 3.1, and\nMistral -- when answering questions originating from Environmental Impact\nStatements prepared by U.S. federal government agencies in accordance with the\nNational Environmental Environmental Act (NEPA). We specifically measure the\nability of LLMs to understand the nuances of legal, technical, and\ncompliance-related information present in NEPA documents in different\ncontextual scenarios. We test the LLMs' internal prior NEPA knowledge by\nproviding questions without any context, as well as assess how LLMs synthesize\nthe contextual information present in long NEPA documents to facilitate the\nquestion/answering task. We compare the performance of the models in handling\ndifferent types of questions (e.g., problem-solving, divergent, etc.). Our\nresults suggest that RAG powered models significantly outperform those provided\nwith only the PDF context in terms of answer accuracy, regardless of the choice\nof the LLM. Our further analysis reveals that many models perform better\nanswering closed type questions (Yes/No) than divergent and problem-solving\nquestions.\n","authors":["Hung Phan","Anurag Acharya","Rounak Meyur","Sarthak Chaturvedi","Shivam Sharma","Mike Parker","Dan Nally","Ali Jannesari","Karl Pazdernik","Mahantesh Halappanavar","Sai Munikoti","Sameera Horawalavithana"],"pdf_url":"https://arxiv.org/pdf/2407.07321v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2410.12194v1","updated":"2024-10-16T03:30:09Z","published":"2024-10-16T03:30:09Z","title":"Negative-Prompt-driven Alignment for Generative Language Model","summary":"  Large language models have achieved remarkable capabilities, but aligning\ntheir outputs with human values and preferences remains a significant\nchallenge. Existing alignment methods primarily focus on positive examples\nwhile overlooking the importance of negative responses in guiding models away\nfrom undesirable behaviors. For instance, the widely-used alignment datasets\nreveals a scarcity of explicit negative examples that contradict human values,\nhindering its ability to discourage harmful or biased outputs during training.\nTo address this limitation, we propose NEAT, i.e., NEgative-prompt-driven\nAlignmenT, to introduce negative prompts to generate undesirable responses\nalongside positive examples during the optimization process. NEAT explicitly\npenalizes the model for producing harmful outputs, guiding it not only toward\ndesirable behaviors but also steering it away from generating undesirable,\nbiased responses. This dual feedback mechanism enables better alignment with\nhuman preferences, crucial in contexts where avoiding harm is paramount.\nStarting from a pre-trained language model, NEAT performs online alignment by\nincorporating a ranking loss derived from an expanded preference dataset\ncontaining both positive and negative examples. Extensive experiments validate\nNEAT's effectiveness in significantly enhancing language models' alignment with\nhuman values and preferences.\n","authors":["Shiqi Qiao","Ning Xv","Biao Liu","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2410.12194v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.12790v1","updated":"2024-10-16T17:59:49Z","published":"2024-10-16T17:59:49Z","title":"Dual Prototype Evolving for Test-Time Generalization of Vision-Language\n  Models","summary":"  Test-time adaptation, which enables models to generalize to diverse data with\nunlabeled test samples, holds significant value in real-world scenarios.\nRecently, researchers have applied this setting to advanced pre-trained\nvision-language models (VLMs), developing approaches such as test-time prompt\ntuning to further extend their practical applicability. However, these methods\ntypically focus solely on adapting VLMs from a single modality and fail to\naccumulate task-specific knowledge as more samples are processed. To address\nthis, we introduce Dual Prototype Evolving (DPE), a novel test-time adaptation\napproach for VLMs that effectively accumulates task-specific knowledge from\nmulti-modalities. Specifically, we create and evolve two sets of\nprototypes--textual and visual--to progressively capture more accurate\nmulti-modal representations for target classes during test time. Moreover, to\npromote consistent multi-modal representations, we introduce and optimize\nlearnable residuals for each test sample to align the prototypes from both\nmodalities. Extensive experimental results on 15 benchmark datasets demonstrate\nthat our proposed DPE consistently outperforms previous state-of-the-art\nmethods while also exhibiting competitive computational efficiency. Code is\navailable at https://github.com/zhangce01/DPE-CLIP.\n","authors":["Ce Zhang","Simon Stepputtis","Katia Sycara","Yaqi Xie"],"pdf_url":"https://arxiv.org/pdf/2410.12790v1.pdf","comment":"Accepted by NeurIPS 2024. Project page:\n  https://zhangce01.github.io/DPE-CLIP"},{"id":"http://arxiv.org/abs/2410.12787v1","updated":"2024-10-16T17:59:02Z","published":"2024-10-16T17:59:02Z","title":"The Curse of Multi-Modalities: Evaluating Hallucinations of Large\n  Multimodal Models across Language, Visual, and Audio","summary":"  Recent advancements in large multimodal models (LMMs) have significantly\nenhanced performance across diverse tasks, with ongoing efforts to further\nintegrate additional modalities such as video and audio. However, most existing\nLMMs remain vulnerable to hallucinations, the discrepancy between the factual\nmultimodal input and the generated textual output, which has limited their\napplicability in various real-world scenarios. This paper presents the first\nsystematic investigation of hallucinations in LMMs involving the three most\ncommon modalities: language, visual, and audio. Our study reveals two key\ncontributors to hallucinations: overreliance on unimodal priors and spurious\ninter-modality correlations. To address these challenges, we introduce the\nbenchmark The Curse of Multi-Modalities (CMM), which comprehensively evaluates\nhallucinations in LMMs, providing a detailed analysis of their underlying\nissues. Our findings highlight key vulnerabilities, including imbalances in\nmodality integration and biases from training data, underscoring the need for\nbalanced cross-modal learning and enhanced hallucination mitigation strategies.\nBased on our observations and findings, we suggest potential research\ndirections that could enhance the reliability of LMMs.\n","authors":["Sicong Leng","Yun Xing","Zesen Cheng","Yang Zhou","Hang Zhang","Xin Li","Deli Zhao","Shijian Lu","Chunyan Miao","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2410.12787v1.pdf","comment":"Project Page: cmm-damovl.site"},{"id":"http://arxiv.org/abs/2410.12781v1","updated":"2024-10-16T17:54:06Z","published":"2024-10-16T17:54:06Z","title":"Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage\n  Gaussian Splats","summary":"  We propose Long-LRM, a generalizable 3D Gaussian reconstruction model that is\ncapable of reconstructing a large scene from a long sequence of input images.\nSpecifically, our model can process 32 source images at 960x540 resolution\nwithin only 1.3 seconds on a single A100 80G GPU. Our architecture features a\nmixture of the recent Mamba2 blocks and the classical transformer blocks which\nallowed many more tokens to be processed than prior work, enhanced by efficient\ntoken merging and Gaussian pruning steps that balance between quality and\nefficiency. Unlike previous feed-forward models that are limited to processing\n1~4 input images and can only reconstruct a small portion of a large scene,\nLong-LRM reconstructs the entire scene in a single feed-forward step. On\nlarge-scale scene datasets such as DL3DV-140 and Tanks and Temples, our method\nachieves performance comparable to optimization-based approaches while being\ntwo orders of magnitude more efficient. Project page:\nhttps://arthurhero.github.io/projects/llrm\n","authors":["Chen Ziwen","Hao Tan","Kai Zhang","Sai Bi","Fujun Luan","Yicong Hong","Li Fuxin","Zexiang Xu"],"pdf_url":"https://arxiv.org/pdf/2410.12781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12777v1","updated":"2024-10-16T17:51:25Z","published":"2024-10-16T17:51:25Z","title":"Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned\n  Concepts","summary":"  With the rapid progress of diffusion-based content generation, significant\nefforts are being made to unlearn harmful or copyrighted concepts from\npretrained diffusion models (DMs) to prevent potential model misuse. However,\nit is observed that even when DMs are properly unlearned before release,\nmalicious finetuning can compromise this process, causing DMs to relearn the\nunlearned concepts. This occurs partly because certain benign concepts (e.g.,\n\"skin\") retained in DMs are related to the unlearned ones (e.g., \"nudity\"),\nfacilitating their relearning via finetuning. To address this, we propose\nmeta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an\nunlearned DM when used as is; moreover, if the meta-unlearned DM undergoes\nmalicious finetuning on unlearned concepts, the related benign concepts\nretained within it will be triggered to self-destruct, hindering the relearning\nof unlearned concepts. Our meta-unlearning framework is compatible with most\nexisting unlearning methods, requiring only the addition of an\neasy-to-implement meta objective. We validate our approach through empirical\nexperiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4\nand SDXL), supported by extensive ablation studies. Our code is available at\nhttps://github.com/sail-sg/Meta-Unlearning.\n","authors":["Hongcheng Gao","Tianyu Pang","Chao Du","Taihang Hu","Zhijie Deng","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.12777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12769v1","updated":"2024-10-16T17:44:58Z","published":"2024-10-16T17:44:58Z","title":"Towards Zero-Shot Camera Trap Image Categorization","summary":"  This paper describes the search for an alternative approach to the automatic\ncategorization of camera trap images. First, we benchmark state-of-the-art\nclassifiers using a single model for all images. Next, we evaluate methods\ncombining MegaDetector with one or more classifiers and Segment Anything to\nassess their impact on reducing location-specific overfitting. Last, we propose\nand test two approaches using large language and foundational models, such as\nDINOv2, BioCLIP, BLIP, and ChatGPT, in a zero-shot scenario. Evaluation carried\nout on two publicly available datasets (WCT from New Zealand, CCT20 from the\nSouthwestern US) and a private dataset (CEF from Central Europe) revealed that\ncombining MegaDetector with two separate classifiers achieves the highest\naccuracy. This approach reduced the relative error of a single BEiTV2\nclassifier by approximately 42\\% on CCT20, 48\\% on CEF, and 75\\% on WCT.\nBesides, as the background is removed, the error in terms of accuracy in new\nlocations is reduced to half. The proposed zero-shot pipeline based on DINOv2\nand FAISS achieved competitive results (1.0\\% and 4.7\\% smaller on CCT20, and\nCEF, respectively), which highlights the potential of zero-shot approaches for\ncamera trap image categorization.\n","authors":["Jiří Vyskočil","Lukas Picek"],"pdf_url":"https://arxiv.org/pdf/2410.12769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12763v1","updated":"2024-10-16T17:37:43Z","published":"2024-10-16T17:37:43Z","title":"Gravity-aligned Rotation Averaging with Circular Regression","summary":"  Reconstructing a 3D scene from unordered images is pivotal in computer vision\nand robotics, with applications spanning crowd-sourced mapping and beyond.\nWhile global Structure-from-Motion (SfM) techniques are scalable and fast, they\noften compromise on accuracy. To address this, we introduce a principled\napproach that integrates gravity direction into the rotation averaging phase of\nglobal pipelines, enhancing camera orientation accuracy and reducing the\ndegrees of freedom. This additional information is commonly available in recent\nconsumer devices, such as smartphones, mixed-reality devices and drones, making\nthe proposed method readily accessible. Rooted in circular regression, our\nalgorithm has similar convergence guarantees as linear regression. It also\nsupports scenarios where only a subset of cameras have known gravity.\nAdditionally, we propose a mechanism to refine error-prone gravity. We achieve\nstate-of-the-art accuracy on four large-scale datasets. Particularly, the\nproposed method improves upon the SfM baseline by 13 AUC@$1^\\circ$ points, on\naverage, while running eight times faster. It also outperforms the standard\nplanar pose graph optimization technique by 23 AUC@$1^\\circ$ points. The code\nis at https://github.com/colmap/glomap.\n","authors":["Linfei Pan","Marc Pollefeys","Dániel Baráth"],"pdf_url":"https://arxiv.org/pdf/2410.12763v1.pdf","comment":"accepted at ECCV2024"},{"id":"http://arxiv.org/abs/2410.12761v1","updated":"2024-10-16T17:32:23Z","published":"2024-10-16T17:32:23Z","title":"SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And\n  Video Generation","summary":"  Recent advances in diffusion models have significantly enhanced their ability\nto generate high-quality images and videos, but they have also increased the\nrisk of producing unsafe content. Existing unlearning/editing-based methods for\nsafe generation remove harmful concepts from models but face several\nchallenges: (1) They cannot instantly remove harmful concepts without training.\n(2) Their safe generation capabilities depend on collected training data. (3)\nThey alter model weights, risking degradation in quality for content unrelated\nto toxic concepts. To address these, we propose SAFREE, a novel, training-free\napproach for safe T2I and T2V, that does not alter the model's weights.\nSpecifically, we detect a subspace corresponding to a set of toxic concepts in\nthe text embedding space and steer prompt embeddings away from this subspace,\nthereby filtering out harmful content while preserving intended semantics. To\nbalance the trade-off between filtering toxicity and preserving safe concepts,\nSAFREE incorporates a novel self-validating filtering mechanism that\ndynamically adjusts the denoising steps when applying the filtered embeddings.\nAdditionally, we incorporate adaptive re-attention mechanisms within the\ndiffusion latent space to selectively diminish the influence of features\nrelated to toxic concepts at the pixel level. In the end, SAFREE ensures\ncoherent safety checking, preserving the fidelity, quality, and safety of the\noutput. SAFREE achieves SOTA performance in suppressing unsafe content in T2I\ngeneration compared to training-free baselines and effectively filters targeted\nconcepts while maintaining high-quality images. It also shows competitive\nresults against training-based methods. We extend SAFREE to various T2I\nbackbones and T2V tasks, showcasing its flexibility and generalization. SAFREE\nprovides a robust and adaptable safeguard for ensuring safe visual generation.\n","authors":["Jaehong Yoon","Shoubin Yu","Vaidehi Patil","Huaxiu Yao","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.12761v1.pdf","comment":"The first two authors contributed equally; Project page:\n  https://safree-safe-t2i-t2v.github.io/"},{"id":"http://arxiv.org/abs/2410.10551v2","updated":"2024-10-16T17:04:50Z","published":"2024-10-14T14:32:05Z","title":"Preserving Cardiac Integrity: A Topology-Infused Approach to Whole Heart\n  Segmentation","summary":"  Whole heart segmentation (WHS) supports cardiovascular disease (CVD)\ndiagnosis, disease monitoring, treatment planning, and prognosis. Deep learning\nhas become the most widely used method for WHS applications in recent years.\nHowever, segmentation of whole-heart structures faces numerous challenges\nincluding heart shape variability during the cardiac cycle, clinical artifacts\nlike motion and poor contrast-to-noise ratio, domain shifts in multi-center\ndata, and the distinct modalities of CT and MRI. To address these limitations\nand improve segmentation quality, this paper introduces a new\ntopology-preserving module that is integrated into deep neural networks. The\nimplementation achieves anatomically plausible segmentation by using learned\ntopology-preserving fields, which are based entirely on 3D convolution and are\ntherefore very effective for 3D voxel data. We incorporate natural constraints\nbetween structures into the end-to-end training and enrich the feature\nrepresentation of the neural network. The effectiveness of the proposed method\nis validated on an open-source medical heart dataset, specifically using the\nWHS++ data. The results demonstrate that the architecture performs\nexceptionally well, achieving a Dice coefficient of 0.939 during testing. This\nindicates full topology preservation for individual structures and\nsignificantly outperforms other baselines in preserving the overall scene\ntopology.\n","authors":["Chenyu Zhang","Wenxue Guan","Xiaodan Xing","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.10551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12742v1","updated":"2024-10-16T17:01:28Z","published":"2024-10-16T17:01:28Z","title":"PND-Net: Plant Nutrition Deficiency and Disease Classification using\n  Graph Convolutional Network","summary":"  Crop yield production could be enhanced for agricultural growth if various\nplant nutrition deficiencies, and diseases are identified and detected at early\nstages. The deep learning methods have proven its superior performances in the\nautomated detection of plant diseases and nutrition deficiencies from visual\nsymptoms in leaves. This article proposes a new deep learning method for plant\nnutrition deficiencies and disease classification using a graph convolutional\nnetwork (GNN), added upon a base convolutional neural network (CNN). Sometimes,\na global feature descriptor might fail to capture the vital region of a\ndiseased leaf, which causes inaccurate classification of disease. To address\nthis issue, regional feature learning is crucial for a holistic feature\naggregation. In this work, region-based feature summarization at multi-scales\nis explored using spatial pyramidal pooling for discriminative feature\nrepresentation. A GCN is developed to capacitate learning of finer details for\nclassifying plant diseases and insufficiency of nutrients. The proposed method,\ncalled Plant Nutrition Deficiency and Disease Network (PND-Net), is evaluated\non two public datasets for nutrition deficiency, and two for disease\nclassification using four CNNs. The best classification performances are: (a)\n90.00% Banana and 90.54% Coffee nutrition deficiency; and (b) 96.18% Potato\ndiseases and 84.30% on PlantDoc datasets using Xception backbone. Furthermore,\nadditional experiments have been carried out for generalization, and the\nproposed method has achieved state-of-the-art performances on two public\ndatasets, namely the Breast Cancer Histopathology Image Classification\n(BreakHis 40X: 95.50%, and BreakHis 100X: 96.79% accuracy) and Single cells in\nPap smear images for cervical cancer classification (SIPaKMeD: 99.18%\naccuracy). Also, PND-Net achieves improved performances using five-fold cross\nvalidation.\n","authors":["Asish Bera","Debotosh Bhattacharjee","Ondrej Krejcar"],"pdf_url":"https://arxiv.org/pdf/2410.12742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12725v1","updated":"2024-10-16T16:36:23Z","published":"2024-10-16T16:36:23Z","title":"Optimizing 3D Geometry Reconstruction from Implicit Neural\n  Representations","summary":"  Implicit neural representations have emerged as a powerful tool in learning\n3D geometry, offering unparalleled advantages over conventional representations\nlike mesh-based methods. A common type of INR implicitly encodes a shape's\nboundary as the zero-level set of the learned continuous function and learns a\nmapping from a low-dimensional latent space to the space of all possible shapes\nrepresented by its signed distance function. However, most INRs struggle to\nretain high-frequency details, which are crucial for accurate geometric\ndepiction, and they are computationally expensive. To address these\nlimitations, we present a novel approach that both reduces computational\nexpenses and enhances the capture of fine details. Our method integrates\nperiodic activation functions, positional encodings, and normals into the\nneural network architecture. This integration significantly enhances the\nmodel's ability to learn the entire space of 3D shapes while preserving\nintricate details and sharp features, areas where conventional representations\noften fall short.\n","authors":["Shen Fan","Przemyslaw Musialski"],"pdf_url":"https://arxiv.org/pdf/2410.12725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12718v1","updated":"2024-10-16T16:28:08Z","published":"2024-10-16T16:28:08Z","title":"RAFA-Net: Region Attention Network For Food Items And Agricultural\n  Stress Recognition","summary":"  Deep Convolutional Neural Networks (CNNs) have facilitated remarkable success\nin recognizing various food items and agricultural stress. A decent performance\nboost has been witnessed in solving the agro-food challenges by mining and\nanalyzing of region-based partial feature descriptors. Also, computationally\nexpensive ensemble learning schemes using multiple CNNs have been studied in\nearlier works. This work proposes a region attention scheme for modelling\nlong-range dependencies by building a correlation among different regions\nwithin an input image. The attention method enhances feature representation by\nlearning the usefulness of context information from complementary regions.\nSpatial pyramidal pooling and average pooling pair aggregate partial\ndescriptors into a holistic representation. Both pooling methods establish\nspatial and channel-wise relationships without incurring extra parameters. A\ncontext gating scheme is applied to refine the descriptiveness of weighted\nattentional features, which is relevant for classification. The proposed Region\nAttention network for Food items and Agricultural stress recognition method,\ndubbed RAFA-Net, has been experimented on three public food datasets, and has\nachieved state-of-the-art performances with distinct margins. The highest top-1\naccuracies of RAFA-Net are 91.69%, 91.56%, and 96.97% on the UECFood-100,\nUECFood-256, and MAFood-121 datasets, respectively. In addition, better\naccuracies have been achieved on two benchmark agricultural stress datasets.\nThe best top-1 accuracies on the Insect Pest (IP-102) and PlantDoc-27 plant\ndisease datasets are 92.36%, and 85.54%, respectively; implying RAFA-Net's\ngeneralization capability.\n","authors":["Asish Bera","Ondrej Krejcar","Debotosh Bhattacharjee"],"pdf_url":"https://arxiv.org/pdf/2410.12718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12705v1","updated":"2024-10-16T16:11:49Z","published":"2024-10-16T16:11:49Z","title":"WorldCuisines: A Massive-Scale Benchmark for Multilingual and\n  Multicultural Visual Question Answering on Global Cuisines","summary":"  Vision Language Models (VLMs) often struggle with culture-specific knowledge,\nparticularly in languages other than English and in underrepresented cultural\ncontexts. To evaluate their understanding of such knowledge, we introduce\nWorldCuisines, a massive-scale benchmark for multilingual and multicultural,\nvisually grounded language understanding. This benchmark includes a visual\nquestion answering (VQA) dataset with text-image pairs across 30 languages and\ndialects, spanning 9 language families and featuring over 1 million data\npoints, making it the largest multicultural VQA benchmark to date. It includes\ntasks for identifying dish names and their origins. We provide evaluation\ndatasets in two sizes (12k and 60k instances) alongside a training dataset (1\nmillion instances). Our findings show that while VLMs perform better with\ncorrect location context, they struggle with adversarial contexts and\npredicting specific regional cuisines and languages. To support future\nresearch, we release a knowledge base with annotated food entries and images\nalong with the VQA data.\n","authors":["Genta Indra Winata","Frederikus Hudi","Patrick Amadeus Irawan","David Anugraha","Rifki Afina Putri","Yutong Wang","Adam Nohejl","Ubaidillah Ariq Prathama","Nedjma Ousidhoum","Afifa Amriani","Anar Rzayev","Anirban Das","Ashmari Pramodya","Aulia Adila","Bryan Wilie","Candy Olivia Mawalim","Ching Lam Cheng","Daud Abolade","Emmanuele Chersoni","Enrico Santus","Fariz Ikhwantri","Garry Kuwanto","Hanyang Zhao","Haryo Akbarianto Wibowo","Holy Lovenia","Jan Christian Blaise Cruz","Jan Wira Gotama Putra","Junho Myung","Lucky Susanto","Maria Angelica Riera Machin","Marina Zhukova","Michael Anugraha","Muhammad Farid Adilazuarda","Natasha Santosa","Peerat Limkonchotiwat","Raj Dabre","Rio Alexander Audino","Samuel Cahyawijaya","Shi-Xiong Zhang","Stephanie Yulia Salim","Yi Zhou","Yinxuan Gui","David Ifeoluwa Adelani","En-Shiun Annie Lee","Shogo Okada","Ayu Purwarianti","Alham Fikri Aji","Taro Watanabe","Derry Tanti Wijaya","Alice Oh","Chong-Wah Ngo"],"pdf_url":"https://arxiv.org/pdf/2410.12705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12700v1","updated":"2024-10-16T16:03:42Z","published":"2024-10-16T16:03:42Z","title":"Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via\n  Lightweight Value Optimization","summary":"  Recent advancements in diffusion models trained on large-scale data have\nenabled the generation of indistinguishable human-level images, yet they often\nproduce harmful content misaligned with human values, e.g., social bias, and\noffensive content. Despite extensive research on Large Language Models (LLMs),\nthe challenge of Text-to-Image (T2I) model alignment remains largely\nunexplored. Addressing this problem, we propose LiVO (Lightweight Value\nOptimization), a novel lightweight method for aligning T2I models with human\nvalues. LiVO only optimizes a plug-and-play value encoder to integrate a\nspecified value principle with the input prompt, allowing the control of\ngenerated images over both semantics and values. Specifically, we design a\ndiffusion model-tailored preference optimization loss, which theoretically\napproximates the Bradley-Terry model used in LLM alignment but provides a more\nflexible trade-off between image quality and value conformity. To optimize the\nvalue encoder, we also develop a framework to automatically construct a\ntext-image preference dataset of 86k (prompt, aligned image, violating image,\nvalue principle) samples. Without updating most model parameters and through\nadaptive value selection from the input prompt, LiVO significantly reduces\nharmful outputs and achieves faster convergence, surpassing several strong\nbaselines and taking an initial step towards ethically aligned T2I models.\n","authors":["Xingqi Wang","Xiaoyuan Yi","Xing Xie","Jia Jia"],"pdf_url":"https://arxiv.org/pdf/2410.12700v1.pdf","comment":"Accepted by ACM Multimedia 2024. The dataset and code can be found at\n  https://github.com/achernarwang/LiVO"},{"id":"http://arxiv.org/abs/2410.12696v1","updated":"2024-10-16T15:59:02Z","published":"2024-10-16T15:59:02Z","title":"AdaptiveDrag: Semantic-Driven Dragging on Diffusion-Based Image Editing","summary":"  Recently, several point-based image editing methods (e.g., DragDiffusion,\nFreeDrag, DragNoise) have emerged, yielding precise and high-quality results\nbased on user instructions. However, these methods often make insufficient use\nof semantic information, leading to less desirable results. In this paper, we\nproposed a novel mask-free point-based image editing method, AdaptiveDrag,\nwhich provides a more flexible editing approach and generates images that\nbetter align with user intent. Specifically, we design an auto mask generation\nmodule using super-pixel division for user-friendliness. Next, we leverage a\npre-trained diffusion model to optimize the latent, enabling the dragging of\nfeatures from handle points to target points. To ensure a comprehensive\nconnection between the input image and the drag process, we have developed a\nsemantic-driven optimization. We design adaptive steps that are supervised by\nthe positions of the points and the semantic regions derived from super-pixel\nsegmentation. This refined optimization process also leads to more realistic\nand accurate drag results. Furthermore, to address the limitations in the\ngenerative consistency of the diffusion model, we introduce an innovative\ncorresponding loss during the sampling process. Building on these effective\ndesigns, our method delivers superior generation results using only the single\ninput image and the handle-target point pairs. Extensive experiments have been\nconducted and demonstrate that the proposed method outperforms others in\nhandling various drag instructions (e.g., resize, movement, extension) across\ndifferent domains (e.g., animals, human face, land space, clothing).\n","authors":["DuoSheng Chen","Binghui Chen","Yifeng Geng","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2410.12696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12695v1","updated":"2024-10-16T15:58:47Z","published":"2024-10-16T15:58:47Z","title":"MultiCamCows2024 -- A Multi-view Image Dataset for AI-driven\n  Holstein-Friesian Cattle Re-Identification on a Working Farm","summary":"  We present MultiCamCows2024, a farm-scale image dataset filmed across\nmultiple cameras for the biometric identification of individual\nHolstein-Friesian cattle exploiting their unique black and white coat-patterns.\nCaptured by three ceiling-mounted visual sensors covering adjacent barn areas\nover seven days on a working dairy farm, the dataset comprises 101, 329 images\nof 90 cows, plus the underlying original CCTV footage. The dataset is provided\nalongside full computer vision recognition baselines, that is both a supervised\nand self-supervised learning framework for individual cow identification\ntrained on cattle tracklets. We report a performance above 96% single image\nidentification accuracy from the dataset and demonstrate that combining data\nfrom multiple cameras during learning enhances self-supervised identification.\nWe show that our framework enables fully automatic cattle identification,\nbarring only the simple human verification of tracklet integrity during data\ncollection. Crucially, our study highlights that multi-camera, supervised and\nself-supervised components in tandem not only deliver highly accurate\nindividual cow identification but also achieve this efficiently with no\nlabelling of cattle identities by humans at all. We argue that this improvement\nin efficacy has practical implications for livestock management, behaviour\nanalysis, and agricultural monitoring. For full reproducibility and practical\nease of use, we publish all key software and code including re-identification\ncomponents and the species detector with this paper.\n","authors":["Phoenix Yu","Tilo Burghardt","Andrew W Dowsey","Neill W Campbell"],"pdf_url":"https://arxiv.org/pdf/2410.12695v1.pdf","comment":"26 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.12694v1","updated":"2024-10-16T15:54:11Z","published":"2024-10-16T15:54:11Z","title":"VividMed: Vision Language Model with Versatile Visual Grounding for\n  Medicine","summary":"  Recent advancements in Vision Language Models (VLMs) have demonstrated\nremarkable promise in generating visually grounded responses. However, their\napplication in the medical domain is hindered by unique challenges. For\ninstance, most VLMs rely on a single method of visual grounding, whereas\ncomplex medical tasks demand more versatile approaches. Additionally, while\nmost VLMs process only 2D images, a large portion of medical images are 3D. The\nlack of medical data further compounds these obstacles. To address these\nchallenges, we present VividMed, a vision language model with versatile visual\ngrounding for medicine. Our model supports generating both semantic\nsegmentation masks and instance-level bounding boxes, and accommodates various\nimaging modalities, including both 2D and 3D data. We design a three-stage\ntraining procedure and an automatic data synthesis pipeline based on open\ndatasets and models. Besides visual grounding tasks, VividMed also excels in\nother common downstream tasks, including Visual Question Answering (VQA) and\nreport generation. Ablation studies empirically show that the integration of\nvisual grounding ability leads to improved performance on these tasks. Our code\nis publicly available at https://github.com/function2-llx/MMMM.\n","authors":["Lingxiao Luo","Bingda Tang","Xuanzhong Chen","Rong Han","Ting Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02181v2","updated":"2024-10-16T15:53:15Z","published":"2024-08-05T01:50:09Z","title":"AssemAI: Interpretable Image-Based Anomaly Detection for Manufacturing\n  Pipelines","summary":"  Anomaly detection in manufacturing pipelines remains a critical challenge,\nintensified by the complexity and variability of industrial environments. This\npaper introduces AssemAI, an interpretable image-based anomaly detection system\ntailored for smart manufacturing pipelines. Utilizing a curated image dataset\nfrom an industry-focused rocket assembly pipeline, we address the challenge of\nimbalanced image data and demonstrate the importance of image-based methods in\nanomaly detection. Our primary contributions include deriving an image dataset,\nfine-tuning an object detection model YOLO-FF, and implementing a custom\nanomaly detection model for assembly pipelines. The proposed approach leverages\ndomain knowledge in data preparation, model development and reasoning. We\nimplement several anomaly detection models on the derived image dataset,\nincluding a Convolutional Neural Network, Vision Transformer (ViT), and\npre-trained versions of these models. Additionally, we incorporate\nexplainability techniques at both user and model levels, utilizing ontology for\nuser-level explanations and SCORE-CAM for in-depth feature and model analysis.\nFinally, the best-performing anomaly detection model and YOLO-FF are deployed\nin a real-time setting. Our results include ablation studies on the baselines\nand a comprehensive evaluation of the proposed system. This work highlights the\nbroader impact of advanced image-based anomaly detection in enhancing the\nreliability and efficiency of smart manufacturing processes. The image dataset,\ncodes to reproduce the results and additional experiments are available at\nhttps://github.com/renjithk4/AssemAI.\n","authors":["Renjith Prasad","Chathurangi Shyalika","Ramtin Zand","Fadi El Kalach","Revathy Venkataramanan","Ramy Harik","Amit Sheth"],"pdf_url":"https://arxiv.org/pdf/2408.02181v2.pdf","comment":"8 Pages, 6 Figures, 4 Tables, Predictive Models in Engineering\n  Applications special session (MLPMEA )at International Conference on Machine\n  Learning and Applications (ICMLA) 2024"},{"id":"http://arxiv.org/abs/2410.12692v1","updated":"2024-10-16T15:52:32Z","published":"2024-10-16T15:52:32Z","title":"Machine Learning Approach to Brain Tumor Detection and Classification","summary":"  Brain tumor detection and classification are critical tasks in medical image\nanalysis, particularly in early-stage diagnosis, where accurate and timely\ndetection can significantly improve treatment outcomes. In this study, we apply\nvarious statistical and machine learning models to detect and classify brain\ntumors using brain MRI images. We explore a variety of statistical models\nincluding linear, logistic, and Bayesian regressions, and the machine learning\nmodels including decision tree, random forest, single-layer perceptron,\nmulti-layer perceptron, convolutional neural network (CNN), recurrent neural\nnetwork, and long short-term memory. Our findings show that CNN outperforms\nother models, achieving the best performance. Additionally, we confirm that the\nCNN model can also work for multi-class classification, distinguishing between\nfour categories of brain MRI images such as normal, glioma, meningioma, and\npituitary tumor images. This study demonstrates that machine learning\napproaches are suitable for brain tumor detection and classification,\nfacilitating real-world medical applications in assisting radiologists with\nearly and accurate diagnosis.\n","authors":["Alice Oh","Inyoung Noh","Jian Choo","Jihoo Lee","Justin Park","Kate Hwang","Sanghyeon Kim","Soo Min Oh"],"pdf_url":"https://arxiv.org/pdf/2410.12692v1.pdf","comment":"7 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.12686v1","updated":"2024-10-16T15:48:28Z","published":"2024-10-16T15:48:28Z","title":"Automatic Mapping of Anatomical Landmarks from Free-Text Using Large\n  Language Models: Insights from Llama-2","summary":"  Anatomical landmarks are vital in medical imaging for navigation and anomaly\ndetection. Modern large language models (LLMs), like Llama-2, offer promise for\nautomating the mapping of these landmarks in free-text radiology reports to\ncorresponding positions in image data. Recent studies propose LLMs may develop\ncoherent representations of generative processes. Motivated by these insights,\nwe investigated whether LLMs accurately represent the spatial positions of\nanatomical landmarks. Through experiments with Llama-2 models, we found that\nthey can linearly represent anatomical landmarks in space with considerable\nrobustness to different prompts. These results underscore the potential of LLMs\nto enhance the efficiency and accuracy of medical imaging workflows.\n","authors":["Mohamad Abdi","Gerardo Hemosillo Valadez","Halid Ziya Yerebakan"],"pdf_url":"https://arxiv.org/pdf/2410.12686v1.pdf","comment":"6 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2405.01474v2","updated":"2024-10-16T15:45:35Z","published":"2024-05-02T17:07:25Z","title":"Understanding Figurative Meaning through Explainable Visual Entailment","summary":"  Large Vision-Language Models (VLMs) have demonstrated strong capabilities in\ntasks requiring a fine-grained understanding of literal meaning in images and\ntext, such as visual question-answering or visual entailment. However, there\nhas been little exploration of these models' capabilities when presented with\nimages and captions containing figurative meaning, such as metaphors or humor.\nTo close this gap, we propose a new task framing the figurative meaning\nunderstanding problem as an explainable visual entailment task, where the model\nhas to predict whether the image (premise) entails a caption (hypothesis) and\njustify the predicted label with a textual explanation. The figurative\nphenomena can be present either in the image, the caption, or both. Utilizing a\nhuman-AI collaboration approach, we build the accompanying expert-verified\ndataset V-FLUTE, containing 6,027 {image, caption, label, explanation}\ninstances spanning five diverse figurative phenomena: metaphors, similes,\nidioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs\nstruggle to generalize from literal to figurative meaning, particularly when it\nis present in images. Further, we identify common types of errors in VLM\nreasoning via human evaluation.\n","authors":["Arkadiy Saakyan","Shreyas Kulkarni","Tuhin Chakrabarty","Smaranda Muresan"],"pdf_url":"https://arxiv.org/pdf/2405.01474v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12673v1","updated":"2024-10-16T15:37:29Z","published":"2024-10-16T15:37:29Z","title":"MambaBEV: An efficient 3D detection model with Mamba2","summary":"  A stable 3D object detection model based on BEV paradigm with temporal\ninformation is very important for autonomous driving systems. However, current\ntemporal fusion model use convolutional layer or deformable self-attention is\nnot conducive to the exchange of global information of BEV space and has more\ncomputational cost. Recently, a newly proposed based model specialized in\nprocessing sequence called mamba has shown great potential in multiple\ndownstream task. In this work, we proposed a mamba2-based BEV 3D object\ndetection model named MambaBEV. We also adapt an end to end self driving\nparadigm to test the performance of the model. Our work performs pretty good\nresults on nucences datasets:Our base version achieves 51.7% NDS. Our code will\nbe available soon.\n","authors":["Zihan You","Hao Wang","Qichao Zhao","Jinxiang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.12673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12669v1","updated":"2024-10-16T15:34:13Z","published":"2024-10-16T15:34:13Z","title":"3DIS: Depth-Driven Decoupled Instance Synthesis for Text-to-Image\n  Generation","summary":"  The increasing demand for controllable outputs in text-to-image generation\nhas spurred advancements in multi-instance generation (MIG), allowing users to\ndefine both instance layouts and attributes. However, unlike image-conditional\ngeneration methods such as ControlNet, MIG techniques have not been widely\nadopted in state-of-the-art models like SD2 and SDXL, primarily due to the\nchallenge of building robust renderers that simultaneously handle instance\npositioning and attribute rendering. In this paper, we introduce Depth-Driven\nDecoupled Instance Synthesis (3DIS), a novel framework that decouples the MIG\nprocess into two stages: (i) generating a coarse scene depth map for accurate\ninstance positioning and scene composition, and (ii) rendering fine-grained\nattributes using pre-trained ControlNet on any foundational model, without\nadditional training. Our 3DIS framework integrates a custom adapter into LDM3D\nfor precise depth-based layouts and employs a finetuning-free method for\nenhanced instance-level attribute rendering. Extensive experiments on\nCOCO-Position and COCO-MIG benchmarks demonstrate that 3DIS significantly\noutperforms existing methods in both layout precision and attribute rendering.\nNotably, 3DIS offers seamless compatibility with diverse foundational models,\nproviding a robust, adaptable solution for advanced multi-instance generation.\nThe code is available at: https://github.com/limuloo/3DIS.\n","authors":["Dewei Zhou","Ji Xie","Zongxin Yang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.12669v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.12662v1","updated":"2024-10-16T15:20:08Z","published":"2024-10-16T15:20:08Z","title":"Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models","summary":"  Vision-language alignment in Large Vision-Language Models (LVLMs)\nsuccessfully enables LLMs to understand visual input. However, we find that\nexisting vision-language alignment methods fail to transfer the existing safety\nmechanism for text in LLMs to vision, which leads to vulnerabilities in toxic\nimage. To explore the cause of this problem, we give the insightful explanation\nof where and how the safety mechanism of LVLMs operates and conduct comparative\nanalysis between text and vision. We find that the hidden states at the\nspecific transformer layers play a crucial role in the successful activation of\nsafety mechanism, while the vision-language alignment at hidden states level in\ncurrent methods is insufficient. This results in a semantic shift for input\nimages compared to text in hidden states, therefore misleads the safety\nmechanism. To address this, we propose a novel Text-Guided vision-language\nAlignment method (TGA) for LVLMs. TGA retrieves the texts related to input\nvision and uses them to guide the projection of vision into the hidden states\nspace in LLMs. Experiments show that TGA not only successfully transfers the\nsafety mechanism for text in basic LLMs to vision in vision-language alignment\nfor LVLMs without any safety fine-tuning on the visual modality but also\nmaintains the general performance on various vision tasks (Safe and Good).\n","authors":["Shicheng Xu","Liang Pang","Yunchang Zhu","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.12662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08601v3","updated":"2024-10-16T15:16:31Z","published":"2024-02-13T17:08:35Z","title":"Latent Inversion with Timestep-aware Sampling for Training-free\n  Non-rigid Editing","summary":"  Text-guided non-rigid editing involves complex edits for input images, such\nas changing motion or compositions within their surroundings. Since it requires\nmanipulating the input structure, existing methods often struggle with\npreserving object identity and background, particularly when combined with\nStable Diffusion. In this work, we propose a training-free approach for\nnon-rigid editing with Stable Diffusion, aimed at improving the identity\npreservation quality without compromising editability. Our approach comprises\nthree stages: text optimization, latent inversion, and timestep-aware text\ninjection sampling. Inspired by the success of Imagic, we employ their text\noptimization for smooth editing. Then, we introduce latent inversion to\npreserve the input image's identity without additional model fine-tuning. To\nfully utilize the input reconstruction ability of latent inversion, we suggest\ntimestep-aware text injection sampling. This effectively retains the structure\nof the input image by injecting the source text prompt in early sampling steps\nand then transitioning to the target prompt in subsequent sampling steps. This\nstrategic approach seamlessly harmonizes with text optimization, facilitating\ncomplex non-rigid edits to the input without losing the original identity. We\ndemonstrate the effectiveness of our method in terms of identity preservation,\neditability, and aesthetic quality through extensive experiments.\n","authors":["Yunji Jung","Seokju Lee","Tair Djanibekov","Hyunjung Shim","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2402.08601v3.pdf","comment":"This manuscript has been submitted to Pattern Recognition Letters"},{"id":"http://arxiv.org/abs/2410.12641v1","updated":"2024-10-16T15:00:31Z","published":"2024-10-16T15:00:31Z","title":"Cascade learning in multi-task encoder-decoder networks for concurrent\n  bone segmentation and glenohumeral joint assessment in shoulder CT scans","summary":"  Osteoarthritis is a degenerative condition affecting bones and cartilage,\noften leading to osteophyte formation, bone density loss, and joint space\nnarrowing. Treatment options to restore normal joint function vary depending on\nthe severity of the condition. This work introduces an innovative deep-learning\nframework processing shoulder CT scans. It features the semantic segmentation\nof the proximal humerus and scapula, the 3D reconstruction of bone surfaces,\nthe identification of the glenohumeral (GH) joint region, and the staging of\nthree common osteoarthritic-related pathologies: osteophyte formation (OS), GH\nspace reduction (JS), and humeroscapular alignment (HSA). The pipeline\ncomprises two cascaded CNN architectures: 3D CEL-UNet for segmentation and 3D\nArthro-Net for threefold classification. A retrospective dataset of 571 CT\nscans featuring patients with various degrees of GH osteoarthritic-related\npathologies was used to train, validate, and test the pipeline. Root mean\nsquared error and Hausdorff distance median values for 3D reconstruction were\n0.22mm and 1.48mm for the humerus and 0.24mm and 1.48mm for the scapula,\noutperforming state-of-the-art architectures and making it potentially suitable\nfor a PSI-based shoulder arthroplasty preoperative plan context. The\nclassification accuracy for OS, JS, and HSA consistently reached around 90%\nacross all three categories. The computational time for the inference pipeline\nwas less than 15s, showcasing the framework's efficiency and compatibility with\northopedic radiology practice. The outcomes represent a promising advancement\ntoward the medical translation of artificial intelligence tools. This progress\naims to streamline the preoperative planning pipeline delivering high-quality\nbone surfaces and supporting surgeons in selecting the most suitable surgical\napproach according to the unique patient joint conditions.\n","authors":["Luca Marsilio","Davide Marzorati","Matteo Rossi","Andrea Moglia","Luca Mainardi","Alfonso Manzotti","Pietro Cerveri"],"pdf_url":"https://arxiv.org/pdf/2410.12641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12628v1","updated":"2024-10-16T14:50:47Z","published":"2024-10-16T14:50:47Z","title":"DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse\n  Synthetic Data and Global-to-Local Adaptive Perception","summary":"  Document Layout Analysis is crucial for real-world document understanding\nsystems, but it encounters a challenging trade-off between speed and accuracy:\nmultimodal methods leveraging both text and visual features achieve higher\naccuracy but suffer from significant latency, whereas unimodal methods relying\nsolely on visual features offer faster processing speeds at the expense of\naccuracy. To address this dilemma, we introduce DocLayout-YOLO, a novel\napproach that enhances accuracy while maintaining speed advantages through\ndocument-specific optimizations in both pre-training and model design. For\nrobust document pre-training, we introduce the Mesh-candidate BestFit\nalgorithm, which frames document synthesis as a two-dimensional bin packing\nproblem, generating the large-scale, diverse DocSynth-300K dataset.\nPre-training on the resulting DocSynth-300K dataset significantly improves\nfine-tuning performance across various document types. In terms of model\noptimization, we propose a Global-to-Local Controllable Receptive Module that\nis capable of better handling multi-scale variations of document elements.\nFurthermore, to validate performance across different document types, we\nintroduce a complex and challenging benchmark named DocStructBench. Extensive\nexperiments on downstream datasets demonstrate that DocLayout-YOLO excels in\nboth speed and accuracy. Code, data, and models are available at\nhttps://github.com/opendatalab/DocLayout-YOLO.\n","authors":["Zhiyuan Zhao","Hengrui Kang","Bin Wang","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2410.12628v1.pdf","comment":"Github Repo: https://github.com/opendatalab/DocLayout-YOLO"},{"id":"http://arxiv.org/abs/2410.12613v1","updated":"2024-10-16T14:29:29Z","published":"2024-10-16T14:29:29Z","title":"Exploring Model Kinship for Merging Large Language Models","summary":"  Model merging has become one of the key technologies for enhancing the\ncapabilities and efficiency of Large Language Models (LLMs). However, our\nunderstanding of the expected performance gains and principles when merging any\ntwo models remains limited. In this work, we introduce model kinship, the\ndegree of similarity or relatedness between LLMs, analogous to biological\nevolution. With comprehensive empirical analysis, we find that there is a\ncertain relationship between model kinship and the performance gains after\nmodel merging, which can help guide our selection of candidate models. Inspired\nby this, we propose a new model merging strategy: Top-k Greedy Merging with\nModel Kinship, which can yield better performance on benchmark datasets.\nSpecifically, we discover that using model kinship as a criterion can assist us\nin continuously performing model merging, alleviating the degradation (local\noptima) in model evolution, whereas model kinship can serve as a guide to\nescape these traps. Code is available at\nhttps://github.com/zjunlp/ModelKinship.\n","authors":["Yedi Hu","Yunzhi Yao","Ningyu Zhang","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12613v1.pdf","comment":"Ongoing work"},{"id":"http://arxiv.org/abs/2401.12452v3","updated":"2024-10-16T14:19:28Z","published":"2024-01-23T02:41:06Z","title":"Self-supervised Learning of LiDAR 3D Point Clouds via 2D-3D Neural\n  Calibration","summary":"  This paper introduces a novel self-supervised learning framework for\nenhancing 3D perception in autonomous driving scenes. Specifically, our\napproach, namely NCLR, focuses on 2D-3D neural calibration, a novel pretext\ntask that estimates the rigid pose aligning camera and LiDAR coordinate\nsystems. First, we propose the learnable transformation alignment to bridge the\ndomain gap between image and point cloud data, converting features into a\nunified representation space for effective comparison and matching. Second, we\nidentify the overlapping area between the image and point cloud with the fused\nfeatures. Third, we establish dense 2D-3D correspondences to estimate the rigid\npose. The framework not only learns fine-grained matching from points to pixels\nbut also achieves alignment of the image and point cloud at a holistic level,\nunderstanding their relative pose. We demonstrate the efficacy of NCLR by\napplying the pre-trained backbone to downstream tasks, such as LiDAR-based 3D\nsemantic segmentation, object detection, and panoptic segmentation.\nComprehensive experiments on various datasets illustrate the superiority of\nNCLR over existing self-supervised methods. The results confirm that joint\nlearning from different modalities significantly enhances the network's\nunderstanding abilities and effectiveness of learned representation. The code\nis publicly available at https://github.com/Eaphan/NCLR.\n","authors":["Yifan Zhang","Siyu Ren","Junhui Hou","Jinjian Wu","Yixuan Yuan","Guangming Shi"],"pdf_url":"https://arxiv.org/pdf/2401.12452v3.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.11808v2","updated":"2024-10-16T14:18:53Z","published":"2024-03-18T14:05:52Z","title":"Dynamic Tuning Towards Parameter and Inference Efficiency for ViT\n  Adaptation","summary":"  Existing parameter-efficient fine-tuning (PEFT) methods have achieved\nsignificant success on vision transformers (ViTs) adaptation by improving\nparameter efficiency. However, the exploration of enhancing inference\nefficiency during adaptation remains underexplored. This limits the broader\napplication of pre-trained ViT models, especially when the model is\ncomputationally extensive. In this paper, we propose Dynamic Tuning (DyT), a\nnovel approach to improve both parameter and inference efficiency for ViT\nadaptation. Specifically, besides using the lightweight adapter modules, we\npropose a token dispatcher to distinguish informative tokens from less\nimportant ones, allowing the latter to dynamically skip the original block,\nthereby reducing the redundant computation during inference. Additionally, we\nexplore multiple design variants to find the best practice of DyT. Finally,\ninspired by the mixture-of-experts (MoE) mechanism, we introduce an enhanced\nadapter to further boost the adaptation performance. We validate DyT across\nvarious tasks, including image/video recognition and semantic segmentation. For\ninstance, DyT achieves superior performance compared to existing PEFT methods\nwhile evoking only 71% of their FLOPs on the VTAB-1K benchmark.\n","authors":["Wangbo Zhao","Jiasheng Tang","Yizeng Han","Yibing Song","Kai Wang","Gao Huang","Fan Wang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2403.11808v2.pdf","comment":"Accepted to NeurIPS2024"},{"id":"http://arxiv.org/abs/2407.09797v2","updated":"2024-10-16T14:16:21Z","published":"2024-07-13T07:58:48Z","title":"ScaleFlow++: Robust and Accurate Estimation of 3D Motion from Video","summary":"  Perceiving and understanding 3D motion is a core technology in fields such as\nautonomous driving, robots, and motion prediction. This paper proposes a 3D\nmotion perception method called ScaleFlow++ that is easy to generalize. With\njust a pair of RGB images, ScaleFlow++ can robustly estimate optical flow and\nmotion-in-depth (MID). Most existing methods directly regress MID from two RGB\nframes or optical flow, resulting in inaccurate and unstable results. Our key\ninsight is cross-scale matching, which extracts deep motion clues by matching\nobjects in pairs of images at different scales. Unlike previous methods,\nScaleFlow++ integrates optical flow and MID estimation into a unified\narchitecture, estimating optical flow and MID end-to-end based on feature\nmatching. Moreover, we also proposed modules such as global initialization\nnetwork, global iterative optimizer, and hybrid training pipeline to integrate\nglobal motion information, reduce the number of iterations, and prevent\noverfitting during training. On KITTI, ScaleFlow++ achieved the best monocular\nscene flow estimation performance, reducing SF-all from 6.21 to 5.79. The\nevaluation of MID even surpasses RGBD-based methods. In addition, ScaleFlow++\nhas achieved stunning zero-shot generalization performance in both rigid and\nnonrigid scenes. Code is available at\n\\url{https://github.com/HanLingsgjk/CSCV}.\n","authors":["Han Ling","Quansen Sun"],"pdf_url":"https://arxiv.org/pdf/2407.09797v2.pdf","comment":"14 pages; Previously this version appeared as arXiv:2409.12202 which\n  was submitted as a new work by accident"},{"id":"http://arxiv.org/abs/2410.12595v1","updated":"2024-10-16T14:12:26Z","published":"2024-10-16T14:12:26Z","title":"CMAL: A Novel Cross-Modal Associative Learning Framework for\n  Vision-Language Pre-Training","summary":"  With the flourishing of social media platforms, vision-language pre-training\n(VLP) recently has received great attention and many remarkable progresses have\nbeen achieved. The success of VLP largely benefits from the information\ncomplementation and enhancement between different modalities. However, most of\nrecent studies focus on cross-modal contrastive learning (CMCL) to promote\nimage-text alignment by pulling embeddings of positive sample pairs together\nwhile pushing those of negative pairs apart, which ignores the natural\nasymmetry property between different modalities and requires large-scale\nimage-text corpus to achieve arduous progress. To mitigate this predicament, we\npropose CMAL, a Cross-Modal Associative Learning framework with anchor points\ndetection and cross-modal associative learning for VLP. Specifically, we first\nrespectively embed visual objects and textual tokens into separate hypersphere\nspaces to learn intra-modal hidden features, and then design a cross-modal\nassociative prompt layer to perform anchor point masking and swap feature\nfilling for constructing a hybrid cross-modal associative prompt. Afterwards,\nwe exploit a unified semantic encoder to learn their cross-modal interactive\nfeatures for context adaptation. Finally, we design an associative mapping\nclassification layer to learn potential associative mappings between modalities\nat anchor points, within which we develop a fresh self-supervised associative\nmapping classification task to boost CMAL's performance. Experimental results\nverify the effectiveness of CMAL, showing that it achieves competitive\nperformance against previous CMCL-based methods on four common downstream\nvision-and-language tasks, with significantly fewer corpus. Especially, CMAL\nobtains new state-of-the-art results on SNLI-VE and REC (testA).\n","authors":["Zhiyuan Ma","Jianjun Li","Guohui Li","Kaiyan Huang"],"pdf_url":"https://arxiv.org/pdf/2410.12595v1.pdf","comment":"vision-language pre-training, contrastive learning, cross-modal,\n  associative learning, associative mapping classification"},{"id":"http://arxiv.org/abs/2410.12592v1","updated":"2024-10-16T14:10:53Z","published":"2024-10-16T14:10:53Z","title":"Cocoon: Robust Multi-Modal Perception with Uncertainty-Aware Sensor\n  Fusion","summary":"  An important paradigm in 3D object detection is the use of multiple\nmodalities to enhance accuracy in both normal and challenging conditions,\nparticularly for long-tail scenarios. To address this, recent studies have\nexplored two directions of adaptive approaches: MoE-based adaptive fusion,\nwhich struggles with uncertainties arising from distinct object configurations,\nand late fusion for output-level adaptive fusion, which relies on separate\ndetection pipelines and limits comprehensive understanding. In this work, we\nintroduce Cocoon, an object- and feature-level uncertainty-aware fusion\nframework. The key innovation lies in uncertainty quantification for\nheterogeneous representations, enabling fair comparison across modalities\nthrough the introduction of a feature aligner and a learnable surrogate ground\ntruth, termed feature impression. We also define a training objective to ensure\nthat their relationship provides a valid metric for uncertainty quantification.\nCocoon consistently outperforms existing static and adaptive methods in both\nnormal and challenging conditions, including those with natural and artificial\ncorruptions. Furthermore, we show the validity and efficacy of our uncertainty\nmetric across diverse datasets.\n","authors":["Minkyoung Cho","Yulong Cao","Jiachen Sun","Qingzhao Zhang","Marco Pavone","Jeong Joon Park","Heng Yang","Z. Morley Mao"],"pdf_url":"https://arxiv.org/pdf/2410.12592v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2410.12591v1","updated":"2024-10-16T14:10:48Z","published":"2024-10-16T14:10:48Z","title":"Rethinking Visual Counterfactual Explanations Through Region Constraint","summary":"  Visual counterfactual explanations (VCEs) have recently gained immense\npopularity as a tool for clarifying the decision-making process of image\nclassifiers. This trend is largely motivated by what these explanations promise\nto deliver -- indicate semantically meaningful factors that change the\nclassifier's decision. However, we argue that current state-of-the-art\napproaches lack a crucial component -- the region constraint -- whose absence\nprevents from drawing explicit conclusions, and may even lead to faulty\nreasoning due to phenomenons like confirmation bias. To address the issue of\nprevious methods, which modify images in a very entangled and widely dispersed\nmanner, we propose region-constrained VCEs (RVCEs), which assume that only a\npredefined image region can be modified to influence the model's prediction. To\neffectively sample from this subclass of VCEs, we propose Region-Constrained\nCounterfactual Schr\\\"odinger Bridges (RCSB), an adaptation of a tractable\nsubclass of Schr\\\"odinger Bridges to the problem of conditional inpainting,\nwhere the conditioning signal originates from the classifier of interest. In\naddition to setting a new state-of-the-art by a large margin, we extend RCSB to\nallow for exact counterfactual reasoning, where the predefined region contains\nonly the factor of interest, and incorporating the user to actively interact\nwith the RVCE by predefining the regions manually.\n","authors":["Bartlomiej Sobieski","Jakub Grzywaczewski","Bartlomiej Sadlej","Matthew Tivnan","Przemyslaw Biecek"],"pdf_url":"https://arxiv.org/pdf/2410.12591v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.12589v1","updated":"2024-10-16T14:10:15Z","published":"2024-10-16T14:10:15Z","title":"From Lab to Pocket: A Novel Continual Learning-based Mobile Application\n  for Screening COVID-19","summary":"  Artificial intelligence (AI) has emerged as a promising tool for predicting\nCOVID-19 from medical images. In this paper, we propose a novel continual\nlearning-based approach and present the design and implementation of a mobile\napplication for screening COVID-19. Our approach demonstrates the ability to\nadapt to evolving datasets, including data collected from different locations\nor hospitals, varying virus strains, and diverse clinical presentations,\nwithout retraining from scratch. We have evaluated state-of-the-art continual\nlearning methods for detecting COVID-19 from chest X-rays and selected the\nbest-performing model for our mobile app. We evaluated various deep learning\narchitectures to select the best-performing one as a foundation model for\ncontinual learning. Both regularization and memory-based methods for continual\nlearning were tested, using different memory sizes to develop the optimal\ncontinual learning model for our app. DenseNet161 emerged as the best\nfoundation model with 96.87\\% accuracy, and Learning without Forgetting (LwF)\nwas the top continual learning method with an overall performance of 71.99\\%.\nThe mobile app design considers both patient and doctor perspectives. It\nincorporates the continual learning DenseNet161 LwF model on a cloud server,\nenabling the model to learn from new instances of chest X-rays and their\nclassifications as they are submitted. The app is designed, implemented, and\nevaluated to ensure it provides an efficient tool for COVID-19 screening. The\napp is available to download from\nhttps://github.com/DannyFGitHub/COVID-19PneumoCheckApp.\n","authors":["Danny Falero","Muhammad Ashad Kabir","Nusrat Homaira"],"pdf_url":"https://arxiv.org/pdf/2410.12589v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2410.08469v2","updated":"2024-10-16T14:09:14Z","published":"2024-10-11T02:42:13Z","title":"Semantic Token Reweighting for Interpretable and Controllable Text\n  Embeddings in CLIP","summary":"  A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial\nrole in translating textual input into an embedding space shared with images,\nthereby facilitating the interpretative analysis of vision tasks through\nnatural language. Despite the varying significance of different textual\nelements within a sentence depending on the context, efforts to account for\nvariation of importance in constructing text embeddings have been lacking. We\npropose a framework of Semantic Token Reweighting to build Interpretable text\nembeddings (SToRI), which incorporates controllability as well. SToRI refines\nthe text encoding process in CLIP by differentially weighting semantic elements\nbased on contextual importance, enabling finer control over emphasis responsive\nto data-driven insights and user preferences. The efficacy of SToRI is\ndemonstrated through comprehensive experiments on few-shot image classification\nand image retrieval tailored to user preferences.\n","authors":["Eunji Kim","Kyuhong Shim","Simyung Chang","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2410.08469v2.pdf","comment":"Accepted at EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.12584v1","updated":"2024-10-16T14:04:06Z","published":"2024-10-16T14:04:06Z","title":"Self-DenseMobileNet: A Robust Framework for Lung Nodule Classification\n  using Self-ONN and Stacking-based Meta-Classifier","summary":"  In this study, we propose a novel and robust framework, Self-DenseMobileNet,\ndesigned to enhance the classification of nodules and non-nodules in chest\nradiographs (CXRs). Our approach integrates advanced image standardization and\nenhancement techniques to optimize the input quality, thereby improving\nclassification accuracy. To enhance predictive accuracy and leverage the\nstrengths of multiple models, the prediction probabilities from\nSelf-DenseMobileNet were transformed into tabular data and used to train eight\nclassical machine learning (ML) models; the top three performers were then\ncombined via a stacking algorithm, creating a robust meta-classifier that\nintegrates their collective insights for superior classification performance.\nTo enhance the interpretability of our results, we employed class activation\nmapping (CAM) to visualize the decision-making process of the best-performing\nmodel. Our proposed framework demonstrated remarkable performance on internal\nvalidation data, achieving an accuracy of 99.28\\% using a Meta-Random Forest\nClassifier. When tested on an external dataset, the framework maintained strong\ngeneralizability with an accuracy of 89.40\\%. These results highlight a\nsignificant improvement in the classification of CXRs with lung nodules.\n","authors":["Md. Sohanur Rahman","Muhammad E. H. Chowdhury","Hasib Ryan Rahman","Mosabber Uddin Ahmed","Muhammad Ashad Kabir","Sanjiban Sekhar Roy","Rusab Sarmun"],"pdf_url":"https://arxiv.org/pdf/2410.12584v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2410.11639v2","updated":"2024-10-16T13:48:37Z","published":"2024-10-15T14:29:47Z","title":"Efficient and Effective Universal Adversarial Attack against\n  Vision-Language Pre-training Models","summary":"  Vision-language pre-training (VLP) models, trained on large-scale image-text\npairs, have become widely used across a variety of downstream\nvision-and-language (V+L) tasks. This widespread adoption raises concerns about\ntheir vulnerability to adversarial attacks. Non-universal adversarial attacks,\nwhile effective, are often impractical for real-time online applications due to\ntheir high computational demands per data instance. Recently, universal\nadversarial perturbations (UAPs) have been introduced as a solution, but\nexisting generator-based UAP methods are significantly time-consuming. To\novercome the limitation, we propose a direct optimization-based UAP approach,\ntermed DO-UAP, which significantly reduces resource consumption while\nmaintaining high attack performance. Specifically, we explore the necessity of\nmultimodal loss design and introduce a useful data augmentation strategy.\nExtensive experiments conducted on three benchmark VLP datasets, six popular\nVLP models, and three classical downstream tasks demonstrate the efficiency and\neffectiveness of DO-UAP. Specifically, our approach drastically decreases the\ntime consumption by 23-fold while achieving a better attack performance.\n","authors":["Fan Yang","Yihao Huang","Kailong Wang","Ling Shi","Geguang Pu","Yang Liu","Haoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2410.11639v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2402.15704v4","updated":"2024-10-16T13:41:53Z","published":"2024-02-24T03:44:06Z","title":"Adaptive Convolutional Neural Network for Image Super-resolution","summary":"  Convolutional neural networks can automatically learn features via deep\nnetwork architectures and given input samples. However, the robustness of\nobtained models may face challenges in varying scenes. Bigger differences in\nnetwork architecture are beneficial to extract more diversified structural\ninformation to strengthen the robustness of an obtained super-resolution model.\nIn this paper, we proposed a adaptive convolutional neural network for image\nsuper-resolution (ADSRNet). To capture more information, ADSRNet is implemented\nby a heterogeneous parallel network. The upper network can enhance relation of\ncontext information, salient information relation of a kernel mapping and\nrelations of shallow and deep layers to improve performance of image\nsuper-resolution. That can strengthen adaptability of an obtained\nsuper-resolution model for different scenes. The lower network utilizes a\nsymmetric architecture to enhance relations of different layers to mine more\nstructural information, which is complementary with a upper network for image\nsuper-resolution. The relevant experimental results show that the proposed\nADSRNet is effective to deal with image resolving. Codes are obtained at\nhttps://github.com/hellloxiaotian/ADSRNet.\n","authors":["Chunwei Tian","Xuanyu Zhang","Tao Wang","Yongjun Zhang","Qi Zhu","Chia-Wen Lin"],"pdf_url":"https://arxiv.org/pdf/2402.15704v4.pdf","comment":"11pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.12564v1","updated":"2024-10-16T13:38:31Z","published":"2024-10-16T13:38:31Z","title":"FTII-Bench: A Comprehensive Multimodal Benchmark for Flow Text with\n  Image Insertion","summary":"  Benefiting from the revolutionary advances in large language models (LLMs)\nand foundational vision models, large vision-language models (LVLMs) have also\nmade significant progress. However, current benchmarks focus on tasks that\nevaluating only a single aspect of LVLM capabilities (e.g., recognition,\ndetection, understanding). These tasks fail to fully demonstrate LVLMs'\npotential in complex application scenarios. To comprehensively assess the\nperformance of existing LVLMs, we propose a more challenging task called the\nFlow Text with Image Insertion task (FTII). This task requires LVLMs to\nsimultaneously possess outstanding abilities in image comprehension,\ninstruction understanding, and long-text interpretation. Specifically, given\nseveral text paragraphs and a set of candidate images, as the text paragraphs\naccumulate, the LVLMs are required to select the most suitable image from the\ncandidates to insert after the corresponding paragraph. Constructing a\nbenchmark for such a task is highly challenging, particularly in determining\nthe sequence of flowing text and images. To address this challenge, we turn to\nprofessional news reports, which naturally contain a gold standard for\nimage-text sequences. Based on this, we introduce the Flow Text with Image\nInsertion Benchmark (FTII-Bench), which includes 318 high-quality Chinese\nimage-text news articles and 307 high-quality English image-text news articles,\ncovering 10 different news domains. Using these 625 high-quality articles, we\nconstruct problems of two different types with multiple levels of difficulty.\nFurthermore, we establish two different evaluation pipelines based on the CLIP\nmodel and existing LVLMs. We evaluate 9 open-source and 2 closed-source LVLMs\nas well as 2 CLIP-based models. Results indicate that even the most advanced\nmodels (e.g., GPT-4o) face significant challenges when tackling the FTII task.\n","authors":["Jiacheng Ruan","Yebin Yang","Zehao Lin","Feiyu Xiong","Zeyun Tang","Zhiyu Li"],"pdf_url":"https://arxiv.org/pdf/2410.12564v1.pdf","comment":"Work in progress. 9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.12562v1","updated":"2024-10-16T13:38:01Z","published":"2024-10-16T13:38:01Z","title":"Adaptive Prompt Learning with SAM for Few-shot Scanning Probe Microscope\n  Image Segmentation","summary":"  The Segment Anything Model (SAM) has demonstrated strong performance in image\nsegmentation of natural scene images. However, its effectiveness diminishes\nmarkedly when applied to specific scientific domains, such as Scanning Probe\nMicroscope (SPM) images. This decline in accuracy can be attributed to the\ndistinct data distribution and limited availability of the data inherent in the\nscientific images. On the other hand, the acquisition of adequate SPM datasets\nis both time-intensive and laborious as well as skill-dependent. To address\nthese challenges, we propose an Adaptive Prompt Learning with SAM (APL-SAM)\nframework tailored for few-shot SPM image segmentation. Our approach\nincorporates two key innovations to enhance SAM: 1) An Adaptive Prompt Learning\nmodule leverages few-shot embeddings derived from limited support set to learn\nadaptively central representatives, serving as visual prompts. This innovation\neliminates the need for time-consuming online user interactions for providing\nprompts, such as exhaustively marking points and bounding boxes slice by slice;\n2) A multi-source, multi-level mask decoder specifically designed for few-shot\nSPM image segmentation is introduced, which can effectively capture the\ncorrespondence between the support and query images. To facilitate\ncomprehensive training and evaluation, we introduce a new dataset, SPM-Seg,\ncurated for SPM image segmentation. Extensive experiments on this dataset\nreveal that the proposed APL-SAM framework significantly outperforms the\noriginal SAM, achieving over a 30% improvement in terms of Dice Similarity\nCoefficient with only one-shot guidance. Moreover, APL-SAM surpasses\nstate-of-the-art few-shot segmentation methods and even fully supervised\napproaches in performance. Code and dataset used in this study will be made\navailable upon acceptance.\n","authors":["Yao Shen","Ziwei Wei","Chunmeng Liu","Shuming Wei","Qi Zhao","Kaiyang Zeng","Guangyao Li"],"pdf_url":"https://arxiv.org/pdf/2410.12562v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.12561v1","updated":"2024-10-16T13:36:47Z","published":"2024-10-16T13:36:47Z","title":"Development of Image Collection Method Using YOLO and Siamese Network","summary":"  As we enter the era of big data, collecting high-quality data is very\nimportant. However, collecting data by humans is not only very time-consuming\nbut also expensive. Therefore, many scientists have devised various methods to\ncollect data using computers. Among them, there is a method called web\ncrawling, but the authors found that the crawling method has a problem in that\nunintended data is collected along with the user. The authors found that this\ncan be filtered using the object recognition model YOLOv10. However, there are\ncases where data that is not properly filtered remains. Here, image\nreclassification was performed by additionally utilizing the distance output\nfrom the Siamese network, and higher performance was recorded than other\nclassification models. (average \\_f1 score YOLO+MobileNet\n0.678->YOLO+SiameseNet 0.772)) The user can specify a distance threshold to\nadjust the balance between data deficiency and noise-robustness. The authors\nalso found that the Siamese network can achieve higher performance with fewer\nresources because the cropped images are used for object recognition when\nprocessing images in the Siamese network. (Class 20 mean-based f1 score,\nnon-crop+Siamese(MobileNetV3-Small) 80.94 -> crop\npreprocessing+Siamese(MobileNetV3-Small) 82.31) In this way, the image\nretrieval system that utilizes two consecutive models to reduce errors can save\nusers' time and effort, and build better quality data faster and with fewer\nresources than before.\n","authors":["Chan Young Shin","Ah Hyun Lee","Jun Young Lee","Ji Min Lee","Soo Jin Park"],"pdf_url":"https://arxiv.org/pdf/2410.12561v1.pdf","comment":"15 pages, 13 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.12557v1","updated":"2024-10-16T13:34:40Z","published":"2024-10-16T13:34:40Z","title":"One Step Diffusion via Shortcut Models","summary":"  Diffusion models and flow-matching models have enabled generating diverse and\nrealistic images by learning to transfer noise to data. However, sampling from\nthese models involves iterative denoising over many neural network passes,\nmaking generation slow and expensive. Previous approaches for speeding up\nsampling require complex training regimes, such as multiple training phases,\nmultiple networks, or fragile scheduling. We introduce shortcut models, a\nfamily of generative models that use a single network and training phase to\nproduce high-quality samples in a single or multiple sampling steps. Shortcut\nmodels condition the network not only on the current noise level but also on\nthe desired step size, allowing the model to skip ahead in the generation\nprocess. Across a wide range of sampling step budgets, shortcut models\nconsistently produce higher quality samples than previous approaches, such as\nconsistency models and reflow. Compared to distillation, shortcut models reduce\ncomplexity to a single network and training phase and additionally allow\nvarying step budgets at inference time.\n","authors":["Kevin Frans","Danijar Hafner","Sergey Levine","Pieter Abbeel"],"pdf_url":"https://arxiv.org/pdf/2410.12557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12542v1","updated":"2024-10-16T13:20:57Z","published":"2024-10-16T13:20:57Z","title":"Evaluating Utility of Memory Efficient Medical Image Generation: A Study\n  on Lung Nodule Segmentation","summary":"  The scarcity of publicly available medical imaging data limits the\ndevelopment of effective AI models. This work proposes a memory-efficient\npatch-wise denoising diffusion probabilistic model (DDPM) for generating\nsynthetic medical images, focusing on CT scans with lung nodules. Our approach\ngenerates high-utility synthetic images with nodule segmentation while\nefficiently managing memory constraints, enabling the creation of training\ndatasets. We evaluate the method in two scenarios: training a segmentation\nmodel exclusively on synthetic data, and augmenting real-world training data\nwith synthetic images. In the first case, models trained solely on synthetic\ndata achieve Dice scores comparable to those trained on real-world data\nbenchmarks. In the second case, augmenting real-world data with synthetic\nimages significantly improves segmentation performance. The generated images\ndemonstrate their potential to enhance medical image datasets in scenarios with\nlimited real-world data.\n","authors":["Kathrin Khadra","Utku Türkbey"],"pdf_url":"https://arxiv.org/pdf/2410.12542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11795v2","updated":"2024-10-16T13:10:32Z","published":"2024-10-15T17:19:46Z","title":"Efficient Diffusion Models: A Comprehensive Survey from Principles to\n  Practices","summary":"  As one of the most popular and sought-after generative models in the recent\nyears, diffusion models have sparked the interests of many researchers and\nsteadily shown excellent advantage in various generative tasks such as image\nsynthesis, video generation, molecule design, 3D scene rendering and multimodal\ngeneration, relying on their dense theoretical principles and reliable\napplication practices. The remarkable success of these recent efforts on\ndiffusion models comes largely from progressive design principles and efficient\narchitecture, training, inference, and deployment methodologies. However, there\nhas not been a comprehensive and in-depth review to summarize these principles\nand practices to help the rapid understanding and application of diffusion\nmodels. In this survey, we provide a new efficiency-oriented perspective on\nthese existing efforts, which mainly focuses on the profound principles and\nefficient practices in architecture designs, model training, fast inference and\nreliable deployment, to guide further theoretical research, algorithm migration\nand model application for new scenarios in a reader-friendly way.\n\\url{https://github.com/ponyzym/Efficient-DMs-Survey}\n","authors":["Zhiyuan Ma","Yuzhu Zhang","Guoli Jia","Liangliang Zhao","Yichao Ma","Mingjie Ma","Gaofeng Liu","Kaiyan Zhang","Jianjun Li","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.11795v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12526v1","updated":"2024-10-16T13:03:15Z","published":"2024-10-16T13:03:15Z","title":"Shaping a Stabilized Video by Mitigating Unintended Changes for\n  Concept-Augmented Video Editing","summary":"  Text-driven video editing utilizing generative diffusion models has garnered\nsignificant attention due to their potential applications. However, existing\napproaches are constrained by the limited word embeddings provided in\npre-training, which hinders nuanced editing targeting open concepts with\nspecific attributes. Directly altering the keywords in target prompts often\nresults in unintended disruptions to the attention mechanisms. To achieve more\nflexible editing easily, this work proposes an improved concept-augmented video\nediting approach that generates diverse and stable target videos flexibly by\ndevising abstract conceptual pairs. Specifically, the framework involves\nconcept-augmented textual inversion and a dual prior supervision mechanism. The\nformer enables plug-and-play guidance of stable diffusion for video editing,\neffectively capturing target attributes for more stylized results. The dual\nprior supervision mechanism significantly enhances video stability and\nfidelity. Comprehensive evaluations demonstrate that our approach generates\nmore stable and lifelike videos, outperforming state-of-the-art methods.\n","authors":["Mingce Guo","Jingxuan He","Shengeng Tang","Zhangye Wang","Lechao Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.12526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12524v1","updated":"2024-10-16T13:02:45Z","published":"2024-10-16T13:02:45Z","title":"MambaPainter: Neural Stroke-Based Rendering in a Single Step","summary":"  Stroke-based rendering aims to reconstruct an input image into an oil\npainting style by predicting brush stroke sequences. Conventional methods\nperform this prediction stroke-by-stroke or require multiple inference steps\ndue to the limitations of a predictable number of strokes. This procedure leads\nto inefficient translation speed, limiting their practicality. In this study,\nwe propose MambaPainter, capable of predicting a sequence of over 100 brush\nstrokes in a single inference step, resulting in rapid translation. We achieve\nthis sequence prediction by incorporating the selective state-space model.\nAdditionally, we introduce a simple extension to patch-based rendering, which\nwe use to translate high-resolution images, improving the visual quality with a\nminimal increase in computational cost. Experimental results demonstrate that\nMambaPainter can efficiently translate inputs to oil painting-style images\ncompared to state-of-the-art methods. The codes are available at\nhttps://github.com/STomoya/MambaPainter.\n","authors":["Tomoya Sawada","Marie Katsurai"],"pdf_url":"https://arxiv.org/pdf/2410.12524v1.pdf","comment":"Accepted to SIGGRAPH Asia 2024 posters"},{"id":"http://arxiv.org/abs/2410.12520v1","updated":"2024-10-16T12:58:08Z","published":"2024-10-16T12:58:08Z","title":"QueensCAMP: an RGB-D dataset for robust Visual SLAM","summary":"  Visual Simultaneous Localization and Mapping (VSLAM) is a fundamental\ntechnology for robotics applications. While VSLAM research has achieved\nsignificant advancements, its robustness under challenging situations, such as\npoor lighting, dynamic environments, motion blur, and sensor failures, remains\na challenging issue. To address these challenges, we introduce a novel RGB-D\ndataset designed for evaluating the robustness of VSLAM systems. The dataset\ncomprises real-world indoor scenes with dynamic objects, motion blur, and\nvarying illumination, as well as emulated camera failures, including lens dirt,\ncondensation, underexposure, and overexposure. Additionally, we offer\nopen-source scripts for injecting camera failures into any images, enabling\nfurther customization by the research community. Our experiments demonstrate\nthat ORB-SLAM2, a traditional VSLAM algorithm, and TartanVO, a Deep\nLearning-based VO algorithm, can experience performance degradation under these\nchallenging conditions. Therefore, this dataset and the camera failure\nopen-source tools provide a valuable resource for developing more robust VSLAM\nsystems capable of handling real-world challenges.\n","authors":["Hudson M. S. Bruno","Esther L. Colombini","Sidney N. Givigi Jr"],"pdf_url":"https://arxiv.org/pdf/2410.12520v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2403.17834v2","updated":"2024-10-16T12:49:19Z","published":"2024-03-26T16:19:56Z","title":"Developing Generalist Foundation Models from a Multimodal Dataset for 3D\n  Computed Tomography","summary":"  While computer vision has achieved tremendous success with multimodal\nencoding and direct textual interaction with images via chat-based large\nlanguage models, similar advancements in medical imaging AI, particularly in 3D\nimaging, have been limited due to the scarcity of comprehensive datasets. To\naddress this critical gap, we introduce CT-RATE, the first dataset that pairs\n3D medical images with corresponding textual reports. CT-RATE comprises 25,692\nnon-contrast 3D chest CT scans from 21,304 unique patients. Through various\nreconstructions, these scans are expanded to 50,188 volumes, totaling over 14.3\nmillion 2D slices. Each scan is accompanied by its corresponding radiology\nreport. Leveraging CT-RATE, we develop CT-CLIP, a CT-focused contrastive\nlanguage-image pretraining framework designed for broad applications without\nthe need for task-specific training. We demonstrate how CT-CLIP can be used in\ntwo tasks: multi-abnormality detection and case retrieval. Remarkably, in\nmulti-abnormality detection, CT-CLIP outperforms state-of-the-art fully\nsupervised models across all key metrics, effectively eliminating the need for\nmanual annotation. In case retrieval, it efficiently retrieves relevant cases\nusing either image or textual queries, thereby enhancing knowledge\ndissemination. By combining CT-CLIP's vision encoder with a pretrained large\nlanguage model, we create CT-CHAT, a vision-language foundational chat model\nfor 3D chest CT volumes. Finetuned on over 2.7 million question-answer pairs\nderived from the CT-RATE dataset, CT-CHAT surpasses other multimodal AI\nassistants, underscoring the necessity for specialized methods in 3D medical\nimaging. Collectively, the open-source release of CT-RATE, CT-CLIP, and CT-CHAT\nnot only addresses critical challenges in 3D medical imaging but also lays the\ngroundwork for future innovations in medical AI and improved patient care.\n","authors":["Ibrahim Ethem Hamamci","Sezgin Er","Furkan Almas","Ayse Gulnihan Simsek","Sevval Nil Esirgun","Irem Dogan","Muhammed Furkan Dasdelen","Omer Faruk Durugol","Bastian Wittmann","Tamaz Amiranashvili","Enis Simsar","Mehmet Simsar","Emine Bensu Erdemir","Abdullah Alanbay","Anjany Sekuboyina","Berkan Lafci","Christian Bluethgen","Mehmet Kemal Ozdemir","Bjoern Menze"],"pdf_url":"https://arxiv.org/pdf/2403.17834v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10114v2","updated":"2024-10-16T12:30:53Z","published":"2024-10-14T03:05:12Z","title":"Mixture of Experts Made Personalized: Federated Prompt Learning for\n  Vision-Language Models","summary":"  Prompt learning for pre-trained Vision-Language Models (VLMs) like CLIP has\ndemonstrated potent applicability across diverse downstream tasks. This\nlightweight approach has quickly gained traction from federated learning (FL)\nresearchers who seek to efficiently adapt VLMs to heterogeneous scenarios.\nHowever, current federated prompt learning methods are habitually restricted to\nthe traditional FL paradigm, where the participating clients are generally only\nallowed to download a single globally aggregated model from the server. While\njustifiable for training full-sized models under federated settings, in this\nwork, we argue that this paradigm is ill-suited for lightweight prompts. By\nfacilitating the clients to download multiple pre-aggregated prompts as fixed\nnon-local experts, we propose Personalized Federated Mixture of Adaptive\nPrompts (pFedMoAP), a novel FL framework that personalizes the prompt learning\nprocess through the lens of Mixture of Experts (MoE). pFedMoAP implements a\nlocal attention-based gating network that learns to generate enhanced text\nfeatures for better alignment with local image data on the client, benefiting\nfrom both local and downloaded non-local adaptive prompt experts. The non-local\nexperts are sparsely selected from a server-maintained pool, fostering\ncollaborative learning across clients. To evaluate the proposed algorithm, we\nconduct extensive experiments across 9 datasets under various heterogeneous\nfederated settings. The results show that pFedMoAP consistently outperforms the\nstate-of-the-art alternatives, underscoring its efficacy in personalizing\nprompt learning for CLIP within the federated learning paradigm.\n","authors":["Jun Luo","Chen Chen","Shandong Wu"],"pdf_url":"https://arxiv.org/pdf/2410.10114v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.12501v1","updated":"2024-10-16T12:27:10Z","published":"2024-10-16T12:27:10Z","title":"DH-VTON: Deep Text-Driven Virtual Try-On via Hybrid Attention Learning","summary":"  Virtual Try-ON (VTON) aims to synthesis specific person images dressed in\ngiven garments, which recently receives numerous attention in online shopping\nscenarios. Currently, the core challenges of the VTON task mainly lie in the\nfine-grained semantic extraction (i.e.,deep semantics) of the given reference\ngarments during depth estimation and effective texture preservation when the\ngarments are synthesized and warped onto human body. To cope with these issues,\nwe propose DH-VTON, a deep text-driven virtual try-on model featuring a special\nhybrid attention learning strategy and deep garment semantic preservation\nmodule. By standing on the shoulder of a well-built pre-trained\npaint-by-example (abbr. PBE) approach, we present our DH-VTON pipeline in this\nwork. Specifically, to extract the deep semantics of the garments, we first\nintroduce InternViT-6B as fine-grained feature learner, which can be trained to\nalign with the large-scale intrinsic knowledge with deep text semantics\n(e.g.,\"neckline\" or \"girdle\") to make up for the deficiency of the commonly\nadopted CLIP encoder. Based on this, to enhance the customized dressing\nabilities, we further introduce Garment-Feature ControlNet Plus (abbr. GFC+)\nmodule and propose to leverage a fresh hybrid attention strategy for training,\nwhich can adaptively integrate fine-grained characteristics of the garments\ninto the different layers of the VTON model, so as to achieve multi-scale\nfeatures preservation effects. Extensive experiments on several representative\ndatasets demonstrate that our method outperforms previous diffusion-based and\nGAN-based approaches, showing competitive performance in preserving garment\ndetails and generating authentic human images.\n","authors":["Jiabao Wei","Zhiyuan Ma"],"pdf_url":"https://arxiv.org/pdf/2410.12501v1.pdf","comment":"5 pages, 6 figures, ICASSP2025"},{"id":"http://arxiv.org/abs/2410.12490v1","updated":"2024-10-16T12:13:17Z","published":"2024-10-16T12:13:17Z","title":"Stabilize the Latent Space for Image Autoregressive Modeling: A Unified\n  Perspective","summary":"  Latent-based image generative models, such as Latent Diffusion Models (LDMs)\nand Mask Image Models (MIMs), have achieved notable success in image generation\ntasks. These models typically leverage reconstructive autoencoders like VQGAN\nor VAE to encode pixels into a more compact latent space and learn the data\ndistribution in the latent space instead of directly from pixels. However, this\npractice raises a pertinent question: Is it truly the optimal choice? In\nresponse, we begin with an intriguing observation: despite sharing the same\nlatent space, autoregressive models significantly lag behind LDMs and MIMs in\nimage generation. This finding contrasts sharply with the field of NLP, where\nthe autoregressive model GPT has established a commanding presence. To address\nthis discrepancy, we introduce a unified perspective on the relationship\nbetween latent space and generative models, emphasizing the stability of latent\nspace in image generative modeling. Furthermore, we propose a simple but\neffective discrete image tokenizer to stabilize the latent space for image\ngenerative modeling. Experimental results show that image autoregressive\nmodeling with our tokenizer (DiGIT) benefits both image understanding and image\ngeneration with the next token prediction principle, which is inherently\nstraightforward for GPT models but challenging for other generative models.\nRemarkably, for the first time, a GPT-style autoregressive model for images\noutperforms LDMs, which also exhibits substantial improvement akin to GPT when\nscaling up model size. Our findings underscore the potential of an optimized\nlatent space and the integration of discrete tokenization in advancing the\ncapabilities of image generative models. The code is available at\n\\url{https://github.com/DAMO-NLP-SG/DiGIT}.\n","authors":["Yongxin Zhu","Bocheng Li","Hang Zhang","Xin Li","Linli Xu","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2410.12490v1.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.12489v1","updated":"2024-10-16T12:09:38Z","published":"2024-10-16T12:09:38Z","title":"Synthetic Augmentation for Anatomical Landmark Localization using DDPMs","summary":"  Deep learning techniques for anatomical landmark localization (ALL) have\nshown great success, but their reliance on large annotated datasets remains a\nproblem due to the tedious and costly nature of medical data acquisition and\nannotation. While traditional data augmentation, variational autoencoders\n(VAEs), and generative adversarial networks (GANs) have already been used to\nsynthetically expand medical datasets, diffusion-based generative models have\nrecently started to gain attention for their ability to generate high-quality\nsynthetic images. In this study, we explore the use of denoising diffusion\nprobabilistic models (DDPMs) for generating medical images and their\ncorresponding heatmaps of landmarks to enhance the training of a supervised\ndeep learning model for ALL. Our novel approach involves a DDPM with a\n2-channel input, incorporating both the original medical image and its heatmap\nof annotated landmarks. We also propose a novel way to assess the quality of\nthe generated images using a Markov Random Field (MRF) model for landmark\nmatching and a Statistical Shape Model (SSM) to check landmark plausibility,\nbefore we evaluate the DDPM-augmented dataset in the context of an ALL task\ninvolving hand X-Rays.\n","authors":["Arnela Hadzic","Lea Bogensperger","Simon Johannes Joham","Martin Urschler"],"pdf_url":"https://arxiv.org/pdf/2410.12489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04492v3","updated":"2024-10-16T12:07:27Z","published":"2024-10-06T14:11:39Z","title":"Interpret Your Decision: Logical Reasoning Regularization for\n  Generalization in Visual Classification","summary":"  Vision models excel in image classification but struggle to generalize to\nunseen data, such as classifying images from unseen domains or discovering\nnovel categories. In this paper, we explore the relationship between logical\nreasoning and deep learning generalization in visual classification. A logical\nregularization termed L-Reg is derived which bridges a logical analysis\nframework to image classification. Our work reveals that L-Reg reduces the\ncomplexity of the model in terms of the feature distribution and classifier\nweights. Specifically, we unveil the interpretability brought by L-Reg, as it\nenables the model to extract the salient features, such as faces to persons,\nfor classification. Theoretical analysis and experiments demonstrate that L-Reg\nenhances generalization across various scenarios, including multi-domain\ngeneralization and generalized category discovery. In complex real-world\nscenarios where images span unknown classes and unseen domains, L-Reg\nconsistently improves generalization, highlighting its practical efficacy.\n","authors":["Zhaorui Tan","Xi Yang","Qiufeng Wang","Anh Nguyen","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2410.04492v3.pdf","comment":"Accepted by NeurIPS2024 as Spotlight"},{"id":"http://arxiv.org/abs/2406.15020v3","updated":"2024-10-16T11:58:16Z","published":"2024-06-21T09:49:34Z","title":"A3D: Does Diffusion Dream about 3D Alignment?","summary":"  We tackle the problem of text-driven 3D generation from a geometry alignment\nperspective. Given a set of text prompts, we aim to generate a collection of\nobjects with semantically corresponding parts aligned across them. Recent\nmethods based on Score Distillation have succeeded in distilling the knowledge\nfrom 2D diffusion models to high-quality representations of the 3D objects.\nThese methods handle multiple text queries separately, and therefore the\nresulting objects have a high variability in object pose and structure.\nHowever, in some applications, such as 3D asset design, it may be desirable to\nobtain a set of objects aligned with each other. In order to achieve the\nalignment of the corresponding parts of the generated objects, we propose to\nembed these objects into a common latent space and optimize the continuous\ntransitions between these objects. We enforce two kinds of properties of these\ntransitions: smoothness of the transition and plausibility of the intermediate\nobjects along the transition. We demonstrate that both of these properties are\nessential for good alignment. We provide several practical scenarios that\nbenefit from alignment between the objects, including 3D editing and object\nhybridization, and experimentally demonstrate the effectiveness of our method.\nhttps://voyleg.github.io/a3d/\n","authors":["Savva Ignatyev","Nina Konovalova","Daniil Selikhanovych","Oleg Voynov","Nikolay Patakin","Ilya Olkov","Dmitry Senushkin","Alexey Artemov","Anton Konushin","Alexander Filippov","Peter Wonka","Evgeny Burnaev"],"pdf_url":"https://arxiv.org/pdf/2406.15020v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03394v2","updated":"2024-10-16T11:48:21Z","published":"2024-06-05T15:44:54Z","title":"Gaussian Primitives for Deformable Image Registration","summary":"  Deformable Image Registration (DIR) is essential for aligning medical images\nthat exhibit anatomical variations, facilitating applications such as disease\ntracking and radiotherapy planning. While classical iterative methods and deep\nlearning approaches have achieved success in DIR, they are often hindered by\ncomputational inefficiency or poor generalization. In this paper, we introduce\nGaussianDIR, a novel, case-specific optimization DIR method inspired by 3D\nGaussian splatting. In general, GaussianDIR represents image deformations using\na sparse set of mobile and flexible Gaussian primitives, each defined by a\ncenter position, covariance, and local rigid transformation. This compact and\nexplicit representation reduces noise and computational overhead while\nimproving interpretability. Furthermore, the movement of individual voxel is\nderived via blending the local rigid transformation of the neighboring Gaussian\nprimitives. By this, GaussianDIR captures both global smoothness and local\nrigidity as well as reduces the computational burden. To address varying levels\nof deformation complexity, GaussianDIR also integrates an adaptive density\ncontrol mechanism that dynamically adjusts the density of Gaussian primitives.\nAdditionally, we employ multi-scale Gaussian primitives to capture both coarse\nand fine deformations, reducing optimization to local minima. Experimental\nresults on brain MRI, lung CT, and cardiac MRI datasets demonstrate that\nGaussianDIR outperforms existing DIR methods in both accuracy and efficiency,\nhighlighting its potential for clinical applications. Finally, as a\ntraining-free approach, it challenges the stereotype that iterative methods are\ninherently slow and transcend the limitations of poor generalization.\n","authors":["Jihe Li","Xiang Liu","Fabian Zhang","Xia Li","Xixin Cao","Ye Zhang","Joachim Buhmann"],"pdf_url":"https://arxiv.org/pdf/2406.03394v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12474v1","updated":"2024-10-16T11:42:11Z","published":"2024-10-16T11:42:11Z","title":"Mind the Gap Between Prototypes and Images in Cross-domain Finetuning","summary":"  In cross-domain few-shot classification (CFC), recent works mainly focus on\nadapting a simple transformation head on top of a frozen pre-trained backbone\nwith few labeled data to project embeddings into a task-specific metric space\nwhere classification can be performed by measuring similarities between image\ninstance and prototype representations. Technically, an assumption implicitly\nadopted in such a framework is that the prototype and image instance embeddings\nshare the same representation transformation. However, in this paper, we find\nthat there naturally exists a gap, which resembles the modality gap, between\nthe prototype and image instance embeddings extracted from the frozen\npre-trained backbone, and simply applying the same transformation during the\nadaptation phase constrains exploring the optimal representations and shrinks\nthe gap between prototype and image representations. To solve this problem, we\npropose a simple yet effective method, contrastive prototype-image adaptation\n(CoPA), to adapt different transformations respectively for prototypes and\nimages similarly to CLIP by treating prototypes as text prompts. Extensive\nexperiments on Meta-Dataset demonstrate that CoPA achieves the state-of-the-art\nperformance more efficiently. Meanwhile, further analyses also indicate that\nCoPA can learn better representation clusters, enlarge the gap, and achieve\nminimal validation loss at the enlarged gap.\n","authors":["Hongduan Tian","Feng Liu","Zhanke Zhou","Tongliang Liu","Chengqi Zhang","Bo Han"],"pdf_url":"https://arxiv.org/pdf/2410.12474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06241v2","updated":"2024-10-16T11:39:54Z","published":"2024-10-08T17:56:33Z","title":"BroadWay: Boost Your Text-to-Video Generation Model in a Training-free\n  Way","summary":"  The text-to-video (T2V) generation models, offering convenient visual\ncreation, have recently garnered increasing attention. Despite their\nsubstantial potential, the generated videos may present artifacts, including\nstructural implausibility, temporal inconsistency, and a lack of motion, often\nresulting in near-static video. In this work, we have identified a correlation\nbetween the disparity of temporal attention maps across different blocks and\nthe occurrence of temporal inconsistencies. Additionally, we have observed that\nthe energy contained within the temporal attention maps is directly related to\nthe magnitude of motion amplitude in the generated videos. Based on these\nobservations, we present BroadWay, a training-free method to improve the\nquality of text-to-video generation without introducing additional parameters,\naugmenting memory or sampling time. Specifically, BroadWay is composed of two\nprincipal components: 1) Temporal Self-Guidance improves the structural\nplausibility and temporal consistency of generated videos by reducing the\ndisparity between the temporal attention maps across various decoder blocks. 2)\nFourier-based Motion Enhancement enhances the magnitude and richness of motion\nby amplifying the energy of the map. Extensive experiments demonstrate that\nBroadWay significantly improves the quality of text-to-video generation with\nnegligible additional cost.\n","authors":["Jiazi Bu","Pengyang Ling","Pan Zhang","Tong Wu","Xiaoyi Dong","Yuhang Zang","Yuhang Cao","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06241v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09408v2","updated":"2024-10-16T11:28:19Z","published":"2024-08-18T08:38:20Z","title":"VrdONE: One-stage Video Visual Relation Detection","summary":"  Video Visual Relation Detection (VidVRD) focuses on understanding how\nentities interact over time and space in videos, a key step for gaining deeper\ninsights into video scenes beyond basic visual tasks. Traditional methods for\nVidVRD, challenged by its complexity, typically split the task into two parts:\none for identifying what relation categories are present and another for\ndetermining their temporal boundaries. This split overlooks the inherent\nconnection between these elements. Addressing the need to recognize entity\npairs' spatiotemporal interactions across a range of durations, we propose\nVrdONE, a streamlined yet efficacious one-stage model. VrdONE combines the\nfeatures of subjects and objects, turning predicate detection into 1D instance\nsegmentation on their combined representations. This setup allows for both\nrelation category identification and binary mask generation in one go,\neliminating the need for extra steps like proposal generation or\npost-processing. VrdONE facilitates the interaction of features across various\nframes, adeptly capturing both short-lived and enduring relations.\nAdditionally, we introduce the Subject-Object Synergy (SOS) module, enhancing\nhow subjects and objects perceive each other before combining. VrdONE achieves\nstate-of-the-art performances on the VidOR benchmark and ImageNet-VidVRD,\nshowcasing its superior capability in discerning relations across different\ntemporal scales. The code is available at https://github.com/lucaspk512/vrdone.\n","authors":["Xinjie Jiang","Chenxi Zheng","Xuemiao Xu","Bangzhen Liu","Weiying Zheng","Huaidong Zhang","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2408.09408v2.pdf","comment":"12 pages, 8 figures, accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2410.11190v2","updated":"2024-10-16T11:19:56Z","published":"2024-10-15T02:10:45Z","title":"Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex\n  Capabilities","summary":"  GPT-4o, an all-encompassing model, represents a milestone in the development\nof large multi-modal language models. It can understand visual, auditory, and\ntextual modalities, directly output audio, and support flexible duplex\ninteraction. Models from the open-source community often achieve some\nfunctionalities of GPT-4o, such as visual understanding and voice chat.\nNevertheless, training a unified model that incorporates all modalities is\nchallenging due to the complexities of multi-modal data, intricate model\narchitectures, and training processes. In this paper, we introduce Mini-Omni2,\na visual-audio assistant capable of providing real-time, end-to-end voice\nresponses to visoin and audio queries. By integrating pretrained visual and\nauditory encoders, Mini-Omni2 maintains performance in individual modalities.\nWe propose a three-stage training process to align modalities, allowing the\nlanguage model to handle multi-modal inputs and outputs after training on a\nlimited dataset. For interaction, we introduce a command-based interruption\nmechanism, enabling more flexible interaction with users. To the best of our\nknowledge, Mini-Omni2 is one of the closest reproductions of GPT-4o, which have\nsimilar form of functionality, and we hope it can offer valuable insights for\nsubsequent research.\n","authors":["Zhifei Xie","Changqiao Wu"],"pdf_url":"https://arxiv.org/pdf/2410.11190v2.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.00355v2","updated":"2024-10-16T10:45:59Z","published":"2024-08-01T07:52:07Z","title":"DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved\n  Denoising Training","summary":"  More and more end-to-end text spotting methods based on Transformer\narchitecture have demonstrated superior performance. These methods utilize a\nbipartite graph matching algorithm to perform one-to-one optimal matching\nbetween predicted objects and actual objects. However, the instability of\nbipartite graph matching can lead to inconsistent optimization targets, thereby\naffecting the training performance of the model. Existing literature applies\ndenoising training to solve the problem of bipartite graph matching instability\nin object detection tasks. Unfortunately, this denoising training method cannot\nbe directly applied to text spotting tasks, as these tasks need to perform\nirregular shape detection tasks and more complex text recognition tasks than\nclassification. To address this issue, we propose a novel denoising training\nmethod (DNTextSpotter) for arbitrary-shaped text spotting. Specifically, we\ndecompose the queries of the denoising part into noised positional queries and\nnoised content queries. We use the four Bezier control points of the Bezier\ncenter curve to generate the noised positional queries. For the noised content\nqueries, considering that the output of the text in a fixed positional order is\nnot conducive to aligning position with content, we employ a masked character\nsliding method to initialize noised content queries, thereby assisting in the\nalignment of text content and position. To improve the model's perception of\nthe background, we further utilize an additional loss function for background\ncharacters classification in the denoising training part.Although DNTextSpotter\nis conceptually simple, it outperforms the state-of-the-art methods on four\nbenchmarks (Total-Text, SCUT-CTW1500, ICDAR15, and Inverse-Text), especially\nyielding an improvement of 11.3% against the best approach in Inverse-Text\ndataset.\n","authors":["Yu Xie","Qian Qiao","Jun Gao","Tianxiang Wu","Jiaqing Fan","Yue Zhang","Jielei Zhang","Huyang Sun"],"pdf_url":"https://arxiv.org/pdf/2408.00355v2.pdf","comment":"Accepted by ACM'MM2024"},{"id":"http://arxiv.org/abs/2410.12441v1","updated":"2024-10-16T10:36:29Z","published":"2024-10-16T10:36:29Z","title":"A Primal-dual algorithm for image reconstruction with ICNNs","summary":"  We address the optimization problem in a data-driven variational\nreconstruction framework, where the regularizer is parameterized by an\ninput-convex neural network (ICNN). While gradient-based methods are commonly\nused to solve such problems, they struggle to effectively handle non-smoothness\nwhich often leads to slow convergence. Moreover, the nested structure of the\nneural network complicates the application of standard non-smooth optimization\ntechniques, such as proximal algorithms. To overcome these challenges, we\nreformulate the problem and eliminate the network's nested structure. By\nrelating this reformulation to epigraphical projections of the activation\nfunctions, we transform the problem into a convex optimization problem that can\nbe efficiently solved using a primal-dual algorithm. We also prove that this\nreformulation is equivalent to the original variational problem. Through\nexperiments on several imaging tasks, we demonstrate that the proposed approach\noutperforms subgradient methods in terms of both speed and stability.\n","authors":["Hok Shing Wong","Matthias J. Ehrhardt","Subhadip Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2410.12441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00275v2","updated":"2024-10-16T10:27:14Z","published":"2024-09-30T23:04:55Z","title":"On Large Uni- and Multi-modal Models for Unsupervised Classification of\n  Social Media Images: Nature's Contribution to People as a case study","summary":"  Social media images have proven to be a valuable source of information for\nunderstanding human interactions with important subjects such as cultural\nheritage, biodiversity, and nature, among others. The task of grouping such\nimages into a number of semantically meaningful clusters without labels is\nchallenging due to the high diversity and complex nature of the visual content\nin addition to their large volume. On the other hand, recent advances in Large\nVisual Models (LVMs), Large Language Models (LLMs), and Large Visual Language\nModels (LVLMs) provide an important opportunity to explore new productive and\nscalable solutions. This work proposes, analyzes, and compares various\napproaches based on one or more state-of-the-art LVM, LLM, and LVLM, for\nmapping social media images into a number of predefined classes. As a case\nstudy, we consider the problem of understanding the interactions between humans\nand nature, also known as Nature's Contribution to People or Cultural Ecosystem\nServices (CES). Our experiments show that the highest-performing approaches,\nwith accuracy above 95%, still require the creation of a small labeled dataset.\nThese include the fine-tuned LVM DINOv2 and the LVLM LLaVA-1.5 combined with a\nfine-tuned LLM. The top fully unsupervised approaches, achieving accuracy above\n84%, are the LVLMs, specifically the proprietary GPT-4 model and the public\nLLaVA-1.5 model. Additionally, the LVM DINOv2, when applied in a 10-shot\nlearning setup, delivered competitive results with an accuracy of 83.99%,\nclosely matching the performance of the LVLM LLaVA-1.5.\n","authors":["Rohaifa Khaldi","Domingo Alcaraz-Segura","Ignacio Sánchez-Herrera","Javier Martinez-Lopez","Carlos Javier Navarro","Siham Tabik"],"pdf_url":"https://arxiv.org/pdf/2410.00275v2.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2405.17257v2","updated":"2024-10-16T10:08:50Z","published":"2024-05-27T15:14:47Z","title":"Topological reconstruction of sampled surfaces via Morse theory","summary":"  In this work, we study the perception problem for sampled surfaces (possibly\nwith boundary) using tools from computational topology, specifically, how to\nidentify their underlying topology starting from point-cloud samples in space,\nsuch as those obtained with 3D scanners. We present a reconstruction algorithm\nbased on a careful topological study of the point sample that allows us to\nobtain a cellular decomposition of it using a Morse function. No triangulation\nor local implicit equations are used as intermediate steps, avoiding in this\nway reconstruction-induced artifices. The algorithm can be run without any\nprior knowledge of the surface topology, density or regularity of the\npoint-sample. The results consist of a piece-wise decomposition of the given\nsurface as a union of Morse cells (i.e. topological disks), suitable for tasks\nsuch as mesh-independent reparametrization or noise-filtering, and a small-rank\ncellular complex determining the topology of the surface. The algorithm, which\nwe test with several real and synthetic surfaces, can be applied to smooth\nsurfaces with or without boundary, embedded in an ambient space of any\ndimension.\n","authors":["Franco Coltraro","Jaume Amorós","Maria Alberich-Carramiñana","Carme Torras"],"pdf_url":"https://arxiv.org/pdf/2405.17257v2.pdf","comment":"39 pages, 17 figures, 1 table, 1 algorithm, 1 appendix"},{"id":"http://arxiv.org/abs/2410.12419v1","updated":"2024-10-16T10:04:22Z","published":"2024-10-16T10:04:22Z","title":"Attention-Guided Perturbation for Consistency Regularization in\n  Semi-Supervised Medical Image Segmentation","summary":"  Medical image segmentation is a pivotal step in diagnostic and therapeutic\nprocesses. However, the acquisition of high-quality annotated data is often\nconstrained by scarcity and cost. Semi-supervised learning offers a promising\napproach to enhance model performance by using unlabeled data. While\nconsistency regularization is a prevalent method in semi-supervised image\nsegmentation, there is a dearth of research on perturbation strategies tailored\nfor semi-supervised medical image segmentation tasks. This paper introduces an\nattention-guided perturbation strategy for semi-supervised consistency\nregularization in the context of medical image segmentation. We add the\nperturbation based on the attention from the model in the image and feature\nlevel to achieve consistency regularization. The method is adept at\naccommodating the intricate structures and high-dimensional semantics inherent\nin medical images, thereby enhancing the performance of semi-supervised\nsegmentation tasks. Our method achieved state-of-the-art results on benchmark\ndatasets, including a 90.4\\% Dice score on the ACDC dataset in the 7-case\nscenario.\n","authors":["Yuxuan Cheng","Chenxi Shao","Jie Ma","Guoliang Li"],"pdf_url":"https://arxiv.org/pdf/2410.12419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11288v2","updated":"2024-10-16T09:59:17Z","published":"2024-06-17T07:51:44Z","title":"MFC-Bench: Benchmarking Multimodal Fact-Checking with Large\n  Vision-Language Models","summary":"  Large vision-language models (LVLMs) have significantly improved multimodal\nreasoning tasks, such as visual question answering and image captioning. These\nmodels embed multimodal facts within their parameters, rather than relying on\nexternal knowledge bases to store factual information explicitly. However, the\ncontent discerned by LVLMs may deviate from actual facts due to inherent bias\nor incorrect inference. To address this issue, we introduce MFC-Bench, a\nrigorous and comprehensive benchmark designed to evaluate the factual accuracy\nof LVLMs across three stages of verdict prediction for MFC: Manipulation,\nOut-of-Context, and Veracity Classification. Through our evaluation on\nMFC-Bench, we benchmarked a dozen diverse and representative LVLMs, uncovering\nthat current models still fall short in multimodal fact-checking and\ndemonstrate insensitivity to various forms of manipulated content. We hope that\nMFC-Bench could raise attention to the trustworthy AI potentially assisted by\nLVLMs in the future. The MFC-Bench and accompanying resources are publicly\naccessible at https://github.com/wskbest/MFC-Bench, contributing to ongoing\nresearch in the multimodal fact-checking field.\n","authors":["Shengkang Wang","Hongzhan Lin","Ziyang Luo","Zhen Ye","Guang Chen","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2406.11288v2.pdf","comment":"28 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.12414v1","updated":"2024-10-16T09:59:11Z","published":"2024-10-16T09:59:11Z","title":"Triplet: Triangle Patchlet for Mesh-Based Inverse Rendering and Scene\n  Parameters Approximation","summary":"  Recent advancements in Radiance Fields have significantly improved novel-view\nsynthesis. However, in many real-world applications, the more advanced\nchallenge lies in inverse rendering, which seeks to derive the physical\nproperties of a scene, including light, geometry, textures, and materials.\nMeshes, as a traditional representation adopted by many simulation pipeline,\nhowever, still show limited influence in radiance field for inverse rendering.\nThis paper introduces a novel framework called Triangle Patchlet (abbr.\nTriplet), a mesh-based representation, to comprehensively approximate these\nscene parameters. We begin by assembling Triplets with either randomly\ngenerated points or sparse points obtained from camera calibration where all\nfaces are treated as an independent element. Next, we simulate the physical\ninteraction of light and optimize the scene parameters using traditional\ngraphics rendering techniques like rasterization and ray tracing, accompanying\nwith density control and propagation. An iterative mesh extracting process is\nalso suggested, where we continue to optimize on geometry and materials with\ngraph-based operation. We also introduce several regulation terms to enable\nbetter generalization of materials property. Our framework could precisely\nestimate the light, materials and geometry with mesh without prior of light,\nmaterials and geometry in a unified framework. Experiments demonstrate that our\napproach can achieve state-of-the-art visual quality while reconstructing\nhigh-quality geometry and accurate material properties.\n","authors":["Jiajie Yang"],"pdf_url":"https://arxiv.org/pdf/2410.12414v1.pdf","comment":"https://github.com/RANDO11199/Triplet"},{"id":"http://arxiv.org/abs/2410.12411v1","updated":"2024-10-16T09:52:38Z","published":"2024-10-16T09:52:38Z","title":"AdaCropFollow: Self-Supervised Online Adaptation for Visual Under-Canopy\n  Navigation","summary":"  Under-canopy agricultural robots can enable various applications like precise\nmonitoring, spraying, weeding, and plant manipulation tasks throughout the\ngrowing season. Autonomous navigation under the canopy is challenging due to\nthe degradation in accuracy of RTK-GPS and the large variability in the visual\nappearance of the scene over time. In prior work, we developed a supervised\nlearning-based perception system with semantic keypoint representation and\ndeployed this in various field conditions. A large number of failures of this\nsystem can be attributed to the inability of the perception model to adapt to\nthe domain shift encountered during deployment. In this paper, we propose a\nself-supervised online adaptation method for adapting the semantic keypoint\nrepresentation using a visual foundational model, geometric prior, and pseudo\nlabeling. Our preliminary experiments show that with minimal data and\nfine-tuning of parameters, the keypoint prediction model trained with labels on\nthe source domain can be adapted in a self-supervised manner to various\nchallenging target domains onboard the robot computer using our method. This\ncan enable fully autonomous row-following capability in under-canopy robots\nacross fields and crops without requiring human intervention.\n","authors":["Arun N. Sivakumar","Federico Magistri","Mateus V. Gasparino","Jens Behley","Cyrill Stachniss","Girish Chowdhary"],"pdf_url":"https://arxiv.org/pdf/2410.12411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10441v2","updated":"2024-10-16T09:45:06Z","published":"2024-10-14T12:35:12Z","title":"Free Video-LLM: Prompt-guided Visual Perception for Efficient\n  Training-free Video LLMs","summary":"  Vision-language large models have achieved remarkable success in various\nmulti-modal tasks, yet applying them to video understanding remains challenging\ndue to the inherent complexity and computational demands of video data. While\ntraining-based video-LLMs deliver high performance, they often require\nsubstantial resources for training and inference. Conversely, training-free\napproaches offer a more efficient alternative by adapting pre-trained\nimage-LLMs models for video tasks without additional training, but they face\ninference efficiency bottlenecks due to the large number of visual tokens\ngenerated from video frames. In this work, we present a novel prompt-guided\nvisual perception framework (abbreviated as Free Video-LLM) for efficient\ninference of training-free video LLMs. The proposed framework decouples\nspatial-temporal dimension and performs temporal frame sampling and spatial RoI\ncropping respectively based on task-specific prompts. Our method effectively\nreduces the number of visual tokens while maintaining high performance across\nmultiple video question-answering benchmarks. Extensive experiments demonstrate\nthat our approach achieves competitive results with significantly fewer tokens,\noffering an optimal trade-off between accuracy and computational efficiency\ncompared to state-of-the-art video LLMs. The code will be available at\nhttps://github.com/contrastive/FreeVideoLLM.\n","authors":["Kai Han","Jianyuan Guo","Yehui Tang","Wei He","Enhua Wu","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2410.10441v2.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2410.12407v1","updated":"2024-10-16T09:42:29Z","published":"2024-10-16T09:42:29Z","title":"Beyond Coarse-Grained Matching in Video-Text Retrieval","summary":"  Video-text retrieval has seen significant advancements, yet the ability of\nmodels to discern subtle differences in captions still requires verification.\nIn this paper, we introduce a new approach for fine-grained evaluation. Our\napproach can be applied to existing datasets by automatically generating hard\nnegative test captions with subtle single-word variations across nouns, verbs,\nadjectives, adverbs, and prepositions. We perform comprehensive experiments\nusing four state-of-the-art models across two standard benchmarks (MSR-VTT and\nVATEX) and two specially curated datasets enriched with detailed descriptions\n(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our\nanalyses show that the current evaluation benchmarks fall short in detecting a\nmodel's ability to perceive subtle single-word differences, 2) our fine-grained\nevaluation highlights the difficulty models face in distinguishing such subtle\nvariations. To enhance fine-grained understanding, we propose a new baseline\nthat can be easily combined with current methods. Experiments on our\nfine-grained evaluations demonstrate that this approach enhances a model's\nability to understand fine-grained differences.\n","authors":["Aozhu Chen","Hazel Doughty","Xirong Li","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12407v1.pdf","comment":"Accepted to ACCV 2024"},{"id":"http://arxiv.org/abs/2407.08374v3","updated":"2024-10-16T09:37:14Z","published":"2024-07-11T10:35:53Z","title":"Enhancing Robustness of Vision-Language Models through Orthogonality\n  Learning and Self-Regularization","summary":"  Efficient fine-tuning of vision-language models (VLMs) like CLIP for specific\ndownstream tasks is gaining significant attention. Previous works primarily\nfocus on prompt learning to adapt the CLIP into a variety of downstream tasks,\nhowever, suffering from task overfitting when fine-tuned on a small data set.\nIn this paper, we introduce an orthogonal fine-tuning method for efficiently\nfine-tuning pretrained weights and enabling enhanced robustness and\ngeneralization, while a self-regularization strategy is further exploited to\nmaintain the stability in terms of zero-shot generalization of VLMs, dubbed\nOrthSR. Specifically, trainable orthogonal matrices are injected seamlessly\ninto the transformer architecture and enforced with orthogonality constraint\nduring the training, benefiting from the norm-preserving property and thus\nleading to stable and faster convergence, while keeping the pre-trained weights\nfrozen. To alleviate deviation from fine-tuning, a self-regularization strategy\nis further employed to retain the generalization of the model during the\ntraining within a bypass manner. In addition, to enrich the sample diversity\nfor downstream tasks under the small dataset scenario, we first explore\nattentive CutOut data augmentation to boost the efficient fine-tuning, leading\nto better model fitting capacity for specific downstream task. Then we support\nthe theoretical analysis on how our approach improves the specific downstream\nperformance and maintains the generalizability. For the first time, we revisit\nthe CLIP and CoOp with our method to effectively improve the model on few-shot\nimage classficiation scenario on par with the elaborated prompt learning\nmethods.\n","authors":["Jinlong Li","Dong Zhao","Zequn Jie","Elisa Ricci","Lin Ma","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2407.08374v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.01472v2","updated":"2024-10-16T09:37:11Z","published":"2023-08-02T23:39:29Z","title":"Reverse Stable Diffusion: What prompt was used to generate this image?","summary":"  Text-to-image diffusion models have recently attracted the interest of many\nresearchers, and inverting the diffusion process can play an important role in\nbetter understanding the generative process and how to engineer prompts in\norder to obtain the desired images. To this end, we study the task of\npredicting the prompt embedding given an image generated by a generative\ndiffusion model. We consider a series of white-box and black-box models (with\nand without access to the weights of the diffusion network) to deal with the\nproposed task. We propose a novel learning framework comprising a joint prompt\nregression and multi-label vocabulary classification objective that generates\nimproved prompts. To further improve our method, we employ a curriculum\nlearning procedure that promotes the learning of image-prompt pairs with lower\nlabeling noise (i.e. that are better aligned). We conduct experiments on the\nDiffusionDB data set, predicting text prompts from images generated by Stable\nDiffusion. In addition, we make an interesting discovery: training a diffusion\nmodel on the prompt generation task can make the model generate images that are\nmuch better aligned with the input prompts, when the model is directly reused\nfor text-to-image generation. Our code is publicly available for download at\nhttps://github.com/CroitoruAlin/Reverse-Stable-Diffusion.\n","authors":["Florinel-Alin Croitoru","Vlad Hondru","Radu Tudor Ionescu","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2308.01472v2.pdf","comment":"Accepted for publication in Computer Vision and Image Understanding"},{"id":"http://arxiv.org/abs/2410.12402v1","updated":"2024-10-16T09:31:24Z","published":"2024-10-16T09:31:24Z","title":"De-Identification of Medical Imaging Data: A Comprehensive Tool for\n  Ensuring Patient Privacy","summary":"  Medical data employed in research frequently comprises sensitive patient\nhealth information (PHI), which is subject to rigorous legal frameworks such as\nthe General Data Protection Regulation (GDPR) or the Health Insurance\nPortability and Accountability Act (HIPAA). Consequently, these types of data\nmust be pseudonymized prior to utilisation, which presents a significant\nchallenge for many researchers. Given the vast array of medical data, it is\nnecessary to employ a variety of de-identification techniques. To facilitate\nthe anonymization process for medical imaging data, we have developed an\nopen-source tool that can be used to de-identify DICOM magnetic resonance\nimages, computer tomography images, whole slide images and magnetic resonance\ntwix raw data. Furthermore, the implementation of a neural network enables the\nremoval of text within the images. The proposed tool automates an elaborate\nanonymization pipeline for multiple types of inputs, reducing the need for\nadditional tools used for de-identification of imaging data. We make our code\npublicly available at\nhttps://github.com/code-lukas/medical_image_deidentification.\n","authors":["Moritz Rempe","Lukas Heine","Constantin Seibold","Fabian Hörst","Jens Kleesiek"],"pdf_url":"https://arxiv.org/pdf/2410.12402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19783v2","updated":"2024-10-16T09:28:22Z","published":"2024-05-30T07:48:32Z","title":"Instruction-Guided Visual Masking","summary":"  Instruction following is crucial in contemporary LLM. However, when extended\nto multimodal setting, it often suffers from misalignment between specific\ntextual instruction and targeted local region of an image. To achieve more\naccurate and nuanced multimodal instruction following, we introduce\nInstruction-guided Visual Masking (IVM), a new versatile visual grounding model\nthat is compatible with diverse multimodal models, such as LMM and robot model.\nBy constructing visual masks for instruction-irrelevant regions, IVM-enhanced\nmultimodal models can effectively focus on task-relevant image regions to\nbetter align with complex instructions. Specifically, we design a visual\nmasking data generation pipeline and create an IVM-Mix-1M dataset with 1\nmillion image-instruction pairs. We further introduce a new learning technique,\nDiscriminator Weighted Supervised Learning (DWSL) for preferential IVM training\nthat prioritizes high-quality data samples. Experimental results on generic\nmultimodal tasks such as VQA and embodied robotic control demonstrate the\nversatility of IVM, which as a plug-and-play tool, significantly boosts the\nperformance of diverse multimodal models, yielding new state-of-the-art results\nacross challenging multimodal benchmarks. Code, model and data are available at\nhttps://github.com/2toinf/IVM.\n","authors":["Jinliang Zheng","Jianxiong Li","Sijie Cheng","Yinan Zheng","Jiaming Li","Jihao Liu","Yu Liu","Jingjing Liu","Xianyuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2405.19783v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.12396v1","updated":"2024-10-16T09:25:11Z","published":"2024-10-16T09:25:11Z","title":"Feature Augmentation for Self-supervised Contrastive Learning: A Closer\n  Look","summary":"  Self-supervised contrastive learning heavily relies on the view variance\nbrought by data augmentation, so that it can learn a view-invariant pre-trained\nrepresentation. Beyond increasing the view variance for contrast, this work\nfocuses on improving the diversity of training data, to improve the\ngeneralization and robustness of the pre-trained models. To this end, we\npropose a unified framework to conduct data augmentation in the feature space,\nknown as feature augmentation. This strategy is domain-agnostic, which augments\nsimilar features to the original ones and thus improves the data diversity. We\nperform a systematic investigation of various feature augmentation\narchitectures, the gradient-flow skill, and the relationship between feature\naugmentation and traditional data augmentation. Our study reveals some\npractical principles for feature augmentation in self-contrastive learning. By\nintegrating feature augmentation on the instance discrimination or the instance\nsimilarity paradigm, we consistently improve the performance of pre-trained\nfeature learning and gain better generalization over the downstream image\nclassification and object detection task.\n","authors":["Yong Zhang","Rui Zhu","Shifeng Zhang","Xu Zhou","Shifeng Chen","Xiaofan Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12396v1.pdf","comment":"IJCNN 2024"},{"id":"http://arxiv.org/abs/2410.12394v1","updated":"2024-10-16T09:23:02Z","published":"2024-10-16T09:23:02Z","title":"Real-time Stereo-based 3D Object Detection for Streaming Perception","summary":"  The ability to promptly respond to environmental changes is crucial for the\nperception system of autonomous driving. Recently, a new task called streaming\nperception was proposed. It jointly evaluate the latency and accuracy into a\nsingle metric for video online perception. In this work, we introduce\nStreamDSGN, the first real-time stereo-based 3D object detection framework\ndesigned for streaming perception. StreamDSGN is an end-to-end framework that\ndirectly predicts the 3D properties of objects in the next moment by leveraging\nhistorical information, thereby alleviating the accuracy degradation of\nstreaming perception. Further, StreamDSGN applies three strategies to enhance\nthe perception accuracy: (1) A feature-flow-based fusion method, which\ngenerates a pseudo-next feature at the current moment to address the\nmisalignment issue between feature and ground truth. (2) An extra regression\nloss for explicit supervision of object motion consistency in consecutive\nframes. (3) A large kernel backbone with a large receptive field for\neffectively capturing long-range spatial contextual features caused by changes\nin object positions. Experiments on the KITTI Tracking dataset show that,\ncompared with the strong baseline, StreamDSGN significantly improves the\nstreaming average precision by up to 4.33%. Our code is available at\nhttps://github.com/weiyangdaren/streamDSGN-pytorch.\n","authors":["Changcai Li","Zonghua Gu","Gang Chen","Libo Huang","Wei Zhang","Huihui Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.12394v1.pdf","comment":"Streaming Perception, 3D Object Detection, NeurIPS2024 poster"},{"id":"http://arxiv.org/abs/2410.12381v1","updated":"2024-10-16T09:04:57Z","published":"2024-10-16T09:04:57Z","title":"HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of\n  Large Multimodal Models Through Coding Tasks","summary":"  Coding tasks have been valuable for evaluating Large Language Models (LLMs),\nas they demand the comprehension of high-level instructions, complex reasoning,\nand the implementation of functional programs -- core capabilities for\nadvancing Artificial General Intelligence. Despite the progress in Large\nMultimodal Models (LMMs), which extend LLMs with visual perception and\nunderstanding capabilities, there remains a notable lack of coding benchmarks\nthat rigorously assess these models, particularly in tasks that emphasize\nvisual reasoning. To address this gap, we introduce HumanEval-V, a novel and\nlightweight benchmark specifically designed to evaluate LMMs' visual\nunderstanding and reasoning capabilities through code generation. HumanEval-V\nincludes 108 carefully crafted, entry-level Python coding tasks derived from\nplatforms like CodeForces and Stack Overflow. Each task is adapted by modifying\nthe context and algorithmic patterns of the original problems, with visual\nelements redrawn to ensure distinction from the source, preventing potential\ndata leakage. LMMs are required to complete the code solution based on the\nprovided visual context and a predefined Python function signature outlining\nthe task requirements. Every task is equipped with meticulously handcrafted\ntest cases to ensure a thorough and reliable evaluation of model-generated\nsolutions. We evaluate 19 state-of-the-art LMMs using HumanEval-V, uncovering\nsignificant challenges. Proprietary models like GPT-4o achieve only 13% pass@1\nand 36.4% pass@10, while open-weight models with 70B parameters score below 4%\npass@1. Ablation studies further reveal the limitations of current LMMs in\nvision reasoning and coding capabilities. These results underscore key areas\nfor future research to enhance LMMs' capabilities. We have open-sourced our\ncode and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark.\n","authors":["Fengji Zhang","Linquan Wu","Huiyu Bai","Guancheng Lin","Xiao Li","Xiao Yu","Yue Wang","Bei Chen","Jacky Keung"],"pdf_url":"https://arxiv.org/pdf/2410.12381v1.pdf","comment":"homepage https://humaneval-v.github.io/"},{"id":"http://arxiv.org/abs/2410.12379v1","updated":"2024-10-16T08:55:09Z","published":"2024-10-16T08:55:09Z","title":"Stylistic Multi-Task Analysis of Ukiyo-e Woodblock Prints","summary":"  In this work we present a large-scale dataset of \\textit{Ukiyo-e} woodblock\nprints. Unlike previous works and datasets in the artistic domain that\nprimarily focus on western art, this paper explores this pre-modern Japanese\nart form with the aim of broadening the scope for stylistic analysis and to\nprovide a benchmark to evaluate a variety of art focused Computer Vision\napproaches. Our dataset consists of over $175.000$ prints with corresponding\nmetadata (\\eg artist, era, and creation date) from the 17th century to present\nday. By approaching stylistic analysis as a Multi-Task problem we aim to more\nefficiently utilize the available metadata, and learn more general\nrepresentations of style. We show results for well-known baselines and\nstate-of-the-art multi-task learning frameworks to enable future comparison,\nand to encourage stylistic analysis on this artistic domain.\n","authors":["Selina Khan","Nanne van Noord"],"pdf_url":"https://arxiv.org/pdf/2410.12379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07914v3","updated":"2024-10-16T08:52:42Z","published":"2024-09-12T10:30:44Z","title":"InterACT: Inter-dependency Aware Action Chunking with Hierarchical\n  Attention Transformers for Bimanual Manipulation","summary":"  Bimanual manipulation presents unique challenges compared to unimanual tasks\ndue to the complexity of coordinating two robotic arms. In this paper, we\nintroduce InterACT: Inter-dependency aware Action Chunking with Hierarchical\nAttention Transformers, a novel imitation learning framework designed\nspecifically for bimanual manipulation. InterACT leverages hierarchical\nattention mechanisms to effectively capture inter-dependencies between dual-arm\njoint states and visual inputs. The framework comprises a Hierarchical\nAttention Encoder, which processes multi-modal inputs through segment-wise and\ncross-segment attention mechanisms, and a Multi-arm Decoder that generates each\narm's action predictions in parallel, while sharing information between the\narms through synchronization blocks by providing the other arm's intermediate\noutput as context. Our experiments, conducted on various simulated and\nreal-world bimanual manipulation tasks, demonstrate that InterACT outperforms\nexisting methods. Detailed ablation studies further validate the significance\nof key components, including the impact of CLS tokens, cross-segment encoders,\nand synchronization blocks on task performance. We provide supplementary\nmaterials and videos on our project page.\n","authors":["Andrew Lee","Ian Chuang","Ling-Yuan Chen","Iman Soltani"],"pdf_url":"https://arxiv.org/pdf/2409.07914v3.pdf","comment":"Accepted at Conference on Robot Learning (CoRL) 2024"},{"id":"http://arxiv.org/abs/2410.12372v1","updated":"2024-10-16T08:44:23Z","published":"2024-10-16T08:44:23Z","title":"GAN Based Top-Down View Synthesis in Reinforcement Learning Environments","summary":"  Human actions are based on the mental perception of the environment. Even\nwhen all the aspects of an environment are not visible, humans have an internal\nmental model that can generalize the partially visible scenes to fully\nconstructed and connected views. This internal mental model uses learned\nabstract representations of spatial and temporal aspects of the environments\nencountered in the past.\n  Artificial agents in reinforcement learning environments also benefit by\nlearning a representation of the environment from experience. It provides the\nagent with viewpoints that are not directly visible to it, helping it make\nbetter policy decisions. It can also be used to predict the future state of the\nenvironment.\n  This project explores learning the top-down view of an RL environment based\non the artificial agent's first-person view observations with a generative\nadversarial network(GAN). The top-down view is useful as it provides a complete\noverview of the environment by building a map of the entire environment. It\nprovides information about the objects' dimensions and shapes along with their\nrelative positions with one another. Initially, when only a partial observation\nof the environment is visible to the agent, only a partial top-down view is\ngenerated. As the agent explores the environment through a set of actions, the\ngenerated top-down view becomes complete. This generated top-down view can\nassist the agent in deducing better policy decisions. The focus of the project\nis to learn the top-down view of an RL environment. It doesn't deal with any\nReinforcement Learning task.\n","authors":["Usama Younus","Vinoj Jayasundara","Shivam Mishra","Suleyman Aslan"],"pdf_url":"https://arxiv.org/pdf/2410.12372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12369v1","updated":"2024-10-16T08:41:19Z","published":"2024-10-16T08:41:19Z","title":"Context-Infused Visual Grounding for Art","summary":"  Many artwork collections contain textual attributes that provide rich and\ncontextualised descriptions of artworks. Visual grounding offers the potential\nfor localising subjects within these descriptions on images, however, existing\napproaches are trained on natural images and generalise poorly to art. In this\npaper, we present CIGAr (Context-Infused GroundingDINO for Art), a visual\ngrounding approach which utilises the artwork descriptions during training as\ncontext, thereby enabling visual grounding on art. In addition, we present a\nnew dataset, Ukiyo-eVG, with manually annotated phrase-grounding annotations,\nand we set a new state-of-the-art for object detection on two artwork datasets.\n","authors":["Selina Khan","Nanne van Noord"],"pdf_url":"https://arxiv.org/pdf/2410.12369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08401v2","updated":"2024-10-16T08:38:06Z","published":"2024-04-12T11:15:15Z","title":"No Bells, Just Whistles: Sports Field Registration by Leveraging\n  Geometric Properties","summary":"  Broadcast sports field registration is traditionally addressed as a\nhomography estimation task, mapping the visible image area to a planar field\nmodel, predominantly focusing on the main camera shot. Addressing the\nshortcomings of previous approaches, we propose a novel calibration pipeline\nenabling camera calibration using a 3D soccer field model and extending the\nprocess to assess the multiple-view nature of broadcast videos. Our approach\nbegins with a keypoint generation pipeline derived from SoccerNet dataset\nannotations, leveraging the geometric properties of the court. Subsequently, we\nexecute classical camera calibration through DLT algorithm in a minimalist\nfashion, without further refinement. Through extensive experimentation on\nreal-world soccer broadcast datasets such as SoccerNet-Calibration, WorldCup\n2014 and TS- WorldCup, our method demonstrates superior performance in both\nmultiple- and single-view 3D camera calibration while maintaining competitive\nresults in homography estimation compared to state-of-the-art techniques.\n","authors":["Marc Gutiérrez-Pérez","Antonio Agudo"],"pdf_url":"https://arxiv.org/pdf/2404.08401v2.pdf","comment":"Accepted in CVPRW 2024"},{"id":"http://arxiv.org/abs/2409.03200v2","updated":"2024-10-16T08:36:17Z","published":"2024-09-05T02:46:36Z","title":"Active Fake: DeepFake Camouflage","summary":"  DeepFake technology has gained significant attention due to its ability to\nmanipulate facial attributes with high realism, raising serious societal\nconcerns. Face-Swap DeepFake is the most harmful among these techniques, which\nfabricates behaviors by swapping original faces with synthesized ones. Existing\nforensic methods, primarily based on Deep Neural Networks (DNNs), effectively\nexpose these manipulations and have become important authenticity indicators.\nHowever, these methods mainly concentrate on capturing the blending\ninconsistency in DeepFake faces, raising a new security issue, termed Active\nFake, emerges when individuals intentionally create blending inconsistency in\ntheir authentic videos to evade responsibility. This tactic is called DeepFake\nCamouflage. To achieve this, we introduce a new framework for creating DeepFake\ncamouflage that generates blending inconsistencies while ensuring\nimperceptibility, effectiveness, and transferability. This framework, optimized\nvia an adversarial learning strategy, crafts imperceptible yet effective\ninconsistencies to mislead forensic detectors. Extensive experiments\ndemonstrate the effectiveness and robustness of our method, highlighting the\nneed for further research in active fake detection.\n","authors":["Pu Sun","Honggang Qi","Yuezun Li"],"pdf_url":"https://arxiv.org/pdf/2409.03200v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10394v2","updated":"2024-10-16T08:20:44Z","published":"2024-10-14T11:30:18Z","title":"PIVOT-R: Primitive-Driven Waypoint-Aware World Model for Robotic\n  Manipulation","summary":"  Language-guided robotic manipulation is a challenging task that requires an\nembodied agent to follow abstract user instructions to accomplish various\ncomplex manipulation tasks. Previous work trivially fitting the data without\nrevealing the relation between instruction and low-level executable actions,\nthese models are prone to memorizing the surficial pattern of the data instead\nof acquiring the transferable knowledge, and thus are fragile to dynamic\nenvironment changes. To address this issue, we propose a PrIrmitive-driVen\nwaypOinT-aware world model for Robotic manipulation (PIVOT-R) that focuses\nsolely on the prediction of task-relevant waypoints. Specifically, PIVOT-R\nconsists of a Waypoint-aware World Model (WAWM) and a lightweight action\nprediction module. The former performs primitive action parsing and\nprimitive-driven waypoint prediction, while the latter focuses on decoding\nlow-level actions. Additionally, we also design an asynchronous hierarchical\nexecutor (AHE), which can use different execution frequencies for different\nmodules of the model, thereby helping the model reduce computational redundancy\nand improve model execution efficiency. Our PIVOT-R outperforms\nstate-of-the-art (SoTA) open-source models on the SeaWave benchmark, achieving\nan average relative improvement of 19.45% across four levels of instruction\ntasks. Moreover, compared to the synchronously executed PIVOT-R, the execution\nefficiency of PIVOT-R with AHE is increased by 28-fold, with only a 2.9% drop\nin performance. These results provide compelling evidence that our PIVOT-R can\nsignificantly improve both the performance and efficiency of robotic\nmanipulation.\n","authors":["Kaidong Zhang","Pengzhen Ren","Bingqian Lin","Junfan Lin","Shikui Ma","Hang Xu","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2410.10394v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.12346v1","updated":"2024-10-16T08:07:18Z","published":"2024-10-16T08:07:18Z","title":"Towards Flexible and Efficient Diffusion Low Light Enhancer","summary":"  Diffusion-based Low-Light Image Enhancement (LLIE) has demonstrated\nsignificant success in improving the visibility of low-light images. However,\nthe substantial computational burden introduced by the iterative sampling\nprocess remains a major concern. Current acceleration methods, whether\ntraining-based or training-free, often lead to significant performance\ndegradation. As a result, to achieve an efficient student model with\nperformance comparable to that of existing multi-step teacher model, it is\nusually necessary to retrain a more capable teacher model. This approach\nintroduces inflexibility, as it requires additional training to enhance the\nteacher's performance. To address these challenges, we propose\n\\textbf{Re}flectance-aware \\textbf{D}iffusion with \\textbf{Di}stilled\n\\textbf{T}rajectory (\\textbf{ReDDiT}), a step distillation framework\nspecifically designed for LLIE. ReDDiT trains a student model to replicate the\nteacher's trajectory in fewer steps while also possessing the ability to\nsurpass the teacher's performance. Specifically, we first introduce a\ntrajectory decoder from the teacher model to provide guidance. Subsequently, a\nreflectance-aware trajectory refinement module is incorporated into the\ndistillation process to enable more deterministic guidance from the teacher\nmodel. Our framework achieves comparable performance to previous\ndiffusion-based methods with redundant steps in just 2 steps while establishing\nnew state-of-the-art (SOTA) results with 8 or 4 steps. Comprehensive\nexperimental evaluations on 10 benchmark datasets validate the effectiveness of\nour method, consistently outperforming existing SOTA methods.\n","authors":["Guanzhou Lan","Qianli Ma","Yuqi Yang","Zhigang Wang","Dong Wang","Yuan Yuan","Bin Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.12346v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2410.12342v1","updated":"2024-10-16T08:02:49Z","published":"2024-10-16T08:02:49Z","title":"TAS: Distilling Arbitrary Teacher and Student via a Hybrid Assistant","summary":"  Most knowledge distillation (KD) methodologies predominantly focus on\nteacher-student pairs with similar architectures, such as both being\nconvolutional neural networks (CNNs). However, the potential and flexibility of\nKD can be greatly improved by expanding it to novel Cross-Architecture KD\n(CAKD), where the knowledge of homogeneous and heterogeneous teachers can be\ntransferred flexibly to a given student. The primary challenge in CAKD lies in\nthe substantial feature gaps between heterogeneous models, originating from the\ndistinction of their inherent inductive biases and module functions. To this\nend, we introduce an assistant model as a bridge to facilitate smooth feature\nknowledge transfer between heterogeneous teachers and students. More\nimportantly, within our proposed design principle, the assistant model combines\nthe advantages of cross-architecture inductive biases and module functions by\nmerging convolution and attention modules derived from both student and teacher\nmodule functions. Furthermore, we observe that heterogeneous features exhibit\ndiverse spatial distributions in CAKD, hindering the effectiveness of\nconventional pixel-wise mean squared error (MSE) loss. Therefore, we leverage a\nspatial-agnostic InfoNCE loss to align features after spatial smoothing,\nthereby improving the feature alignments in CAKD. Our proposed method is\nevaluated across some homogeneous model pairs and arbitrary heterogeneous\ncombinations of CNNs, ViTs, and MLPs, achieving state-of-the-art performance\nfor distilled models with a maximum gain of 11.47% on CIFAR-100 and 3.67% on\nImageNet-1K. Our code and models will be released.\n","authors":["Guopeng Li","Qiang Wang","Ke Yan","Shouhong Ding","Yuan Gao","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2410.12342v1.pdf","comment":"18 pages, 6 figures, and 12 tables"},{"id":"http://arxiv.org/abs/2410.12337v1","updated":"2024-10-16T07:59:07Z","published":"2024-10-16T07:59:07Z","title":"ARIC: An Activity Recognition Dataset in Classroom Surveillance Images","summary":"  The application of activity recognition in the ``AI + Education\" field is\ngaining increasing attention. However, current work mainly focuses on the\nrecognition of activities in manually captured videos and a limited number of\nactivity types, with little attention given to recognizing activities in\nsurveillance images from real classrooms. Activity recognition in classroom\nsurveillance images faces multiple challenges, such as class imbalance and high\nactivity similarity. To address this gap, we constructed a novel multimodal\ndataset focused on classroom surveillance image activity recognition called\nARIC (Activity Recognition In Classroom). The ARIC dataset has advantages of\nmultiple perspectives, 32 activity categories, three modalities, and real-world\nclassroom scenarios. In addition to the general activity recognition tasks, we\nalso provide settings for continual learning and few-shot continual learning.\nWe hope that the ARIC dataset can act as a facilitator for future analysis and\nresearch for open teaching scenarios. You can download preliminary data from\nhttps://ivipclab.github.io/publication_ARIC/ARIC.\n","authors":["Linfeng Xu","Fanman Meng","Qingbo Wu","Lili Pan","Heqian Qiu","Lanxiao Wang","Kailong Chen","Kanglei Geng","Yilei Qian","Haojie Wang","Shuchang Zhou","Shimou Ling","Zejia Liu","Nanlin Chen","Yingjie Xu","Shaoxu Cheng","Bowen Tan","Ziyong Xu","Hongliang Li"],"pdf_url":"https://arxiv.org/pdf/2410.12337v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2409.03354"},{"id":"http://arxiv.org/abs/2410.12332v1","updated":"2024-10-16T07:52:57Z","published":"2024-10-16T07:52:57Z","title":"MC-Bench: A Benchmark for Multi-Context Visual Grounding in the Era of\n  MLLMs","summary":"  While multimodal large language models (MLLMs) have demonstrated\nextraordinary vision-language understanding capabilities and shown potential to\nserve as general-purpose assistants, their abilities to solve instance-level\nvisual-language problems beyond a single image warrant further exploration. In\norder to assess these unproven abilities of MLLMs, this paper proposes a new\nvisual grounding task called multi-context visual grounding, which aims to\nlocalize instances of interest across multiple images based on open-ended text\nprompts. To facilitate this research, we meticulously construct a new dataset\nMC-Bench for benchmarking the visual grounding capabilities of MLLMs. MC-Bench\nfeatures 2K high-quality and manually annotated samples, consisting of\ninstance-level labeled image pairs and corresponding text prompts that indicate\nthe target instances in the images. In total, there are three distinct styles\nof text prompts, covering 20 practical skills. We benchmark over 20\nstate-of-the-art MLLMs and foundation models with potential multi-context\nvisual grounding capabilities. Our evaluation reveals a non-trivial performance\ngap between existing MLLMs and humans across all metrics. We also observe that\nexisting MLLMs typically outperform foundation models without LLMs only on\nimage-level metrics, and the specialist MLLMs trained on single images often\nstruggle to generalize to multi-image scenarios. Moreover, a simple stepwise\nbaseline integrating advanced MLLM and a detector can significantly surpass\nprior end-to-end MLLMs. We hope our MC-Bench and empirical findings can\nencourage the research community to further explore and enhance the untapped\npotentials of MLLMs in instance-level tasks, particularly in multi-image\ncontexts. Project page: https://xuyunqiu.github.io/MC-Bench/.\n","authors":["Yunqiu Xu","Linchao Zhu","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.12332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01068v2","updated":"2024-10-16T07:49:27Z","published":"2024-09-02T08:43:50Z","title":"Progressive Retinal Image Registration via Global and Local Deformable\n  Transformations","summary":"  Retinal image registration plays an important role in the ophthalmological\ndiagnosis process. Since there exist variances in viewing angles and anatomical\nstructures across different retinal images, keypoint-based approaches become\nthe mainstream methods for retinal image registration thanks to their\nrobustness and low latency. These methods typically assume the retinal surfaces\nare planar, and adopt feature matching to obtain the homography matrix that\nrepresents the global transformation between images. Yet, such a planar\nhypothesis inevitably introduces registration errors since retinal surface is\napproximately curved. This limitation is more prominent when registering image\npairs with significant differences in viewing angles. To address this problem,\nwe propose a hybrid registration framework called HybridRetina, which\nprogressively registers retinal images with global and local deformable\ntransformations. For that, we use a keypoint detector and a deformation network\ncalled GAMorph to estimate the global transformation and local deformable\ntransformation, respectively. Specifically, we integrate multi-level pixel\nrelation knowledge to guide the training of GAMorph. Additionally, we utilize\nan edge attention module that includes the geometric priors of the images,\nensuring the deformation field focuses more on the vascular regions of clinical\ninterest. Experiments on two widely-used datasets, FIRE and FLoRI21, show that\nour proposed HybridRetina significantly outperforms some state-of-the-art\nmethods. The code is available at\nhttps://github.com/lyp-deeplearning/awesome-retinal-registration.\n","authors":["Yepeng Liu","Baosheng Yu","Tian Chen","Yuliang Gu","Bo Du","Yongchao Xu","Jun Cheng"],"pdf_url":"https://arxiv.org/pdf/2409.01068v2.pdf","comment":"Accepted at BIBM 2024"},{"id":"http://arxiv.org/abs/2410.12328v1","updated":"2024-10-16T07:48:53Z","published":"2024-10-16T07:48:53Z","title":"Improved Anomaly Detection through Conditional Latent Space VAE\n  Ensembles","summary":"  We propose a novel Conditional Latent space Variational Autoencoder (CL-VAE)\nto perform improved pre-processing for anomaly detection on data with known\ninlier classes and unknown outlier classes. This proposed variational\nautoencoder (VAE) improves latent space separation by conditioning on\ninformation within the data. The method fits a unique prior distribution to\neach class in the dataset, effectively expanding the classic prior distribution\nfor VAEs to include a Gaussian mixture model. An ensemble of these VAEs are\nmerged in the latent spaces to form a group consensus that greatly improves the\naccuracy of anomaly detection across data sets. Our approach is compared\nagainst the capabilities of a typical VAE, a CNN, and a PCA, with regards AUC\nfor anomaly detection. The proposed model shows increased accuracy in anomaly\ndetection, achieving an AUC of 97.4% on the MNIST dataset compared to 95.7% for\nthe second best model. In addition, the CL-VAE shows increased benefits from\nensembling, a more interpretable latent space, and an increased ability to\nlearn patterns in complex data with limited model sizes.\n","authors":["Oskar Åström","Alexandros Sopasakis"],"pdf_url":"https://arxiv.org/pdf/2410.12328v1.pdf","comment":"13 pages of main article, 19 pages including references and appendix,\n  4 figures"},{"id":"http://arxiv.org/abs/2410.12324v1","updated":"2024-10-16T07:44:56Z","published":"2024-10-16T07:44:56Z","title":"PAPL-SLAM: Principal Axis-Anchored Monocular Point-Line SLAM","summary":"  In point-line SLAM systems, the utilization of line structural information\nand the optimization of lines are two significant problems. The former is\nusually addressed through structural regularities, while the latter typically\ninvolves using minimal parameter representations of lines in optimization.\nHowever, separating these two steps leads to the loss of constraint information\nto each other. We anchor lines with similar directions to a principal axis and\noptimize them with $n+2$ parameters for $n$ lines, solving both problems\ntogether. Our method considers scene structural information, which can be\neasily extended to different world hypotheses while significantly reducing the\nnumber of line parameters to be optimized, enabling rapid and accurate mapping\nand tracking. To further enhance the system's robustness and avoid mismatch, we\nhave modeled the line-axis probabilistic data association and provided the\nalgorithm for axis creation, updating, and optimization. Additionally,\nconsidering that most real-world scenes conform to the Atlanta World\nhypothesis, we provide a structural line detection strategy based on vertical\npriors and vanishing points. Experimental results and ablation studies on\nvarious indoor and outdoor datasets demonstrate the effectiveness of our\nsystem.\n","authors":["Guanghao Li","Yu Cao","Qi Chen","Yifan Yang","Jian Pu"],"pdf_url":"https://arxiv.org/pdf/2410.12324v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.12312v1","updated":"2024-10-16T07:25:24Z","published":"2024-10-16T07:25:24Z","title":"FaceChain-FACT: Face Adapter with Decoupled Training for\n  Identity-preserved Personalization","summary":"  In the field of human-centric personalized image generation, the\nadapter-based method obtains the ability to customize and generate portraits by\ntext-to-image training on facial data. This allows for identity-preserved\npersonalization without additional fine-tuning in inference. Although there are\nimprovements in efficiency and fidelity, there is often a significant\nperformance decrease in test following ability, controllability, and diversity\nof generated faces compared to the base model. In this paper, we analyze that\nthe performance degradation is attributed to the failure to decouple identity\nfeatures from other attributes during extraction, as well as the failure to\ndecouple the portrait generation training from the overall generation task. To\naddress these issues, we propose the Face Adapter with deCoupled Training\n(FACT) framework, focusing on both model architecture and training strategy. To\ndecouple identity features from others, we leverage a transformer-based\nface-export encoder and harness fine-grained identity features. To decouple the\nportrait generation training, we propose Face Adapting Increment\nRegularization~(FAIR), which effectively constrains the effect of face adapters\non the facial region, preserving the generative ability of the base model.\nAdditionally, we incorporate a face condition drop and shuffle mechanism,\ncombined with curriculum learning, to enhance facial controllability and\ndiversity. As a result, FACT solely learns identity preservation from training\ndata, thereby minimizing the impact on the original text-to-image capabilities\nof the base model. Extensive experiments show that FACT has both\ncontrollability and fidelity in both text-to-image generation and inpainting\nsolutions for portrait generation.\n","authors":["Cheng Yu","Haoyu Xie","Lei Shang","Yang Liu","Jun Dan","Baigui Sun","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2410.12312v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.07167v2","updated":"2024-10-16T07:23:03Z","published":"2024-10-09T17:59:04Z","title":"Deciphering Cross-Modal Alignment in Large Vision-Language Models with\n  Modality Integration Rate","summary":"  We present the Modality Integration Rate (MIR), an effective, robust, and\ngeneralized metric to indicate the multi-modal pre-training quality of Large\nVision Language Models (LVLMs). Large-scale pre-training plays a critical role\nin building capable LVLMs, while evaluating its training quality without the\ncostly supervised fine-tuning stage is under-explored. Loss, perplexity, and\nin-context evaluation results are commonly used pre-training metrics for Large\nLanguage Models (LLMs), while we observed that these metrics are less\nindicative when aligning a well-trained LLM with a new modality. Due to the\nlack of proper metrics, the research of LVLMs in the critical pre-training\nstage is hindered greatly, including the training data choice, efficient module\ndesign, etc. In this paper, we propose evaluating the pre-training quality from\nthe inter-modal distribution distance perspective and present MIR, the Modality\nIntegration Rate, which is 1) \\textbf{Effective} to represent the pre-training\nquality and show a positive relation with the benchmark performance after\nsupervised fine-tuning. 2) \\textbf{Robust} toward different training/evaluation\ndata. 3) \\textbf{Generalize} across training configurations and architecture\nchoices. We conduct a series of pre-training experiments to explore the\neffectiveness of MIR and observe satisfactory results that MIR is indicative\nabout training data selection, training strategy schedule, and model\narchitecture design to get better pre-training results. We hope MIR could be a\nhelpful metric for building capable LVLMs and inspire the following research\nabout modality alignment in different areas. Our code is at:\nhttps://github.com/shikiw/Modality-Integration-Rate.\n","authors":["Qidong Huang","Xiaoyi Dong","Pan Zhang","Yuhang Zang","Yuhang Cao","Jiaqi Wang","Dahua Lin","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.07167v2.pdf","comment":"Project page: https://github.com/shikiw/Modality-Integration-Rate"},{"id":"http://arxiv.org/abs/2408.04804v2","updated":"2024-10-16T07:20:58Z","published":"2024-08-09T01:21:15Z","title":"Hyper-YOLO: When Visual Object Detection Meets Hypergraph Computation","summary":"  We introduce Hyper-YOLO, a new object detection method that integrates\nhypergraph computations to capture the complex high-order correlations among\nvisual features. Traditional YOLO models, while powerful, have limitations in\ntheir neck designs that restrict the integration of cross-level features and\nthe exploitation of high-order feature interrelationships. To address these\nchallenges, we propose the Hypergraph Computation Empowered Semantic Collecting\nand Scattering (HGC-SCS) framework, which transposes visual feature maps into a\nsemantic space and constructs a hypergraph for high-order message propagation.\nThis enables the model to acquire both semantic and structural information,\nadvancing beyond conventional feature-focused learning. Hyper-YOLO incorporates\nthe proposed Mixed Aggregation Network (MANet) in its backbone for enhanced\nfeature extraction and introduces the Hypergraph-Based Cross-Level and\nCross-Position Representation Network (HyperC2Net) in its neck. HyperC2Net\noperates across five scales and breaks free from traditional grid structures,\nallowing for sophisticated high-order interactions across levels and positions.\nThis synergy of components positions Hyper-YOLO as a state-of-the-art\narchitecture in various scale models, as evidenced by its superior performance\non the COCO dataset. Specifically, Hyper-YOLO-N significantly outperforms the\nadvanced YOLOv8-N and YOLOv9-T with 12\\% $\\text{AP}^{val}$ and 9\\%\n$\\text{AP}^{val}$ improvements. The source codes are at\nttps://github.com/iMoonLab/Hyper-YOLO.\n","authors":["Yifan Feng","Jiangang Huang","Shaoyi Du","Shihui Ying","Jun-Hai Yong","Yipeng Li","Guiguang Ding","Rongrong Ji","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2408.04804v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12307v1","updated":"2024-10-16T07:18:36Z","published":"2024-10-16T07:18:36Z","title":"DAT: Improving Adversarial Robustness via Generative Amplitude Mix-up in\n  Frequency Domain","summary":"  To protect deep neural networks (DNNs) from adversarial attacks, adversarial\ntraining (AT) is developed by incorporating adversarial examples (AEs) into\nmodel training. Recent studies show that adversarial attacks disproportionately\nimpact the patterns within the phase of the sample's frequency spectrum --\ntypically containing crucial semantic information -- more than those in the\namplitude, resulting in the model's erroneous categorization of AEs. We find\nthat, by mixing the amplitude of training samples' frequency spectrum with\nthose of distractor images for AT, the model can be guided to focus on phase\npatterns unaffected by adversarial perturbations. As a result, the model's\nrobustness can be improved. Unfortunately, it is still challenging to select\nappropriate distractor images, which should mix the amplitude without affecting\nthe phase patterns. To this end, in this paper, we propose an optimized\nAdversarial Amplitude Generator (AAG) to achieve a better tradeoff between\nimproving the model's robustness and retaining phase patterns. Based on this\ngenerator, together with an efficient AE production procedure, we design a new\nDual Adversarial Training (DAT) strategy. Experiments on various datasets show\nthat our proposed DAT leads to significantly improved robustness against\ndiverse adversarial attacks.\n","authors":["Fengpeng Li","Kemou Li","Haiwei Wu","Jinyu Tian","Jiantao Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.12307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11610v2","updated":"2024-10-16T07:09:12Z","published":"2024-10-15T13:46:19Z","title":"Depth Estimation From Monocular Images With Enhanced Encoder-Decoder\n  Architecture","summary":"  Estimating depth from a single 2D image is a challenging task because of the\nneed for stereo or multi-view data, which normally provides depth information.\nThis paper deals with this challenge by introducing a novel deep learning-based\napproach using an encoder-decoder architecture, where the Inception-ResNet-v2\nmodel is utilized as the encoder. According to the available literature, this\nis the first instance of using Inception-ResNet-v2 as an encoder for monocular\ndepth estimation, illustrating better performance than previous models. The use\nof Inception-ResNet-v2 enables our model to capture complex objects and\nfine-grained details effectively that are generally difficult to predict.\nBesides, our model incorporates multi-scale feature extraction to enhance depth\nprediction accuracy across different kinds of object sizes and distances. We\npropose a composite loss function consisting of depth loss, gradient edge loss,\nand SSIM loss, where the weights are fine-tuned to optimize the weighted sum,\nensuring better balance across different aspects of depth estimation.\nExperimental results on the NYU Depth V2 dataset show that our model achieves\nstate-of-the-art performance, with an ARE of 0.064, RMSE of 0.228, and accuracy\n($\\delta$ $<1.25$) of 89.3%. These metrics demonstrate that our model\neffectively predicts depth, even in challenging circumstances, providing a\nscalable solution for real-world applications in robotics, 3D reconstruction,\nand augmented reality.\n","authors":["Dabbrata Das","Argho Deb Das","Farhan Sadaf"],"pdf_url":"https://arxiv.org/pdf/2410.11610v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17137v3","updated":"2024-10-16T07:08:57Z","published":"2023-11-28T18:59:02Z","title":"Generative Models: What Do They Know? Do They Know Things? Let's Find\n  Out!","summary":"  Generative models excel at mimicking real scenes, suggesting they might\ninherently encode important intrinsic scene properties. In this paper, we aim\nto explore the following key questions: (1) What intrinsic knowledge do\ngenerative models like GANs, Autoregressive models, and Diffusion models\nencode? (2) Can we establish a general framework to recover intrinsic\nrepresentations from these models, regardless of their architecture or model\ntype? (3) How minimal can the required learnable parameters and labeled data be\nto successfully recover this knowledge? (4) Is there a direct link between the\nquality of a generative model and the accuracy of the recovered scene\nintrinsics?\n  Our findings indicate that a small Low-Rank Adaptators (LoRA) can recover\nintrinsic images-depth, normals, albedo and shading-across different generators\n(Autoregressive, GANs and Diffusion) while using the same decoder head that\ngenerates the image. As LoRA is lightweight, we introduce very few learnable\nparameters (as few as 0.04% of Stable Diffusion model weights for a rank of 2),\nand we find that as few as 250 labeled images are enough to generate intrinsic\nimages with these LoRA modules. Finally, we also show a positive correlation\nbetween the generative model's quality and the accuracy of the recovered\nintrinsics through control experiments.\n","authors":["Xiaodan Du","Nicholas Kolkin","Greg Shakhnarovich","Anand Bhattad"],"pdf_url":"https://arxiv.org/pdf/2311.17137v3.pdf","comment":"https://intrinsic-lora.github.io/"},{"id":"http://arxiv.org/abs/2403.14166v3","updated":"2024-10-16T07:07:21Z","published":"2024-03-21T06:34:46Z","title":"Mini-Splatting: Representing Scenes with a Constrained Number of\n  Gaussians","summary":"  In this study, we explore the challenge of efficiently representing scenes\nwith a constrained number of Gaussians. Our analysis shifts from traditional\ngraphics and 2D computer vision to the perspective of point clouds,\nhighlighting the inefficient spatial distribution of Gaussian representation as\na key limitation in model performance. To address this, we introduce strategies\nfor densification including blur split and depth reinitialization, and\nsimplification through intersection preserving and sampling. These techniques\nreorganize the spatial positions of the Gaussians, resulting in significant\nimprovements across various datasets and benchmarks in terms of rendering\nquality, resource consumption, and storage compression. Our Mini-Splatting\nintegrates seamlessly with the original rasterization pipeline, providing a\nstrong baseline for future research in Gaussian-Splatting-based works.\n\\href{https://github.com/fatPeter/mini-splatting}{Code is available}.\n","authors":["Guangchi Fang","Bing Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14166v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12295v1","updated":"2024-10-16T06:55:02Z","published":"2024-10-16T06:55:02Z","title":"Consistency Calibration: Improving Uncertainty Calibration via\n  Consistency among Perturbed Neighbors","summary":"  Calibration is crucial in deep learning applications, especially in fields\nlike healthcare and autonomous driving, where accurate confidence estimates are\nvital for decision-making. However, deep neural networks often suffer from\nmiscalibration, with reliability diagrams and Expected Calibration Error (ECE)\nbeing the only standard perspective for evaluating calibration performance. In\nthis paper, we introduce the concept of consistency as an alternative\nperspective on model calibration, inspired by uncertainty estimation literature\nin large language models (LLMs). We highlight its advantages over the\ntraditional reliability-based view. Building on this concept, we propose a\npost-hoc calibration method called Consistency Calibration (CC), which adjusts\nconfidence based on the model's consistency across perturbed inputs. CC is\nparticularly effective in locally uncertainty estimation, as it requires no\nadditional data samples or label information, instead generating input\nperturbations directly from the source data. Moreover, we show that performing\nperturbations at the logit level significantly improves computational\nefficiency. We validate the effectiveness of CC through extensive comparisons\nwith various post-hoc and training-time calibration methods, demonstrating\nstate-of-the-art performance on standard datasets such as CIFAR-10, CIFAR-100,\nand ImageNet, as well as on long-tailed datasets like ImageNet-LT.\n","authors":["Linwei Tao","Haolan Guo","Minjing Dong","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2410.12295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12284v1","updated":"2024-10-16T06:43:02Z","published":"2024-10-16T06:43:02Z","title":"Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical\n  Decision-Support Setting","summary":"  The growing capabilities of AI models are leading to their wider use,\nincluding in safety-critical domains. Explainable AI (XAI) aims to make these\nmodels safer to use by making their inference process more transparent.\nHowever, current explainability methods are seldom evaluated in the way they\nare intended to be used: by real-world end users. To address this, we conducted\na large-scale user study with 85 healthcare practitioners in the context of\nhuman-AI collaborative chest X-ray analysis. We evaluated three types of\nexplanations: visual explanations (saliency maps), natural language\nexplanations, and a combination of both modalities. We specifically examined\nhow different explanation types influence users depending on whether the AI\nadvice and explanations are factually correct. We find that text-based\nexplanations lead to significant over-reliance, which is alleviated by\ncombining them with saliency maps. We also observe that the quality of\nexplanations, that is, how much factually correct information they entail, and\nhow much this aligns with AI correctness, significantly impacts the usefulness\nof the different explanation types.\n","authors":["Maxime Kayser","Bayar Menzat","Cornelius Emde","Bogdan Bercean","Alex Novak","Abdala Espinosa","Bartlomiej W. Papiez","Susanne Gaube","Thomas Lukasiewicz","Oana-Maria Camburu"],"pdf_url":"https://arxiv.org/pdf/2410.12284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12278v1","updated":"2024-10-16T06:31:59Z","published":"2024-10-16T06:31:59Z","title":"Controlled Automatic Task-Specific Synthetic Data Generation for\n  Hallucination Detection","summary":"  We present a novel approach to automatically generate non-trivial\ntask-specific synthetic datasets for hallucination detection. Our approach\nfeatures a two-step generation-selection pipeline, using hallucination pattern\nguidance and a language style alignment during generation. Hallucination\npattern guidance leverages the most important task-specific hallucination\npatterns while language style alignment aligns the style of the synthetic\ndataset with benchmark text. To obtain robust supervised detectors from\nsynthetic datasets, we also adopt a data mixture strategy to improve\nperformance robustness and generalization. Our results on three datasets show\nthat our generated hallucination text is more closely aligned with\nnon-hallucinated text versus baselines, to train hallucination detectors with\nbetter generalization. Our hallucination detectors trained on synthetic\ndatasets outperform in-context-learning (ICL)-based detectors by a large margin\nof 32%. Our extensive experiments confirm the benefits of our approach with\ncross-task and cross-generator generalization. Our data-mixture-based training\nfurther improves the generalization and robustness of hallucination detection.\n","authors":["Yong Xie","Karan Aggarwal","Aitzaz Ahmad","Stephen Lau"],"pdf_url":"https://arxiv.org/pdf/2410.12278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19454v2","updated":"2024-10-16T06:31:28Z","published":"2024-09-28T20:40:18Z","title":"See Where You Read with Eye Gaze Tracking and Large Language Model","summary":"  Losing track of reading progress during line switching can be frustrating.\nEye gaze tracking technology offers a potential solution by highlighting read\nparagraphs, aiding users in avoiding wrong line switches. However, the gap\nbetween gaze tracking accuracy (2-3 cm) and text line spacing (3-5 mm) makes\ndirect application impractical. Existing methods leverage the linear reading\npattern but fail during jump reading. This paper presents a reading tracking\nand highlighting system that supports both linear and jump reading. Based on\nexperimental insights from the gaze nature study of 16 users, two gaze error\nmodels are designed to enable both jump reading detection and relocation. The\nsystem further leverages the large language model's contextual perception\ncapability in aiding reading tracking. A reading tracking domain-specific\nline-gaze alignment opportunity is also exploited to enable dynamic and\nfrequent calibration of the gaze results. Controlled experiments demonstrate\nreliable linear reading tracking, as well as 84% accuracy in tracking jump\nreading. Furthermore, real field tests with 18 volunteers demonstrated the\nsystem's effectiveness in tracking and highlighting read paragraphs, improving\nreading efficiency, and enhancing user experience.\n","authors":["Sikai Yang","Gang Yan"],"pdf_url":"https://arxiv.org/pdf/2409.19454v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2406.11548v5","updated":"2024-10-16T06:29:55Z","published":"2024-06-17T13:44:53Z","title":"AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic\n  Manipulation","summary":"  The ability to reflect on and correct failures is crucial for robotic systems\nto interact stably with real-life objects. Observing the generalization and\nreasoning capabilities of Multimodal Large Language Models (MLLMs), previous\napproaches have aimed to utilize these models to enhance robotic systems\naccordingly. However, these methods typically focus on high-level planning\ncorrections using an additional MLLM, with limited utilization of failed\nsamples to correct low-level contact poses which is particularly prone to occur\nduring articulated object manipulation. To address this gap, we propose an\nAutonomous Interactive Correction (AIC) MLLM, which makes use of previous\nlow-level interaction experiences to correct SE(3) pose predictions for\narticulated object. Specifically, AIC MLLM is initially fine-tuned to acquire\nboth pose prediction and feedback prompt comprehension abilities. We design two\ntypes of prompt instructions for interactions with objects: 1) visual masks to\nhighlight unmovable parts for position correction, and 2) textual descriptions\nto indicate potential directions for rotation correction. During inference, a\nFeedback Information Extraction module is introduced to recognize the failure\ncause, allowing AIC MLLM to adaptively correct the pose prediction using the\ncorresponding prompts. To further enhance manipulation stability, we devise a\nTest Time Adaptation strategy that enables AIC MLLM to better adapt to the\ncurrent scene configuration. Finally, extensive experiments are conducted in\nboth simulated and real-world environments to evaluate the proposed method. The\nresults demonstrate that our AIC MLLM can efficiently correct failure samples\nby leveraging interaction experience prompts. Our project website is\nhttps://sites.google.com/view/aic-mllm.\n","authors":["Chuyan Xiong","Chengyu Shen","Xiaoqi Li","Kaichen Zhou","Jiaming Liu","Ruiping Wang","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2406.11548v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02237v2","updated":"2024-10-16T06:28:58Z","published":"2024-10-03T06:16:50Z","title":"Key-Grid: Unsupervised 3D Keypoints Detection using Grid Heatmap\n  Features","summary":"  Detecting 3D keypoints with semantic consistency is widely used in many\nscenarios such as pose estimation, shape registration and robotics. Currently,\nmost unsupervised 3D keypoint detection methods focus on the rigid-body\nobjects. However, when faced with deformable objects, the keypoints they\nidentify do not preserve semantic consistency well. In this paper, we introduce\nan innovative unsupervised keypoint detector Key-Grid for both the rigid-body\nand deformable objects, which is an autoencoder framework. The encoder predicts\nkeypoints and the decoder utilizes the generated keypoints to reconstruct the\nobjects. Unlike previous work, we leverage the identified keypoint in formation\nto form a 3D grid feature heatmap called grid heatmap, which is used in the\ndecoder section. Grid heatmap is a novel concept that represents the latent\nvariables for grid points sampled uniformly in the 3D cubic space, where these\nvariables are the shortest distance between the grid points and the skeleton\nconnected by keypoint pairs. Meanwhile, we incorporate the information from\neach layer of the encoder into the decoder section. We conduct an extensive\nevaluation of Key-Grid on a list of benchmark datasets. Key-Grid achieves the\nstate-of-the-art performance on the semantic consistency and position accuracy\nof keypoints. Moreover, we demonstrate the robustness of Key-Grid to noise and\ndownsampling. In addition, we achieve SE-(3) invariance of keypoints though\ngeneralizing Key-Grid to a SE(3)-invariant backbone.\n","authors":["Chengkai Hou","Zhengrong Xue","Bingyang Zhou","Jinghan Ke","Lin Shao","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2410.02237v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12274v1","updated":"2024-10-16T06:28:49Z","published":"2024-10-16T06:28:49Z","title":"Fusion from Decomposition: A Self-Supervised Approach for Image Fusion\n  and Beyond","summary":"  Image fusion is famous as an alternative solution to generate one\nhigh-quality image from multiple images in addition to image restoration from a\nsingle degraded image. The essence of image fusion is to integrate\ncomplementary information from source images. Existing fusion methods struggle\nwith generalization across various tasks and often require labor-intensive\ndesigns, in which it is difficult to identify and extract useful information\nfrom source images due to the diverse requirements of each fusion task.\nAdditionally, these methods develop highly specialized features for different\ndownstream applications, hindering the adaptation to new and diverse downstream\ntasks. To address these limitations, we introduce DeFusion++, a novel framework\nthat leverages self-supervised learning (SSL) to enhance the versatility of\nfeature representation for different image fusion tasks. DeFusion++ captures\nthe image fusion task-friendly representations from large-scale data in a\nself-supervised way, overcoming the constraints of limited fusion datasets.\nSpecifically, we introduce two innovative pretext tasks: common and unique\ndecomposition (CUD) and masked feature modeling (MFM). CUD decomposes source\nimages into abstract common and unique components, while MFM refines these\ncomponents into robust fused features. Jointly training of these tasks enables\nDeFusion++ to produce adaptable representations that can effectively extract\nuseful information from various source images, regardless of the fusion task.\nThe resulting fused representations are also highly adaptable for a wide range\nof downstream tasks, including image segmentation and object detection.\nDeFusion++ stands out by producing versatile fused representations that can\nenhance both the quality of image fusion and the effectiveness of downstream\nhigh-level vision tasks, simplifying the process with the elegant fusion\nframework.\n","authors":["Pengwei Liang","Junjun Jiang","Qing Ma","Xianming Liu","Jiayi Ma"],"pdf_url":"https://arxiv.org/pdf/2410.12274v1.pdf","comment":"18page"},{"id":"http://arxiv.org/abs/2407.12508v2","updated":"2024-10-16T06:25:50Z","published":"2024-07-17T11:45:02Z","title":"MERLIN: Multimodal Embedding Refinement via LLM-based Iterative\n  Navigation for Text-Video Retrieval-Rerank Pipeline","summary":"  The rapid expansion of multimedia content has made accurately retrieving\nrelevant videos from large collections increasingly challenging. Recent\nadvancements in text-video retrieval have focused on cross-modal interactions,\nlarge-scale foundation model training, and probabilistic modeling, yet often\nneglect the crucial user perspective, leading to discrepancies between user\nqueries and the content retrieved. To address this, we introduce MERLIN\n(Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel,\ntraining-free pipeline that leverages Large Language Models (LLMs) for\niterative feedback learning. MERLIN refines query embeddings from a user\nperspective, enhancing alignment between queries and video content through a\ndynamic question answering process. Experimental results on datasets like\nMSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improves\nRecall@1, outperforming existing systems and confirming the benefits of\nintegrating LLMs into multimodal retrieval systems for more responsive and\ncontext-aware multimedia retrieval.\n","authors":["Donghoon Han","Eunhwan Park","Gisang Lee","Adam Lee","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2407.12508v2.pdf","comment":"EMNLP 2024 Industry Track Accepted (Camera-Ready Version)"},{"id":"http://arxiv.org/abs/2410.12270v1","updated":"2024-10-16T06:14:15Z","published":"2024-10-16T06:14:15Z","title":"DaDiff: Domain-aware Diffusion Model for Nighttime UAV Tracking","summary":"  Domain adaptation is an inspiring solution to the misalignment issue of\nday/night image features for nighttime UAV tracking. However, the one-step\nadaptation paradigm is inadequate in addressing the prevalent difficulties\nposed by low-resolution (LR) objects when viewed from the UAVs at night, owing\nto the blurry edge contour and limited detail information. Moreover, these\napproaches struggle to perceive LR objects disturbed by nighttime noise. To\naddress these challenges, this work proposes a novel progressive alignment\nparadigm, named domain-aware diffusion model (DaDiff), aligning nighttime LR\nobject features to the daytime by virtue of progressive and stable generations.\nThe proposed DaDiff includes an alignment encoder to enhance the detail\ninformation of nighttime LR objects, a tracking-oriented layer designed to\nachieve close collaboration with tracking tasks, and a successive distribution\ndiscriminator presented to distinguish different feature distributions at each\ndiffusion timestep successively. Furthermore, an elaborate nighttime UAV\ntracking benchmark is constructed for LR objects, namely NUT-LR, consisting of\n100 annotated sequences. Exhaustive experiments have demonstrated the\nrobustness and feature alignment ability of the proposed DaDiff. The source\ncode and video demo are available at https://github.com/vision4robotics/DaDiff.\n","authors":["Haobo Zuo","Changhong Fu","Guangze Zheng","Liangliang Yao","Kunhan Lu","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2410.12270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19554v2","updated":"2024-10-16T06:12:53Z","published":"2024-09-29T04:43:10Z","title":"Tri-Cam: Practical Eye Gaze Tracking via Camera Network","summary":"  As human eyes serve as conduits of rich information, unveiling emotions,\nintentions, and even aspects of an individual's health and overall well-being,\ngaze tracking also enables various human-computer interaction applications, as\nwell as insights in psychological and medical research. However, existing gaze\ntracking solutions fall short at handling free user movement, and also require\nlaborious user effort in system calibration. We introduce Tri-Cam, a practical\ndeep learning-based gaze tracking system using three affordable RGB webcams. It\nfeatures a split network structure for efficient training, as well as\ndesignated network designs to handle the separated gaze tracking tasks. Tri-Cam\nis also equipped with an implicit calibration module, which makes use of mouse\nclick opportunities to reduce calibration overhead on the user's end. We\nevaluate Tri-Cam against Tobii, the state-of-the-art commercial eye tracker,\nachieving comparable accuracy, while supporting a wider free movement area. In\nconclusion, Tri-Cam provides a user-friendly, affordable, and robust gaze\ntracking solution that could practically enable various applications.\n","authors":["Sikai Yang"],"pdf_url":"https://arxiv.org/pdf/2409.19554v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2410.12269v1","updated":"2024-10-16T06:09:27Z","published":"2024-10-16T06:09:27Z","title":"LoD-Loc: Aerial Visual Localization using LoD 3D Map with Neural\n  Wireframe Alignment","summary":"  We propose a new method named LoD-Loc for visual localization in the air.\nUnlike existing localization algorithms, LoD-Loc does not rely on complex 3D\nrepresentations and can estimate the pose of an Unmanned Aerial Vehicle (UAV)\nusing a Level-of-Detail (LoD) 3D map. LoD-Loc mainly achieves this goal by\naligning the wireframe derived from the LoD projected model with that predicted\nby the neural network. Specifically, given a coarse pose provided by the UAV\nsensor, LoD-Loc hierarchically builds a cost volume for uniformly sampled pose\nhypotheses to describe pose probability distribution and select a pose with\nmaximum probability. Each cost within this volume measures the degree of line\nalignment between projected and predicted wireframes. LoD-Loc also devises a\n6-DoF pose optimization algorithm to refine the previous result with a\ndifferentiable Gaussian-Newton method. As no public dataset exists for the\nstudied problem, we collect two datasets with map levels of LoD3.0 and LoD2.0,\nalong with real RGB queries and ground-truth pose annotations. We benchmark our\nmethod and demonstrate that LoD-Loc achieves excellent performance, even\nsurpassing current state-of-the-art methods that use textured 3D models for\nlocalization. The code and dataset are available at\nhttps://victorzoo.github.io/LoD-Loc.github.io/.\n","authors":["Juelin Zhu","Shen Yan","Long Wang","Shengyue Zhang","Yu Liu","Maojun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12269v1.pdf","comment":"Accepted by NeurIPS 2024; for Project page, see\n  https://victorzoo.github.io/LoD-Loc.github.io/"},{"id":"http://arxiv.org/abs/2408.10894v3","updated":"2024-10-16T06:00:24Z","published":"2024-08-20T14:27:03Z","title":"ViLReF: An Expert Knowledge Enabled Vision-Language Retinal Foundation\n  Model","summary":"  Subtle semantic differences in retinal image and text data present great\nchallenges for pre-training visual-language models. Moreover, false negative\nsamples, i.e., image-text pairs having the same semantics but incorrectly\nregarded as negatives, disrupt the visual-language pre-training process and\naffect the model's learning ability. This work aims to develop a retinal\nfoundation model, called ViLReF, by pre-training on a paired dataset comprising\n451,956 retinal images and corresponding diagnostic text reports. In our\nvision-language pre-training strategy, we leverage expert knowledge to\nfacilitate the extraction of labels and propose a novel constraint, the\nWeighted Similarity Coupling Loss, to adjust the speed of pushing sample pairs\nfurther apart dynamically within the feature space. Furthermore, we employ a\nbatch expansion module with dynamic memory queues, maintained by momentum\nencoders, to supply extra samples and compensate for the vacancies caused by\neliminating false negatives. Extensive experiments are conducted on multiple\ndatasets for downstream classification and segmentation tasks. The experimental\nresults demonstrate the powerful zero-shot and transfer learning capabilities\nof ViLReF, verifying the effectiveness of our pre-training strategy. Our ViLReF\nmodel is available at: https://github.com/T6Yang/ViLReF.\n","authors":["Shengzhu Yang","Jiawei Du","Jia Guo","Weihang Zhang","Hanruo Liu","Huiqi Li","Ningli Wang"],"pdf_url":"https://arxiv.org/pdf/2408.10894v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12259v1","updated":"2024-10-16T05:58:08Z","published":"2024-10-16T05:58:08Z","title":"Optimizing YOLOv5s Object Detection through Knowledge Distillation\n  algorithm","summary":"  This paper explores the application of knowledge distillation technology in\ntarget detection tasks, especially the impact of different distillation\ntemperatures on the performance of student models. By using YOLOv5l as the\nteacher network and a smaller YOLOv5s as the student network, we found that\nwith the increase of distillation temperature, the student's detection accuracy\ngradually improved, and finally achieved mAP50 and mAP50-95 indicators that\nwere better than the original YOLOv5s model at a specific temperature.\nExperimental results show that appropriate knowledge distillation strategies\ncan not only improve the accuracy of the model but also help improve the\nreliability and stability of the model in practical applications. This paper\nalso records in detail the accuracy curve and loss function descent curve\nduring the model training process and shows that the model converges to a\nstable state after 150 training cycles. These findings provide a theoretical\nbasis and technical reference for further optimizing target detection\nalgorithms.\n","authors":["Guanming Huang","Aoran Shen","Yuxiang Hu","Junliang Du","Jiacheng Hu","Yingbin Liang"],"pdf_url":"https://arxiv.org/pdf/2410.12259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12245v1","updated":"2024-10-16T05:16:14Z","published":"2024-10-16T05:16:14Z","title":"Advancing Healthcare: Innovative ML Approaches for Improved Medical\n  Imaging in Data-Constrained Environments","summary":"  Healthcare industries face challenges when experiencing rare diseases due to\nlimited samples. Artificial Intelligence (AI) communities overcome this\nsituation to create synthetic data which is an ethical and privacy issue in the\nmedical domain. This research introduces the CAT-U-Net framework as a new\napproach to overcome these limitations, which enhances feature extraction from\nmedical images without the need for large datasets. The proposed framework adds\nan extra concatenation layer with downsampling parts, thereby improving its\nability to learn from limited data while maintaining patient privacy. To\nvalidate, the proposed framework's robustness, different medical conditioning\ndatasets were utilized including COVID-19, brain tumors, and wrist fractures.\nThe framework achieved nearly 98% reconstruction accuracy, with a Dice\ncoefficient close to 0.946. The proposed CAT-U-Net has the potential to make a\nbig difference in medical image diagnostics in settings with limited data.\n","authors":["Al Amin","Kamrul Hasan","Saleh Zein-Sabatto","Liang Hong","Sachin Shetty","Imtiaz Ahmed","Tariqul Islam"],"pdf_url":"https://arxiv.org/pdf/2410.12245v1.pdf","comment":"7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.07176v2","updated":"2024-10-16T05:11:30Z","published":"2024-07-09T18:42:41Z","title":"Scaling Up Personalized Image Aesthetic Assessment via Task Vector\n  Customization","summary":"  The task of personalized image aesthetic assessment seeks to tailor aesthetic\nscore prediction models to match individual preferences with just a few\nuser-provided inputs. However, the scalability and generalization capabilities\nof current approaches are considerably restricted by their reliance on an\nexpensive curated database. To overcome this long-standing scalability\nchallenge, we present a unique approach that leverages readily available\ndatabases for general image aesthetic assessment and image quality assessment.\nSpecifically, we view each database as a distinct image score regression task\nthat exhibits varying degrees of personalization potential. By determining\noptimal combinations of task vectors, known to represent specific traits of\neach database, we successfully create personalized models for individuals. This\napproach of integrating multiple models allows us to harness a substantial\namount of data. Our extensive experiments demonstrate the effectiveness of our\napproach in generalizing to previously unseen domains-a challenge previous\napproaches have struggled to achieve-making it highly applicable to real-world\nscenarios. Our novel approach significantly advances the field by offering\nscalable solutions for personalized aesthetic assessment and establishing high\nstandards for future research.\nhttps://yeolj00.github.io/personal-projects/personalized-aesthetics/\n","authors":["Jooyeol Yun","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2407.07176v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2410.12242v1","updated":"2024-10-16T05:08:00Z","published":"2024-10-16T05:08:00Z","title":"EG-HumanNeRF: Efficient Generalizable Human NeRF Utilizing Human Prior\n  for Sparse View","summary":"  Generalizable neural radiance field (NeRF) enables neural-based digital human\nrendering without per-scene retraining. When combined with human prior\nknowledge, high-quality human rendering can be achieved even with sparse input\nviews. However, the inference of these methods is still slow, as a large number\nof neural network queries on each ray are required to ensure the rendering\nquality. Moreover, occluded regions often suffer from artifacts, especially\nwhen the input views are sparse. To address these issues, we propose a\ngeneralizable human NeRF framework that achieves high-quality and real-time\nrendering with sparse input views by extensively leveraging human prior\nknowledge. We accelerate the rendering with a two-stage sampling reduction\nstrategy: first constructing boundary meshes around the human geometry to\nreduce the number of ray samples for sampling guidance regression, and then\nvolume rendering using fewer guided samples. To improve rendering quality,\nespecially in occluded regions, we propose an occlusion-aware attention\nmechanism to extract occlusion information from the human priors, followed by\nan image space refinement network to improve rendering quality. Furthermore,\nfor volume rendering, we adopt a signed ray distance function (SRDF)\nformulation, which allows us to propose an SRDF loss at every sample position\nto improve the rendering quality further. Our experiments demonstrate that our\nmethod outperforms the state-of-the-art methods in rendering quality and has a\ncompetitive rendering speed compared with speed-prioritized novel view\nsynthesis methods.\n","authors":["Zhaorong Wang","Yoshihiro Kanamori","Yuki Endo"],"pdf_url":"https://arxiv.org/pdf/2410.12242v1.pdf","comment":"project page: https://github.com/LarsPh/EG-HumanNeRF"},{"id":"http://arxiv.org/abs/2410.12240v1","updated":"2024-10-16T05:00:51Z","published":"2024-10-16T05:00:51Z","title":"Leveraging Spatial Attention and Edge Context for Optimized Feature\n  Selection in Visual Localization","summary":"  Visual localization determines an agent's precise position and orientation\nwithin an environment using visual data. It has become a critical task in the\nfield of robotics, particularly in applications such as autonomous navigation.\nThis is due to the ability to determine an agent's pose using cost-effective\nsensors such as RGB cameras. Recent methods in visual localization employ scene\ncoordinate regression to determine the agent's pose. However, these methods\nface challenges as they attempt to regress 2D-3D correspondences across the\nentire image region, despite not all regions providing useful information. To\naddress this issue, we introduce an attention network that selectively targets\ninformative regions of the image. Using this network, we identify the\nhighest-scoring features to improve the feature selection process and combine\nthe result with edge detection. This integration ensures that the features\nchosen for the training buffer are located within robust regions, thereby\nimproving 2D-3D correspondence and overall localization performance. Our\napproach was tested on the outdoor benchmark dataset, demonstrating superior\nresults compared to previous methods.\n","authors":["Nanda Febri Istighfarin","HyungGi Jo"],"pdf_url":"https://arxiv.org/pdf/2410.12240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00769v3","updated":"2024-10-16T04:53:55Z","published":"2024-02-01T16:58:11Z","title":"AnimateLCM: Computation-Efficient Personalized Style Video Generation\n  without Personalized Video Data","summary":"  This paper introduces an effective method for computation-efficient\npersonalized style video generation without requiring access to any\npersonalized video data. It reduces the necessary generation time of similarly\nsized video diffusion models from 25 seconds to around 1 second while\nmaintaining the same level of performance. The method's effectiveness lies in\nits dual-level decoupling learning approach: 1) separating the learning of\nvideo style from video generation acceleration, which allows for personalized\nstyle video generation without any personalized style video data, and 2)\nseparating the acceleration of image generation from the acceleration of video\nmotion generation, enhancing training efficiency and mitigating the negative\neffects of low-quality video data.\n","authors":["Fu-Yun Wang","Zhaoyang Huang","Weikang Bian","Xiaoyu Shi","Keqiang Sun","Guanglu Song","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2402.00769v3.pdf","comment":"Accepted as a Short Paper by SIGGRAPH ASIA 2024 Technical\n  Communications. This is a short version of the original work. Project Page:\n  https://animatelcm.github.io/"},{"id":"http://arxiv.org/abs/2410.12225v1","updated":"2024-10-16T04:42:10Z","published":"2024-10-16T04:42:10Z","title":"Evaluating Cascaded Methods of Vision-Language Models for Zero-Shot\n  Detection and Association of Hardhats for Increased Construction Safety","summary":"  This paper evaluates the use of vision-language models (VLMs) for zero-shot\ndetection and association of hardhats to enhance construction safety. Given the\nsignificant risk of head injuries in construction, proper enforcement of\nhardhat use is critical. We investigate the applicability of foundation models,\nspecifically OWLv2, for detecting hardhats in real-world construction site\nimages. Our contributions include the creation of a new benchmark dataset,\nHardhat Safety Detection Dataset, by filtering and combining existing datasets\nand the development of a cascaded detection approach. Experimental results on\n5,210 images demonstrate that the OWLv2 model achieves an average precision of\n0.6493 for hardhat detection. We further analyze the limitations and potential\nimprovements for real-world applications, highlighting the strengths and\nweaknesses of current foundation models in safety perception domains.\n","authors":["Lucas Choi","Ross Greer"],"pdf_url":"https://arxiv.org/pdf/2410.12225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12214v1","updated":"2024-10-16T04:19:28Z","published":"2024-10-16T04:19:28Z","title":"Order-Aware Interactive Segmentation","summary":"  Interactive segmentation aims to accurately segment target objects with\nminimal user interactions. However, current methods often fail to accurately\nseparate target objects from the background, due to a limited understanding of\norder, the relative depth between objects in a scene. To address this issue, we\npropose OIS: order-aware interactive segmentation, where we explicitly encode\nthe relative depth between objects into order maps. We introduce a novel\norder-aware attention, where the order maps seamlessly guide the user\ninteractions (in the form of clicks) to attend to the image features. We\nfurther present an object-aware attention module to incorporate a strong\nobject-level understanding to better differentiate objects with similar order.\nOur approach allows both dense and sparse integration of user clicks, enhancing\nboth accuracy and efficiency as compared to prior works. Experimental results\ndemonstrate that OIS achieves state-of-the-art performance, improving mIoU\nafter one click by 7.61 on the HQSeg44K dataset and 1.32 on the DAVIS dataset\nas compared to the previous state-of-the-art SegNext, while also doubling\ninference speed compared to current leading methods. The project page is\nhttps://ukaukaaaa.github.io/projects/OIS/index.html\n","authors":["Bin Wang","Anwesa Choudhuri","Meng Zheng","Zhongpai Gao","Benjamin Planche","Andong Deng","Qin Liu","Terrence Chen","Ulas Bagci","Ziyan Wu"],"pdf_url":"https://arxiv.org/pdf/2410.12214v1.pdf","comment":"Interactive demo can be found in project page:\n  https://ukaukaaaa.github.io/projects/OIS/index.html"},{"id":"http://arxiv.org/abs/2407.16874v2","updated":"2024-10-16T04:18:45Z","published":"2024-07-23T22:46:21Z","title":"Vision-Based Adaptive Robotics for Autonomous Surface Crack Repair","summary":"  Surface cracks in infrastructure can lead to significant deterioration and\ncostly maintenance if not efficiently repaired. Manual repair methods are\nlabor-intensive, time-consuming, and imprecise and thus difficult to scale to\nlarge areas. While advancements in robotic perception and manipulation have\nprogressed autonomous crack repair, existing methods still face three key\nchallenges: accurate localization of cracks within the robot's coordinate\nframe, (ii) adaptability to varying crack depths and widths, and (iii)\nvalidation of the repair process under realistic conditions. This paper\npresents an adaptive, autonomous system for surface crack detection and repair\nusing robotics with advanced sensing technologies to enhance precision and\nsafety for humans. The system uses an RGB-D camera for crack detection, a laser\nscanner for precise measurement, and an extruder and pump for material\ndeposition. To address one of the key challenges, the laser scanner is used to\nenhance the crack coordinates for accurate localization. Furthermore, our\napproach demonstrates that an adaptive crack-filling method is more efficient\nand effective than a fixed-speed approach, with experimental results confirming\nboth precision and consistency. In addition, to ensure real-world applicability\nand testing repeatability, we introduce a novel validation procedure using\n3D-printed crack specimens that accurately simulate real-world conditions. This\nresearch contributes to the evolving field of human-robot interaction in\nconstruction by demonstrating how adaptive robotic systems can reduce the need\nfor manual labor, improve safety, and enhance the efficiency of maintenance\noperations, ultimately paving the way for more sophisticated and integrated\nconstruction robotics.\n","authors":["Joshua Genova","Eric Cabrera","Vedhus Hoskere"],"pdf_url":"https://arxiv.org/pdf/2407.16874v2.pdf","comment":"22 pages, 14 figures, submitted to Advanced Engineering Informatics"},{"id":"http://arxiv.org/abs/2409.02529v3","updated":"2024-10-16T04:08:57Z","published":"2024-09-04T08:42:42Z","title":"Sample what you cant compress","summary":"  For learned image representations, basic autoencoders often produce blurry\nresults. Reconstruction quality can be improved by incorporating additional\npenalties such as adversarial (GAN) and perceptual losses. Arguably, these\napproaches lack a principled interpretation. Concurrently, in generative\nsettings diffusion has demonstrated a remarkable ability to create crisp, high\nquality results and has solid theoretical underpinnings (from variational\ninference to direct study as the Fisher Divergence). Our work combines\nautoencoder representation learning with diffusion and is, to our knowledge,\nthe first to demonstrate the efficacy of jointly learning a continuous encoder\nand decoder under a diffusion-based loss. We demonstrate that this approach\nyields better reconstruction quality as compared to GAN-based autoencoders\nwhile being easier to tune. We also show that the resulting representation is\neasier to model with a latent diffusion model as compared to the representation\nobtained from a state-of-the-art GAN-based loss. Since our decoder is\nstochastic, it can generate details not encoded in the otherwise deterministic\nlatent representation; we therefore name our approach \"Sample what you can't\ncompress\", or SWYCC for short.\n","authors":["Vighnesh Birodkar","Gabriel Barcik","James Lyon","Sergey Ioffe","David Minnen","Joshua V. Dillon"],"pdf_url":"https://arxiv.org/pdf/2409.02529v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10122v2","updated":"2024-10-16T04:04:01Z","published":"2024-10-14T03:22:26Z","title":"MuseTalk: Real-Time High Quality Lip Synchronization with Latent Space\n  Inpainting","summary":"  Achieving high-resolution, identity consistency, and accurate lip-speech\nsynchronization in face visual dubbing presents significant challenges,\nparticularly for real-time applications like live video streaming. We propose\nMuseTalk, which generates lip-sync targets in a latent space encoded by a\nVariational Autoencoder, enabling high-fidelity talking face video generation\nwith efficient inference. Specifically, we project the occluded lower half of\nthe face image and itself as an reference into a low-dimensional latent space\nand use a multi-scale U-Net to fuse audio and visual features at various\nlevels. We further propose a novel sampling strategy during training, which\nselects reference images with head poses closely matching the target, allowing\nthe model to focus on precise lip movement by filtering out redundant\ninformation. Additionally, we analyze the mechanism of lip-sync loss and reveal\nits relationship with input information volume. Extensive experiments show that\nMuseTalk consistently outperforms recent state-of-the-art methods in visual\nfidelity and achieves comparable lip-sync accuracy. As MuseTalk supports the\nonline generation of face at 256x256 at more than 30 FPS with negligible\nstarting latency, it paves the way for real-time applications.\n","authors":["Yue Zhang","Minhao Liu","Zhaokang Chen","Bin Wu","Yubin Zeng","Chao Zhan","Yingjie He","Junxin Huang","Wenjiang Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.10122v2.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.07669v2","updated":"2024-10-16T03:45:50Z","published":"2024-10-10T07:29:24Z","title":"Delta-ICM: Entropy Modeling with Delta Function for Learned Image\n  Compression","summary":"  Image Coding for Machines (ICM) is becoming more important as research in\ncomputer vision progresses. ICM is a vital research field that pursues the use\nof images for image recognition models, facilitating efficient image\ntransmission and storage. The demand for recognition models is growing rapidly\namong the general public, and their performance continues to improve. To meet\nthese needs, exchanging image data between consumer devices and cloud AI using\nICM technology could be one possible solution. In ICM, various image\ncompression methods have adopted Learned Image Compression (LIC). LIC includes\nan entropy model for estimating the bitrate of latent features, and the design\nof this model significantly affects its performance. Typically, LIC methods\nassume that the distribution of latent features follows a normal distribution.\nThis assumption is effective for compressing images intended for human vision.\nHowever, employing an entropy model based on normal distribution is inefficient\nin ICM due to the limitation of image parts that require precise decoding. To\naddress this, we propose Delta-ICM, which uses a probability distribution based\non a delta function. Assuming the delta distribution as a distribution of\nlatent features reduces the entropy of image portions unnecessary for machines.\nWe compress the remaining portions using an entropy model based on normal\ndistribution, similar to existing methods. Delta-ICM selects between the\nentropy model based on the delta distribution and the one based on the normal\ndistribution for each latent feature. Our method outperforms existing ICM\nmethods in image compression performance aimed at machines.\n","authors":["Takahiro Shindo","Taiju Watanabe","Yui Tatsumi","Hiroshi Watanabe"],"pdf_url":"https://arxiv.org/pdf/2410.07669v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07464v2","updated":"2024-10-16T03:44:41Z","published":"2024-07-10T08:40:39Z","title":"Video-to-Audio Generation with Hidden Alignment","summary":"  Generating semantically and temporally aligned audio content in accordance\nwith video input has become a focal point for researchers, particularly\nfollowing the remarkable breakthrough in text-to-video generation. In this\nwork, we aim to offer insights into the video-to-audio generation paradigm,\nfocusing on three crucial aspects: vision encoders, auxiliary embeddings, and\ndata augmentation techniques. Beginning with a foundational model built on a\nsimple yet surprisingly effective intuition, we explore various vision encoders\nand auxiliary embeddings through ablation studies. Employing a comprehensive\nevaluation pipeline that emphasizes generation quality and video-audio\nsynchronization alignment, we demonstrate that our model exhibits\nstate-of-the-art video-to-audio generation capabilities. Furthermore, we\nprovide critical insights into the impact of different data augmentation\nmethods on enhancing the generation framework's overall capacity. We showcase\npossibilities to advance the challenge of generating synchronized audio from\nsemantic and temporal perspectives. We hope these insights will serve as a\nstepping stone toward developing more realistic and accurate audio-visual\ngeneration models.\n","authors":["Manjie Xu","Chenxing Li","Xinyi Tu","Yong Ren","Rilin Chen","Yu Gu","Wei Liang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2407.07464v2.pdf","comment":"https://sites.google.com/view/vta-ldm"},{"id":"http://arxiv.org/abs/2405.17969v2","updated":"2024-10-16T03:35:22Z","published":"2024-05-28T08:56:33Z","title":"Knowledge Circuits in Pretrained Transformers","summary":"  The remarkable capabilities of modern large language models are rooted in\ntheir vast repositories of knowledge encoded within their parameters, enabling\nthem to perceive the world and engage in reasoning. The inner workings of how\nthese models store knowledge have long been a subject of intense interest and\ninvestigation among researchers. To date, most studies have concentrated on\nisolated components within these models, such as the Multilayer Perceptrons and\nattention head. In this paper, we delve into the computation graph of the\nlanguage model to uncover the knowledge circuits that are instrumental in\narticulating specific knowledge. The experiments, conducted with GPT2 and\nTinyLLAMA, have allowed us to observe how certain information heads, relation\nheads, and Multilayer Perceptrons collaboratively encode knowledge within the\nmodel. Moreover, we evaluate the impact of current knowledge editing techniques\non these knowledge circuits, providing deeper insights into the functioning and\nconstraints of these editing methodologies. Finally, we utilize knowledge\ncircuits to analyze and interpret language model behaviors such as\nhallucinations and in-context learning. We believe the knowledge circuits hold\npotential for advancing our understanding of Transformers and guiding the\nimproved design of knowledge editing. Code and data are available in\nhttps://github.com/zjunlp/KnowledgeCircuits.\n","authors":["Yunzhi Yao","Ningyu Zhang","Zekun Xi","Mengru Wang","Ziwen Xu","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.17969v2.pdf","comment":"NeurIPS 2024, 32 pages"},{"id":"http://arxiv.org/abs/2410.12195v1","updated":"2024-10-16T03:33:40Z","published":"2024-10-16T03:33:40Z","title":"Sparse Prototype Network for Explainable Pedestrian Behavior Prediction","summary":"  Predicting pedestrian behavior is challenging yet crucial for applications\nsuch as autonomous driving and smart city. Recent deep learning models have\nachieved remarkable performance in making accurate predictions, but they fail\nto provide explanations of their inner workings. One reason for this problem is\nthe multi-modal inputs. To bridge this gap, we present Sparse Prototype Network\n(SPN), an explainable method designed to simultaneously predict a pedestrian's\nfuture action, trajectory, and pose. SPN leverages an intermediate prototype\nbottleneck layer to provide sample-based explanations for its predictions. The\nprototypes are modality-independent, meaning that they can correspond to any\nmodality from the input. Therefore, SPN can extend to arbitrary combinations of\nmodalities. Regularized by mono-semanticity and clustering constraints, the\nprototypes learn consistent and human-understandable features and achieve\nstate-of-the-art performance on action, trajectory and pose prediction on TITAN\nand PIE. Finally, we propose a metric named Top-K Mono-semanticity Scale to\nquantitatively evaluate the explainability. Qualitative results show the\npositive correlation between sparsity and explainability. Code available at\nhttps://github.com/Equinoxxxxx/SPN.\n","authors":["Yan Feng","Alexander Carballo","Kazuya Takeda"],"pdf_url":"https://arxiv.org/pdf/2410.12195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12191v1","updated":"2024-10-16T03:25:16Z","published":"2024-10-16T03:25:16Z","title":"Test-time adaptation for image compression with distribution\n  regularization","summary":"  Current test- or compression-time adaptation image compression (TTA-IC)\napproaches, which leverage both latent and decoder refinements as a two-step\nadaptation scheme, have potentially enhanced the rate-distortion (R-D)\nperformance of learned image compression models on cross-domain compression\ntasks, \\textit{e.g.,} from natural to screen content images. However, compared\nwith the emergence of various decoder refinement variants, the latent\nrefinement, as an inseparable ingredient, is barely tailored to cross-domain\nscenarios. To this end, we aim to develop an advanced latent refinement method\nby extending the effective hybrid latent refinement (HLR) method, which is\ndesigned for \\textit{in-domain} inference improvement but shows noticeable\ndegradation of the rate cost in \\textit{cross-domain} tasks. Specifically, we\nfirst provide theoretical analyses, in a cue of marginalization approximation\nfrom in- to cross-domain scenarios, to uncover that the vanilla HLR suffers\nfrom an underlying mismatch between refined Gaussian conditional and hyperprior\ndistributions, leading to deteriorated joint probability approximation of\nmarginal distribution with increased rate consumption. To remedy this issue, we\nintroduce a simple Bayesian approximation-endowed \\textit{distribution\nregularization} to encourage learning a better joint probability approximation\nin a plug-and-play manner. Extensive experiments on six in- and cross-domain\ndatasets demonstrate that our proposed method not only improves the R-D\nperformance compared with other latent refinement counterparts, but also can be\nflexibly integrated into existing TTA-IC methods with incremental benefits.\n","authors":["Kecheng Chen","Pingping Zhang","Tiexin Qin","Shiqi Wang","Hong Yan","Haoliang Li"],"pdf_url":"https://arxiv.org/pdf/2410.12191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11211v2","updated":"2024-10-16T03:03:35Z","published":"2024-10-15T02:55:07Z","title":"CVCP-Fusion: On Implicit Depth Estimation for 3D Bounding Box Prediction","summary":"  Combining LiDAR and Camera-view data has become a common approach for 3D\nObject Detection. However, previous approaches combine the two input streams at\na point-level, throwing away semantic information derived from camera features.\nIn this paper we propose Cross-View Center Point-Fusion, a state-of-the-art\nmodel to perform 3D object detection by combining camera and LiDAR-derived\nfeatures in the BEV space to preserve semantic density from the camera stream\nwhile incorporating spacial data from the LiDAR stream. Our architecture\nutilizes aspects from previously established algorithms, Cross-View\nTransformers and CenterPoint, and runs their backbones in parallel, allowing\nefficient computation for real-time processing and application. In this paper\nwe find that while an implicitly calculated depth-estimate may be sufficiently\naccurate in a 2D map-view representation, explicitly calculated geometric and\nspacial information is needed for precise bounding box prediction in the 3D\nworld-view space.\n","authors":["Pranav Gupta","Rishabh Rengarajan","Viren Bankapur","Vedansh Mannem","Lakshit Ahuja","Surya Vijay","Kevin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.11211v2.pdf","comment":"7 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:2205.02833 by other authors"},{"id":"http://arxiv.org/abs/2410.12183v1","updated":"2024-10-16T03:01:44Z","published":"2024-10-16T03:01:44Z","title":"TransAgent: Transfer Vision-Language Foundation Models with\n  Heterogeneous Agent Collaboration","summary":"  Vision-language foundation models (such as CLIP) have recently shown their\npower in transfer learning, owing to large-scale image-text pre-training.\nHowever, target domain data in the downstream tasks can be highly different\nfrom the pre-training phase, which makes it hard for such a single model to\ngeneralize well. Alternatively, there exists a wide range of expert models that\ncontain diversified vision and/or language knowledge pre-trained on different\nmodalities, tasks, networks, and datasets. Unfortunately, these models are\n\"isolated agents\" with heterogeneous structures, and how to integrate their\nknowledge for generalizing CLIP-like models has not been fully explored. To\nbridge this gap, we propose a general and concise TransAgent framework, which\ntransports the knowledge of the isolated agents in a unified manner, and\neffectively guides CLIP to generalize with multi-source knowledge distillation.\nWith such a distinct framework, we flexibly collaborate with 11 heterogeneous\nagents to empower vision-language foundation models, without further cost in\nthe inference phase. Finally, our TransAgent achieves state-of-the-art\nperformance on 11 visual recognition datasets. Under the same low-shot setting,\nit outperforms the popular CoOp with around 10% on average, and 20% on EuroSAT\nwhich contains large domain shifts.\n","authors":["Yiwei Guo","Shaobin Zhuang","Kunchang Li","Yu Qiao","Yali Wang"],"pdf_url":"https://arxiv.org/pdf/2410.12183v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.10471v2","updated":"2024-10-16T02:34:40Z","published":"2024-10-14T13:06:04Z","title":"ReLayout: Towards Real-World Document Understanding via Layout-enhanced\n  Pre-training","summary":"  Recent approaches for visually-rich document understanding (VrDU) uses\nmanually annotated semantic groups, where a semantic group encompasses all\nsemantically relevant but not obviously grouped words. As OCR tools are unable\nto automatically identify such grouping, we argue that current VrDU approaches\nare unrealistic. We thus introduce a new variant of the VrDU task, real-world\nvisually-rich document understanding (ReVrDU), that does not allow for using\nmanually annotated semantic groups. We also propose a new method, ReLayout,\ncompliant with the ReVrDU scenario, which learns to capture semantic grouping\nthrough arranging words and bringing the representations of words that belong\nto the potential same semantic group closer together. Our experimental results\ndemonstrate the performance of existing methods is deteriorated with the ReVrDU\ntask, while ReLayout shows superiour performance.\n","authors":["Zhouqiang Jiang","Bowen Wang","Junhao Chen","Yuta Nakashima"],"pdf_url":"https://arxiv.org/pdf/2410.10471v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12165v1","updated":"2024-10-16T02:06:27Z","published":"2024-10-16T02:06:27Z","title":"Dual-Model Distillation for Efficient Action Classification with Hybrid\n  Edge-Cloud Solution","summary":"  As Artificial Intelligence models, such as Large Video-Language models\n(VLMs), grow in size, their deployment in real-world applications becomes\nincreasingly challenging due to hardware limitations and computational costs.\nTo address this, we design a hybrid edge-cloud solution that leverages the\nefficiency of smaller models for local processing while deferring to larger,\nmore accurate cloud-based models when necessary. Specifically, we propose a\nnovel unsupervised data generation method, Dual-Model Distillation (DMD), to\ntrain a lightweight switcher model that can predict when the edge model's\noutput is uncertain and selectively offload inference to the large model in the\ncloud. Experimental results on the action classification task show that our\nframework not only requires less computational overhead, but also improves\naccuracy compared to using a large model alone. Our framework provides a\nscalable and adaptable solution for action classification in\nresource-constrained environments, with potential applications beyond\nhealthcare. Noteworthy, while DMD-generated data is used for optimizing\nperformance and resource usage in our pipeline, we expect the concept of DMD to\nfurther support future research on knowledge alignment across multiple models.\n","authors":["Timothy Wei","Hsien Xin Peng","Elaine Xu","Bryan Zhao","Lei Ding","Diji Yang"],"pdf_url":"https://arxiv.org/pdf/2410.12165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12158v1","updated":"2024-10-16T01:38:59Z","published":"2024-10-16T01:38:59Z","title":"SAM-Guided Masked Token Prediction for 3D Scene Understanding","summary":"  Foundation models have significantly enhanced 2D task performance, and recent\nworks like Bridge3D have successfully applied these models to improve 3D scene\nunderstanding through knowledge distillation, marking considerable\nadvancements. Nonetheless, challenges such as the misalignment between 2D and\n3D representations and the persistent long-tail distribution in 3D datasets\nstill restrict the effectiveness of knowledge distillation from 2D to 3D using\nfoundation models. To tackle these issues, we introduce a novel SAM-guided\ntokenization method that seamlessly aligns 3D transformer structures with\nregion-level knowledge distillation, replacing the traditional KNN-based\ntokenization techniques. Additionally, we implement a group-balanced\nre-weighting strategy to effectively address the long-tail problem in knowledge\ndistillation. Furthermore, inspired by the recent success of masked feature\nprediction, our framework incorporates a two-stage masked token prediction\nprocess in which the student model predicts both the global embeddings and the\ntoken-wise local embeddings derived from the teacher models trained in the\nfirst stage. Our methodology has been validated across multiple datasets,\nincluding SUN RGB-D, ScanNet, and S3DIS, for tasks like 3D object detection and\nsemantic segmentation. The results demonstrate significant improvements over\ncurrent State-of-the-art self-supervised methods, establishing new benchmarks\nin this field.\n","authors":["Zhimin Chen","Liang Yang","Yingwei Li","Longlong Jing","Bing Li"],"pdf_url":"https://arxiv.org/pdf/2410.12158v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.13607v4","updated":"2024-10-16T01:11:04Z","published":"2024-06-19T14:58:49Z","title":"Ultra-High-Definition Image Restoration: New Benchmarks and A Dual\n  Interaction Prior-Driven Solution","summary":"  Ultra-High-Definition (UHD) image restoration has acquired remarkable\nattention due to its practical demand. In this paper, we construct UHD snow and\nrain benchmarks, named UHD-Snow and UHD-Rain, to remedy the deficiency in this\nfield. The UHD-Snow/UHD-Rain is established by simulating the physics process\nof rain/snow into consideration and each benchmark contains 3200 degraded/clear\nimage pairs of 4K resolution. Furthermore, we propose an effective UHD image\nrestoration solution by considering gradient and normal priors in model design\nthanks to these priors' spatial and detail contributions. Specifically, our\nmethod contains two branches: (a) feature fusion and reconstruction branch in\nhigh-resolution space and (b) prior feature interaction branch in\nlow-resolution space. The former learns high-resolution features and fuses\nprior-guided low-resolution features to reconstruct clear images, while the\nlatter utilizes normal and gradient priors to mine useful spatial features and\ndetail features to guide high-resolution recovery better. To better utilize\nthese priors, we introduce single prior feature interaction and dual prior\nfeature interaction, where the former respectively fuses normal and gradient\npriors with high-resolution features to enhance prior ones, while the latter\ncalculates the similarity between enhanced prior ones and further exploits dual\nguided filtering to boost the feature interaction of dual priors. We conduct\nexperiments on both new and existing public datasets and demonstrate the\nstate-of-the-art performance of our method on UHD image low-light enhancement,\ndehazing, deblurring, desonwing, and deraining. The source codes and benchmarks\nare available at \\url{https://github.com/wlydlut/UHDDIP}.\n","authors":["Liyan Wang","Cong Wang","Jinshan Pan","Xiaofeng Liu","Weixiang Zhou","Xiaoran Sun","Wei Wang","Zhixun Su"],"pdf_url":"https://arxiv.org/pdf/2406.13607v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.09946v3","updated":"2024-10-16T01:10:44Z","published":"2023-05-17T04:56:11Z","title":"AdaMSS: Adaptive Multi-Modality Segmentation-to-Survival Learning for\n  Survival Outcome Prediction from PET/CT Images","summary":"  Survival prediction is a major concern for cancer management. Deep survival\nmodels based on deep learning have been widely adopted to perform end-to-end\nsurvival prediction from medical images. Recent deep survival models achieved\npromising performance by jointly performing tumor segmentation with survival\nprediction, where the models were guided to extract tumor-related information\nthrough Multi-Task Learning (MTL). However, these deep survival models have\ndifficulties in exploring out-of-tumor prognostic information. In addition,\nexisting deep survival models are unable to effectively leverage multi-modality\nimages. Empirically-designed fusion strategies were commonly adopted to fuse\nmulti-modality information via task-specific manually-designed networks, thus\nlimiting the adaptability to different scenarios. In this study, we propose an\nAdaptive Multi-modality Segmentation-to-Survival model (AdaMSS) for survival\nprediction from PET/CT images. Instead of adopting MTL, we propose a novel\nSegmentation-to-Survival Learning (SSL) strategy, where our AdaMSS is trained\nfor tumor segmentation and survival prediction sequentially in two stages. This\nstrategy enables the AdaMSS to focus on tumor regions in the first stage and\ngradually expand its focus to include other prognosis-related regions in the\nsecond stage. We also propose a data-driven strategy to fuse multi-modality\ninformation, which realizes adaptive optimization of fusion strategies based on\ntraining data during training. With the SSL and data-driven fusion strategies,\nour AdaMSS is designed as an adaptive model that can self-adapt its focus\nregions and fusion strategy for different training stages. Extensive\nexperiments with two large clinical datasets show that our AdaMSS outperforms\nstate-of-the-art survival prediction methods.\n","authors":["Mingyuan Meng","Bingxin Gu","Michael Fulham","Shaoli Song","Dagan Feng","Lei Bi","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2305.09946v3.pdf","comment":"The extended version of this paper has been published at npj\n  Precision Oncology as \"Adaptive segmentation-to-survival learning for\n  survival prediction from multi-modality medical images\""},{"id":"http://arxiv.org/abs/2410.12143v1","updated":"2024-10-16T01:06:12Z","published":"2024-10-16T01:06:12Z","title":"Unveiling the Limits of Alignment: Multi-modal Dynamic Local Fusion\n  Network and A Benchmark for Unaligned RGBT Video Object Detection","summary":"  Current RGB-Thermal Video Object Detection (RGBT VOD) methods still depend on\nmanually aligning data at the image level, which hampers its practical\napplication in real-world scenarios since image pairs captured by multispectral\nsensors often differ in both fields of view and resolution. To address this\nlimitation, we propose a Multi-modal Dynamic Local fusion Network (MDLNet)\ndesigned to handle unaligned RGBT image pairs. Specifically, our proposed\nMulti-modal Dynamic Local Fusion (MDLF) module includes a set of predefined\nboxes, each enhanced with random Gaussian noise to generate a dynamic box. Each\nbox selects a local region from the original high-resolution RGB image. This\nregion is then fused with the corresponding information from another modality\nand reinserted into the RGB. This method adapts to various data alignment\nscenarios by interacting with local features across different ranges.\nSimultaneously, we introduce a Cascaded Temporal Scrambler (CTS) within an\nend-to-end architecture. This module leverages consistent spatiotemporal\ninformation from consecutive frames to enhance the representation capability of\nthe current frame while maintaining network efficiency. We have curated an open\ndataset called UVT-VOD2024 for unaligned RGBT VOD. It consists of 30,494 pairs\nof unaligned RGBT images captured directly from a multispectral camera. We\nconduct a comprehensive evaluation and comparison with MDLNet and\nstate-of-the-art (SOTA) models, demonstrating the superior effectiveness of\nMDLNet. We will release our code and UVT-VOD2024 to the public for further\nresearch.\n","authors":["Qishun Wang","Zhengzheng Tu","Kunpeng Wang","Le Gu","Chuanwang Guo"],"pdf_url":"https://arxiv.org/pdf/2410.12143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01680v3","updated":"2024-10-16T01:06:11Z","published":"2024-03-04T02:25:41Z","title":"Zero-shot Generalizable Incremental Learning for Vision-Language Object\n  Detection","summary":"  This paper presents Incremental Vision-Language Object Detection (IVLOD), a\nnovel learning task designed to incrementally adapt pre-trained Vision-Language\nObject Detection Models (VLODMs) to various specialized domains, while\nsimultaneously preserving their zero-shot generalization capabilities for the\ngeneralized domain. To address this new challenge, we present the\nZero-interference Reparameterizable Adaptation (ZiRa), a novel method that\nintroduces Zero-interference Loss and reparameterization techniques to tackle\nIVLOD without incurring additional inference costs or a significant increase in\nmemory usage. Comprehensive experiments on COCO and ODinW-13 datasets\ndemonstrate that ZiRa effectively safeguards the zero-shot generalization\nability of VLODMs while continuously adapting to new tasks. Specifically, after\ntraining on ODinW-13 datasets, ZiRa exhibits superior performance compared to\nCL-DETR and iDETR, boosting zero-shot generalizability by substantial 13.91 and\n8.74 AP, respectively.Our code is available at\nhttps://github.com/JarintotionDin/ZiRaGroundingDINO.\n","authors":["Jieren Deng","Haojian Zhang","Kun Ding","Jianhua Hu","Xingxuan Zhang","Yunkuan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.01680v3.pdf","comment":"This paper has been accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2208.04464v3","updated":"2024-10-16T00:47:58Z","published":"2022-08-08T23:25:05Z","title":"In the Eye of Transformer: Global-Local Correlation for Egocentric Gaze\n  Estimation","summary":"  In this paper, we present the first transformer-based model to address the\nchallenging problem of egocentric gaze estimation. We observe that the\nconnection between the global scene context and local visual information is\nvital for localizing the gaze fixation from egocentric video frames. To this\nend, we design the transformer encoder to embed the global context as one\nadditional visual token and further propose a novel Global-Local Correlation\n(GLC) module to explicitly model the correlation of the global token and each\nlocal token. We validate our model on two egocentric video datasets - EGTEA\nGaze+ and Ego4D. Our detailed ablation studies demonstrate the benefits of our\nmethod. In addition, our approach exceeds previous state-of-the-arts by a large\nmargin. We also provide additional visualizations to support our claim that\nglobal-local correlation serves a key representation for predicting gaze\nfixation from egocentric videos. More details can be found in our website\n(https://bolinlai.github.io/GLC-EgoGazeEst).\n","authors":["Bolin Lai","Miao Liu","Fiona Ryan","James M. Rehg"],"pdf_url":"https://arxiv.org/pdf/2208.04464v3.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2409.18124v3","updated":"2024-10-16T00:36:13Z","published":"2024-09-26T17:58:55Z","title":"Lotus: Diffusion-based Visual Foundation Model for High-quality Dense\n  Prediction","summary":"  Leveraging the visual priors of pre-trained text-to-image diffusion models\noffers a promising solution to enhance zero-shot generalization in dense\nprediction tasks. However, existing methods often uncritically use the original\ndiffusion formulation, which may not be optimal due to the fundamental\ndifferences between dense prediction and image generation. In this paper, we\nprovide a systemic analysis of the diffusion formulation for the dense\nprediction, focusing on both quality and efficiency. And we find that the\noriginal parameterization type for image generation, which learns to predict\nnoise, is harmful for dense prediction; the multi-step noising/denoising\ndiffusion process is also unnecessary and challenging to optimize. Based on\nthese insights, we introduce Lotus, a diffusion-based visual foundation model\nwith a simple yet effective adaptation protocol for dense prediction.\nSpecifically, Lotus is trained to directly predict annotations instead of\nnoise, thereby avoiding harmful variance. We also reformulate the diffusion\nprocess into a single-step procedure, simplifying optimization and\nsignificantly boosting inference speed. Additionally, we introduce a novel\ntuning strategy called detail preserver, which achieves more accurate and\nfine-grained predictions. Without scaling up the training data or model\ncapacity, Lotus achieves SoTA performance in zero-shot depth and normal\nestimation across various datasets. It also enhances efficiency, being\nsignificantly faster than most existing diffusion-based methods. Lotus'\nsuperior quality and efficiency also enable a wide range of practical\napplications, such as joint estimation, single/multi-view 3D reconstruction,\netc. Project page: https://lotus3d.github.io/.\n","authors":["Jing He","Haodong Li","Wei Yin","Yixun Liang","Leheng Li","Kaiqiang Zhou","Hongbo Zhang","Bingbing Liu","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2409.18124v3.pdf","comment":"The first two authors contributed equally. Project page:\n  https://lotus3d.github.io/"},{"id":"http://arxiv.org/abs/2402.02263v5","updated":"2024-10-16T00:15:51Z","published":"2024-02-03T21:12:36Z","title":"MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly\n  Mixed Classifiers","summary":"  Adversarial robustness often comes at the cost of degraded accuracy, impeding\nreal-life applications of robust classification models. Training-based\nsolutions for better trade-offs are limited by incompatibilities with\nalready-trained high-performance large models, necessitating the exploration of\ntraining-free ensemble approaches. Observing that robust models are more\nconfident in correct predictions than in incorrect ones on clean and\nadversarial data alike, we speculate amplifying this \"benign confidence\nproperty\" can reconcile accuracy and robustness in an ensemble setting. To\nachieve so, we propose \"MixedNUTS\", a training-free method where the output\nlogits of a robust classifier and a standard non-robust classifier are\nprocessed by nonlinear transformations with only three parameters, which are\noptimized through an efficient algorithm. MixedNUTS then converts the\ntransformed logits into probabilities and mixes them as the overall output. On\nCIFAR-10, CIFAR-100, and ImageNet datasets, experimental results with custom\nstrong adaptive attacks demonstrate MixedNUTS's vastly improved accuracy and\nnear-SOTA robustness -- it boosts CIFAR-100 clean accuracy by 7.86 points,\nsacrificing merely 0.87 points in robust accuracy.\n","authors":["Yatong Bai","Mo Zhou","Vishal M. Patel","Somayeh Sojoudi"],"pdf_url":"https://arxiv.org/pdf/2402.02263v5.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2406.12593v2","updated":"2024-10-16T13:45:54Z","published":"2024-06-18T13:25:18Z","title":"PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval","summary":"  Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)\nfor efficient document retrieval without relying on external indexes. However,\nDSI needs full re-training to handle updates in dynamic corpora, causing\nsignificant computational inefficiencies. We introduce PromptDSI, a\nprompt-based rehearsal-free approach for instance-wise incremental learning\ndocument retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of\nDSI, leveraging its powerful representation to efficiently index new corpora\nwhile maintaining a balance between stability and plasticity. We eliminate the\ninitial forward pass of prompt-based continual learning methods that doubles\ntraining and inference time. Moreover, we propose a topic-aware prompt pool\nthat employs neural topic embeddings as fixed keys. This strategy ensures\ndiverse and effective prompt usage, addressing the challenge of parameter\nunderutilization caused by the collapse of the query-key matching mechanism.\nOur empirical evaluations demonstrate that BERT-based PromptDSI matches IncDSI\nin managing forgetting while improving new corpora performance by more than 4%\nHits@10 on NQ320k and upto 3% MRR@10 on MS MARCO 300k.\n","authors":["Tuan-Luc Huynh","Thuy-Trang Vu","Weiqing Wang","Yinwei Wei","Trung Le","Dragan Gasevic","Yuan-Fang Li","Thanh-Toan Do"],"pdf_url":"https://arxiv.org/pdf/2406.12593v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2406.14162v3","updated":"2024-10-16T13:16:25Z","published":"2024-06-20T10:04:09Z","title":"DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval\n  Augmented Generation","summary":"  Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information when answering queries that need an integrated analysis\nof information (e.g., Tell me good news in the stock market today.)? To address\nthese concerns, RAG developers need to annotate information retrieval (IR) data\nfor their domain of interest, which is challenging because (1) domain-specific\nqueries usually need nuanced definitions of relevance beyond shallow semantic\nrelevance; and (2) human or GPT-4 annotation is costly and cannot cover all\n(query, document) pairs (i.e., annotation selection bias), thus harming the\neffectiveness in evaluating IR recall. To address these challenges, we propose\nDIRAS (Domain-specific Information Retrieval Annotation with Scalability), a\nmanual-annotation-free schema that fine-tunes open-sourced LLMs to consider\nnuanced relevance definition and annotate (partial) relevance labels with\ncalibrated relevance scores. Extensive evaluation shows that DIRAS enables\nsmaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking\nunseen (query, document) pairs, and is helpful for real-world RAG development.\nAll code, LLM generations, and human annotations can be found in\n\\url{https://github.com/EdisonNi-hku/DIRAS}.\n","authors":["Jingwei Ni","Tobias Schimanski","Meihong Lin","Mrinmaya Sachan","Elliott Ash","Markus Leippold"],"pdf_url":"https://arxiv.org/pdf/2406.14162v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19014v3","updated":"2024-10-16T12:55:55Z","published":"2024-09-24T01:40:50Z","title":"FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark","summary":"  Text-to-SQL systems have become crucial for translating natural language into\nSQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, the Execution Accuracy\n(EX), the most prevalent evaluation metric, still shows many false positives\nand negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our metric improves\nagreement with human experts (from 62 to 87.04 in Cohen's kappa) with\ncomprehensive context and sophisticated criteria. Our extensive experiments\nyield several key insights: (1) Models' performance increases by over 2.6\npoints on average, substantially affecting rankings on Spider and BIRD\nbenchmarks; (2) The underestimation of models in EX primarily stems from\nannotation quality issues; and (3) Model performance on particularly\nchallenging questions tends to be overestimated. This work contributes to a\nmore accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field.\n","authors":["Heegyu Kim","Taeyang Jeon","Seunghwan Choi","Seungtaek Choi","Hyunsouk Cho"],"pdf_url":"https://arxiv.org/pdf/2409.19014v3.pdf","comment":"preprint, under review"},{"id":"http://arxiv.org/abs/2410.12519v1","updated":"2024-10-16T12:54:34Z","published":"2024-10-16T12:54:34Z","title":"RosePO: Aligning LLM-based Recommenders with Human Values","summary":"  Recently, there has been a growing interest in leveraging Large Language\nModels (LLMs) for recommendation systems, which usually adapt a pre-trained LLM\nto the recommendation scenario through supervised fine-tuning (SFT). However,\nboth the pre-training and SFT stages fail to explicitly model the comparative\nrelationships of a user's preferences on different items. To construct a\n\"helpful and harmless\" LLM-based recommender, we propose a general framework --\nRecommendation with smoothing personalized Preference Optimization (RosePO),\nwhich better aligns with customized human values during the post-training\nstage. Specifically, in addition to the input and chosen response that\nnaturally align with SFT data, we design a rejected sampling strategy tailored\nfor enhancing helpfulness, along with two strategies aimed at mitigating biases\nto promote harmlessness. To ensure robustness against uncertain labels present\nin automatically constructed preference data, we introduce a personalized\nsmoothing factor predicted by a preference oracle into the optimization\nobjective. Evaluation on three real-world datasets demonstrates the\neffectiveness of our method, showcasing not only improved recommendation\nperformance but also mitigation of semantic hallucination and popularity bias.\n","authors":["Jiayi Liao","Xiangnan He","Ruobing Xie","Jiancan Wu","Yancheng Yuan","Xingwu Sun","Zhanhui Kang","Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.12519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12473v1","updated":"2024-10-16T11:41:24Z","published":"2024-10-16T11:41:24Z","title":"Unifying Economic and Language Models for Enhanced Sentiment Analysis of\n  the Oil Market","summary":"  Crude oil, a critical component of the global economy, has its prices\ninfluenced by various factors such as economic trends, political events, and\nnatural disasters. Traditional prediction methods based on historical data have\ntheir limits in forecasting, but recent advancements in natural language\nprocessing bring new possibilities for event-based analysis. In particular,\nLanguage Models (LM) and their advancement, the Generative Pre-trained\nTransformer (GPT), have shown potential in classifying vast amounts of natural\nlanguage. However, these LMs often have difficulty with domain-specific\nterminology, limiting their effectiveness in the crude oil sector. Addressing\nthis gap, we introduce CrudeBERT, a fine-tuned LM specifically for the crude\noil market. The results indicate that CrudeBERT's sentiment scores align more\nclosely with the WTI Futures curve and significantly enhance price predictions,\nunderscoring the crucial role of integrating economic principles into LMs.\n","authors":["Himmet Kaplan","Ralf-Peter Mundani","Heiko Rölke","Albert Weichselbraun","Martin Tschudy"],"pdf_url":"https://arxiv.org/pdf/2410.12473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12451v1","updated":"2024-10-16T10:58:53Z","published":"2024-10-16T10:58:53Z","title":"Mitigating Dual Latent Confounding Biases in Recommender Systems","summary":"  Recommender systems are extensively utilised across various areas to predict\nuser preferences for personalised experiences and enhanced user engagement and\nsatisfaction. Traditional recommender systems, however, are complicated by\nconfounding bias, particularly in the presence of latent confounders that\naffect both item exposure and user feedback. Existing debiasing methods often\nfail to capture the complex interactions caused by latent confounders in\ninteraction data, especially when dual latent confounders affect both the user\nand item sides. To address this, we propose a novel debiasing method that\njointly integrates the Instrumental Variables (IV) approach and identifiable\nVariational Auto-Encoder (iVAE) for Debiased representation learning in\nRecommendation systems, referred to as IViDR. Specifically, IViDR leverages the\nembeddings of user features as IVs to address confounding bias caused by latent\nconfounders between items and user feedback, and reconstructs the embedding of\nitems to obtain debiased interaction data. Moreover, IViDR employs an\nIdentifiable Variational Auto-Encoder (iVAE) to infer identifiable\nrepresentations of latent confounders between item exposure and user feedback\nfrom both the original and debiased interaction data. Additionally, we provide\ntheoretical analyses of the soundness of using IV and the identifiability of\nthe latent representations. Extensive experiments on both synthetic and\nreal-world datasets demonstrate that IViDR outperforms state-of-the-art models\nin reducing bias and providing reliable recommendations.\n","authors":["Jianfeng Deng","Qingfeng Chen","Debo Cheng","Jiuyong Li","Lin Liu","Xiaojing Du"],"pdf_url":"https://arxiv.org/pdf/2410.12451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12400v1","updated":"2024-10-16T09:28:58Z","published":"2024-10-16T09:28:58Z","title":"QUIDS: Query Intent Generation via Dual Space Modeling","summary":"  Query understanding is a crucial component of Information Retrieval (IR),\naimed at identifying the underlying search intent of textual queries. However,\nmost existing approaches oversimplify this task into query classification or\nclustering, which fails to fully capture the nuanced intent behind the query.\nIn this paper, we address the task of query intent generation: to automatically\ngenerate detailed and precise intent descriptions for search queries using\nrelevant and irrelevant documents given a query. These intent descriptions can\nhelp users understand why the search engine considered the top-ranked documents\nrelevant, and provide more transparency to the retrieval process. We propose a\ndual-space model that uses semantic relevance and irrelevance information in\nthe returned documents to explain the understanding of the query intent.\nSpecifically, in the encoding process, we project, separate, and distinguish\nrelevant and irrelevant documents in the representation space. Then, we\nintroduce a semantic decoupling model in the novel disentangling space, where\nthe semantics of irrelevant information are removed from the relevant space,\nensuring that only the essential and relevant intent is captured. This process\nrefines the understanding of the query and provides more accurate explanations\nfor the search results. Experiments on benchmark data demonstrate that our\nmethods produce high-quality query intent descriptions, outperforming existing\nmethods for this task, as well as state-of-the-art query-based summarization\nmethods. A token-level visualization of attention scores reveals that our model\neffectively reduces the focus on irrelevant intent topics. Our findings open up\npromising research and application directions for query intent generation,\nparticularly in exploratory search.\n","authors":["Yumeng Wang","Xiuying Chen","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2410.12400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12366v1","updated":"2024-10-16T08:37:39Z","published":"2024-10-16T08:37:39Z","title":"Multi-Cause Deconfounding for Recommender Systems with Latent\n  Confounders","summary":"  In recommender systems, various latent confounding factors (e.g., user social\nenvironment and item public attractiveness) can affect user behavior, item\nexposure, and feedback in distinct ways. These factors may directly or\nindirectly impact user feedback and are often shared across items or users,\nmaking them multi-cause latent confounders. However, existing methods typically\nfail to account for latent confounders between users and their feedback, as\nwell as those between items and user feedback simultaneously. To address the\nproblem of multi-cause latent confounders, we propose a multi-cause\ndeconfounding method for recommender systems with latent confounders (MCDCF).\nMCDCF leverages multi-cause causal effect estimation to learn substitutes for\nlatent confounders associated with both users and items, using user behaviour\ndata. Specifically, MCDCF treats the multiple items that users interact with\nand the multiple users that interact with items as treatment variables,\nenabling it to learn substitutes for the latent confounders that influence the\nestimation of causality between users and their feedback, as well as between\nitems and user feedback. Additionally, we theoretically demonstrate the\nsoundness of our MCDCF method. Extensive experiments on three real-world\ndatasets demonstrate that our MCDCF method effectively recovers latent\nconfounders related to users and items, reducing bias and thereby improving\nrecommendation accuracy.\n","authors":["Zhirong Huang","Shichao Zhang","Debo Cheng","Jiuyong Li","Lin Liu","Guixian Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00333v2","updated":"2024-10-16T05:21:47Z","published":"2024-06-01T07:18:56Z","title":"A Practice-Friendly LLM-Enhanced Paradigm with Preference Parsing for\n  Sequential Recommendation","summary":"  The training paradigm integrating large language models (LLM) is gradually\nreshaping sequential recommender systems (SRS) and has shown promising results.\nHowever, most existing LLM-enhanced methods rely on rich textual information on\nthe item side and instance-level supervised fine-tuning (SFT) to inject\ncollaborative information into LLM, which is inefficient and limited in many\napplications. To alleviate these problems, this paper proposes a\npractice-friendly LLM-enhanced paradigm with preference parsing (P2Rec) for\nSRS. Specifically, in the information reconstruction stage, we design a new\nuser-level SFT task for collaborative information injection with the assistance\nof a pre-trained SRS model, which is more efficient and compatible with limited\ntext information. Our goal is to let LLM learn to reconstruct a corresponding\nprior preference distribution from each user's interaction sequence, where LLM\nneeds to effectively parse the latent category of each item and the\nrelationship between different items to accomplish this task. In the\ninformation augmentation stage, we feed each item into LLM to obtain a set of\nenhanced embeddings that combine collaborative information and LLM inference\ncapabilities. These embeddings can then be used to help train various future\nSRS models. Finally, we verify the effectiveness and efficiency of our TSLRec\non three SRS benchmark datasets.\n","authors":["Dugang Liu","Shenxian Xian","Xiaolin Lin","Xiaolian Zhang","Hong Zhu","Yuan Fang","Zhen Chen","Zhong Ming"],"pdf_url":"https://arxiv.org/pdf/2406.00333v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12229v1","updated":"2024-10-16T04:44:34Z","published":"2024-10-16T04:44:34Z","title":"Comprehending Knowledge Graphs with Large Language Models for\n  Recommender Systems","summary":"  Recently, the introduction of knowledge graphs (KGs) has significantly\nadvanced recommender systems by facilitating the discovery of potential\nassociations between items. However, existing methods still face several\nlimitations. First, most KGs suffer from missing facts or limited scopes. This\ncan lead to biased knowledge representations, thereby constraining the model's\nperformance. Second, existing methods typically convert textual information\ninto IDs, resulting in the loss of natural semantic connections between\ndifferent items. Third, existing methods struggle to capture high-order\nrelationships in global KGs due to their inefficient layer-by-layer information\npropagation mechanisms, which are prone to introducing significant noise. To\naddress these limitations, we propose a novel method called CoLaKG, which\nleverages large language models (LLMs) for knowledge-aware recommendation. The\nextensive world knowledge and remarkable reasoning capabilities of LLMs enable\nthem to supplement KGs. Additionally, the strong text comprehension abilities\nof LLMs allow for a better understanding of semantic information. Based on\nthis, we first extract subgraphs centered on each item from the KG and convert\nthem into textual inputs for the LLM. The LLM then outputs its comprehension of\nthese item-centered subgraphs, which are subsequently transformed into semantic\nembeddings. Furthermore, to utilize the global information of the KG, we\nconstruct an item-item graph using these semantic embeddings, which can\ndirectly capture higher-order associations between items. Both the semantic\nembeddings and the structural information from the item-item graph are\neffectively integrated into the recommendation model through our designed\nrepresentation alignment and neighbor augmentation modules. Extensive\nexperiments on four real-world datasets demonstrate the superiority of our\nmethod.\n","authors":["Ziqiang Cui","Yunpeng Weng","Xing Tang","Fuyuan Lyu","Dugang Liu","Xiuqiang He","Chen Ma"],"pdf_url":"https://arxiv.org/pdf/2410.12229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12228v1","updated":"2024-10-16T04:44:15Z","published":"2024-10-16T04:44:15Z","title":"Triple Modality Fusion: Aligning Visual, Textual, and Graph Data with\n  Large Language Models for Multi-Behavior Recommendations","summary":"  Integrating diverse data modalities is crucial for enhancing the performance\nof personalized recommendation systems. Traditional models, which often rely on\nsingular data sources, lack the depth needed to accurately capture the\nmultifaceted nature of item features and user behaviors. This paper introduces\na novel framework for multi-behavior recommendations, leveraging the fusion of\ntriple-modality, which is visual, textual, and graph data through alignment\nwith large language models (LLMs). By incorporating visual information, we\ncapture contextual and aesthetic item characteristics; textual data provides\ninsights into user interests and item features in detail; and graph data\nelucidates relationships within the item-behavior heterogeneous graphs. Our\nproposed model called Triple Modality Fusion (TMF) utilizes the power of LLMs\nto align and integrate these three modalities, achieving a comprehensive\nrepresentation of user behaviors. The LLM models the user's interactions\nincluding behaviors and item features in natural languages. Initially, the LLM\nis warmed up using only natural language-based prompts. We then devise the\nmodality fusion module based on cross-attention and self-attention mechanisms\nto integrate different modalities from other models into the same embedding\nspace and incorporate them into an LLM. Extensive experiments demonstrate the\neffectiveness of our approach in improving recommendation accuracy. Further\nablation studies validate the effectiveness of our model design and benefits of\nthe TMF.\n","authors":["Luyi Ma","Xiaohan Li","Zezhong Fan","Jianpeng Xu","Jason Cho","Praveen Kanumala","Kaushiki Nag","Sushant Kumar","Kannan Achan"],"pdf_url":"https://arxiv.org/pdf/2410.12228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17969v2","updated":"2024-10-16T03:35:22Z","published":"2024-05-28T08:56:33Z","title":"Knowledge Circuits in Pretrained Transformers","summary":"  The remarkable capabilities of modern large language models are rooted in\ntheir vast repositories of knowledge encoded within their parameters, enabling\nthem to perceive the world and engage in reasoning. The inner workings of how\nthese models store knowledge have long been a subject of intense interest and\ninvestigation among researchers. To date, most studies have concentrated on\nisolated components within these models, such as the Multilayer Perceptrons and\nattention head. In this paper, we delve into the computation graph of the\nlanguage model to uncover the knowledge circuits that are instrumental in\narticulating specific knowledge. The experiments, conducted with GPT2 and\nTinyLLAMA, have allowed us to observe how certain information heads, relation\nheads, and Multilayer Perceptrons collaboratively encode knowledge within the\nmodel. Moreover, we evaluate the impact of current knowledge editing techniques\non these knowledge circuits, providing deeper insights into the functioning and\nconstraints of these editing methodologies. Finally, we utilize knowledge\ncircuits to analyze and interpret language model behaviors such as\nhallucinations and in-context learning. We believe the knowledge circuits hold\npotential for advancing our understanding of Transformers and guiding the\nimproved design of knowledge editing. Code and data are available in\nhttps://github.com/zjunlp/KnowledgeCircuits.\n","authors":["Yunzhi Yao","Ningyu Zhang","Zekun Xi","Mengru Wang","Ziwen Xu","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.17969v2.pdf","comment":"NeurIPS 2024, 32 pages"},{"id":"http://arxiv.org/abs/2410.09119v2","updated":"2024-10-16T03:15:12Z","published":"2024-10-10T21:13:56Z","title":"$\\textit{lucie}$: An Improved Python Package for Loading Datasets from\n  the UCI Machine Learning Repository","summary":"  The University of California--Irvine (UCI) Machine Learning (ML) Repository\n(UCIMLR) is consistently cited as one of the most popular dataset repositories,\nhosting hundreds of high-impact datasets. However, a significant portion,\nincluding 28.4% of the top 250, cannot be imported via the $\\textit{ucimlrepo}$\npackage that is provided and recommended by the UCIMLR website. Instead, they\nare hosted as .zip files, containing nonstandard formats that are difficult to\nimport without additional ad hoc processing. To address this issue, here we\npresent $\\textit{lucie}$ -- $\\underline{l}oad$ $\\underline{U}niversity$\n$\\underline{C}alifornia$ $\\underline{I}rvine$ $\\underline{e}xamples$ -- a\nutility that automatically determines the data format and imports many of these\npreviously non-importable datasets, while preserving as much of a tabular data\nstructure as possible. $\\textit{lucie}$ was designed using the top 100 most\npopular datasets and benchmarked on the next 130, where it resulted in a\nsuccess rate of 95.4% vs. 73.1% for $\\textit{ucimlrepo}$. $\\textit{lucie}$ is\navailable as a Python package on PyPI with 98% code coverage.\n","authors":["Kenneth Ge","Phuc Nguyen","Ramy Arnaout"],"pdf_url":"https://arxiv.org/pdf/2410.09119v2.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.07427v2","updated":"2024-10-16T02:37:50Z","published":"2024-08-14T10:03:40Z","title":"Beyond Inter-Item Relations: Dynamic Adaption for Enhancing LLM-Based\n  Sequential Recommendation","summary":"  Sequential recommender systems (SRS) predict the next items that users may\nprefer based on user historical interaction sequences. Inspired by the rise of\nlarge language models (LLMs) in various AI applications, there is a surge of\nwork on LLM-based SRS. Despite their attractive performance, existing LLM-based\nSRS still exhibit some limitations, including neglecting intra-item relations,\nignoring long-term collaborative knowledge and using inflexible architecture\ndesigns for adaption. To alleviate these issues, we propose an LLM-based\nsequential recommendation model named DARec. Built on top of coarse-grained\nadaption for capturing inter-item relations, DARec is further enhanced with (1)\ncontext masking that models intra-item relations to help LLM better understand\ntoken and item semantics in the context of SRS, (2) collaborative knowledge\ninjection that helps LLM incorporate long-term collaborative knowledge, and (3)\na dynamic adaption mechanism that uses Bayesian optimization to flexibly choose\nlayer-wise adapter architectures in order to better incorporate different\nsequential information. Extensive experiments demonstrate that DARec can\neffectively handle sequential recommendation in a dynamic and adaptive manner.\n","authors":["CanYi Liu","Wei Li"," Youchen"," Zhang","Hui Li","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2408.07427v2.pdf","comment":"11 pages, 14 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.12790v1","updated":"2024-10-16T17:59:49Z","published":"2024-10-16T17:59:49Z","title":"Dual Prototype Evolving for Test-Time Generalization of Vision-Language\n  Models","summary":"  Test-time adaptation, which enables models to generalize to diverse data with\nunlabeled test samples, holds significant value in real-world scenarios.\nRecently, researchers have applied this setting to advanced pre-trained\nvision-language models (VLMs), developing approaches such as test-time prompt\ntuning to further extend their practical applicability. However, these methods\ntypically focus solely on adapting VLMs from a single modality and fail to\naccumulate task-specific knowledge as more samples are processed. To address\nthis, we introduce Dual Prototype Evolving (DPE), a novel test-time adaptation\napproach for VLMs that effectively accumulates task-specific knowledge from\nmulti-modalities. Specifically, we create and evolve two sets of\nprototypes--textual and visual--to progressively capture more accurate\nmulti-modal representations for target classes during test time. Moreover, to\npromote consistent multi-modal representations, we introduce and optimize\nlearnable residuals for each test sample to align the prototypes from both\nmodalities. Extensive experimental results on 15 benchmark datasets demonstrate\nthat our proposed DPE consistently outperforms previous state-of-the-art\nmethods while also exhibiting competitive computational efficiency. Code is\navailable at https://github.com/zhangce01/DPE-CLIP.\n","authors":["Ce Zhang","Simon Stepputtis","Katia Sycara","Yaqi Xie"],"pdf_url":"https://arxiv.org/pdf/2410.12790v1.pdf","comment":"Accepted by NeurIPS 2024. Project page:\n  https://zhangce01.github.io/DPE-CLIP"},{"id":"http://arxiv.org/abs/2410.12785v1","updated":"2024-10-16T17:58:34Z","published":"2024-10-16T17:58:34Z","title":"Metal Price Spike Prediction via a Neurosymbolic Ensemble Approach","summary":"  Predicting price spikes in critical metals such as Cobalt, Copper, Magnesium,\nand Nickel is crucial for mitigating economic risks associated with global\ntrends like the energy transition and reshoring of manufacturing. While\ntraditional models have focused on regression-based approaches, our work\nintroduces a neurosymbolic ensemble framework that integrates multiple neural\nmodels with symbolic error detection and correction rules. This framework is\ndesigned to enhance predictive accuracy by correcting individual model errors\nand offering interpretability through rule-based explanations. We show that our\nmethod provides up to 6.42% improvement in precision, 29.41% increase in recall\nat 13.24% increase in F1 over the best performing neural models. Further, our\nmethod, as it is based on logical rules, has the benefit of affording an\nexplanation as to which combination of neural models directly contribute to a\ngiven prediction.\n","authors":["Nathaniel Lee","Noel Ngu","Harshdeep Singh Sahdev","Pramod Motaganahall","Al Mehdi Saadat Chowdhury","Bowen Xi","Paulo Shakarian"],"pdf_url":"https://arxiv.org/pdf/2410.12785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12784v1","updated":"2024-10-16T17:58:19Z","published":"2024-10-16T17:58:19Z","title":"JudgeBench: A Benchmark for Evaluating LLM-based Judges","summary":"  LLM-based judges have emerged as a scalable alternative to human evaluation\nand are increasingly used to assess, compare, and improve models. However, the\nreliability of LLM-based judges themselves is rarely scrutinized. As LLMs\nbecome more advanced, their responses grow more sophisticated, requiring\nstronger judges to evaluate them. Existing benchmarks primarily focus on a\njudge's alignment with human preferences, but often fail to account for more\nchallenging tasks where crowdsourced human preference is a poor indicator of\nfactual and logical correctness. To address this, we propose a novel evaluation\nframework to objectively evaluate LLM-based judges. Based on this framework, we\npropose JudgeBench, a benchmark for evaluating LLM-based judges on challenging\nresponse pairs spanning knowledge, reasoning, math, and coding. JudgeBench\nleverages a novel pipeline for converting existing difficult datasets into\nchallenging response pairs with preference labels reflecting objective\ncorrectness. Our comprehensive evaluation on a collection of prompted judges,\nfine-tuned judges, multi-agent judges, and reward models shows that JudgeBench\nposes a significantly greater challenge than previous benchmarks, with many\nstrong models (e.g., GPT-4o) performing just slightly better than random\nguessing. Overall, JudgeBench offers a reliable platform for assessing\nincreasingly advanced LLM-based judges. Data and code are available at\nhttps://github.com/ScalerLab/JudgeBench .\n","authors":["Sijun Tan","Siyuan Zhuang","Kyle Montgomery","William Y. Tang","Alejandro Cuadron","Chenguang Wang","Raluca Ada Popa","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2410.12784v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.12783v1","updated":"2024-10-16T17:58:08Z","published":"2024-10-16T17:58:08Z","title":"Context-Scaling versus Task-Scaling in In-Context Learning","summary":"  Transformers exhibit In-Context Learning (ICL), where these models solve new\ntasks by using examples in the prompt without additional training. In our work,\nwe identify and analyze two key components of ICL: (1) context-scaling, where\nmodel performance improves as the number of in-context examples increases and\n(2) task-scaling, where model performance improves as the number of\npre-training tasks increases. While transformers are capable of both\ncontext-scaling and task-scaling, we empirically show that standard Multi-Layer\nPerceptrons (MLPs) with vectorized input are only capable of task-scaling. To\nunderstand how transformers are capable of context-scaling, we first propose a\nsignificantly simplified transformer architecture without key, query, value\nweights. We show that it performs ICL comparably to the original GPT-2 model in\nvarious statistical learning tasks including linear regression, teacher-student\nsettings. Furthermore, a single block of our simplified transformer can be\nviewed as data dependent feature map followed by an MLP. This feature map on\nits own is a powerful predictor that is capable of context-scaling but is not\ncapable of task-scaling. We show empirically that concatenating the output of\nthis feature map with vectorized data as an input to MLPs enables both\ncontext-scaling and task-scaling. This finding provides a simple setting to\nstudy context and task-scaling for ICL.\n","authors":["Amirhesam Abedsoltan","Adityanarayanan Radhakrishnan","Jingfeng Wu","Mikhail Belkin"],"pdf_url":"https://arxiv.org/pdf/2410.12783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16257v2","updated":"2024-10-16T17:57:24Z","published":"2024-06-24T01:45:13Z","title":"Towards Scalable Exact Machine Unlearning Using Parameter-Efficient\n  Fine-Tuning","summary":"  Machine unlearning is the process of efficiently removing the influence of a\ntraining data instance from a trained machine learning model without retraining\nit from scratch. A popular subclass of unlearning approaches is exact machine\nunlearning, which focuses on techniques that explicitly guarantee the removal\nof the influence of a data instance from a model. Exact unlearning approaches\nuse a machine learning model in which individual components are trained on\ndisjoint subsets of the data. During deletion, exact unlearning approaches only\nretrain the affected components rather than the entire model. While existing\napproaches reduce retraining costs, it can still be expensive for an\norganization to retrain a model component as it requires halting a system in\nproduction, which leads to service failure and adversely impacts customers. To\naddress these challenges, we introduce an exact unlearning framework --\nSequence-aware Sharded Sliced Training (S3T), which is designed to enhance the\ndeletion capabilities of an exact unlearning system while minimizing the impact\non model's performance. At the core of S3T, we utilize a lightweight\nparameter-efficient fine-tuning approach that enables parameter isolation by\nsequentially training layers with disjoint data slices. This enables efficient\nunlearning by simply deactivating the layers affected by data deletion.\nFurthermore, to reduce the retraining cost and improve model performance, we\ntrain the model on multiple data sequences, which allows S3T to handle an\nincreased number of deletion requests. Both theoretically and empirically, we\ndemonstrate that S3T attains superior deletion capabilities and enhanced\nperformance compared to baselines across a wide range of settings.\n","authors":["Somnath Basu Roy Chowdhury","Krzysztof Choromanski","Arijit Sehanobish","Avinava Dubey","Snigdha Chaturvedi"],"pdf_url":"https://arxiv.org/pdf/2406.16257v2.pdf","comment":"Preliminary version accepted at the SafeGenAi Workshop, NeurIPS, 2024"},{"id":"http://arxiv.org/abs/2409.06953v2","updated":"2024-10-16T17:56:20Z","published":"2024-09-11T02:29:53Z","title":"Neural Algorithmic Reasoning with Multiple Correct Solutions","summary":"  Neural Algorithmic Reasoning (NAR) aims to optimize classical algorithms.\nHowever, canonical implementations of NAR train neural networks to return only\na single solution, even when there are multiple correct solutions to a problem,\nsuch as single-source shortest paths. For some applications, it is desirable to\nrecover more than one correct solution. To that end, we give the first method\nfor NAR with multiple solutions. We demonstrate our method on two classical\nalgorithms: Bellman-Ford (BF) and Depth-First Search (DFS), favouring deeper\ninsight into two algorithms over a broader survey of algorithms. This method\ninvolves generating appropriate training data as well as sampling and\nvalidating solutions from model output. Each step of our method, which can\nserve as a framework for neural algorithmic reasoning beyond the tasks\npresented in this paper, might be of independent interest to the field and our\nresults represent the first attempt at this task in the NAR literature.\n","authors":["Zeno Kujawa","John Poole","Dobrik Georgiev","Danilo Numeroso","Pietro Liò"],"pdf_url":"https://arxiv.org/pdf/2409.06953v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08895v3","updated":"2024-10-16T17:54:15Z","published":"2024-01-17T00:36:58Z","title":"cedar: Optimized and Unified Machine Learning Input Data Pipelines","summary":"  The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.\n","authors":["Mark Zhao","Emanuel Adamiak","Christos Kozyrakis"],"pdf_url":"https://arxiv.org/pdf/2401.08895v3.pdf","comment":"Accepted to PVLDB Volume 18"},{"id":"http://arxiv.org/abs/2410.12779v1","updated":"2024-10-16T17:53:26Z","published":"2024-10-16T17:53:26Z","title":"Geometry-Aware Generative Autoencoders for Warped Riemannian Metric\n  Learning and Generative Modeling on Data Manifolds","summary":"  Rapid growth of high-dimensional datasets in fields such as single-cell RNA\nsequencing and spatial genomics has led to unprecedented opportunities for\nscientific discovery, but it also presents unique computational and statistical\nchallenges. Traditional methods struggle with geometry-aware data generation,\ninterpolation along meaningful trajectories, and transporting populations via\nfeasible paths. To address these issues, we introduce Geometry-Aware Generative\nAutoencoder (GAGA), a novel framework that combines extensible manifold\nlearning with generative modeling. GAGA constructs a neural network embedding\nspace that respects the intrinsic geometries discovered by manifold learning\nand learns a novel warped Riemannian metric on the data space. This warped\nmetric is derived from both the points on the data manifold and negative\nsamples off the manifold, allowing it to characterize a meaningful geometry\nacross the entire latent space. Using this metric, GAGA can uniformly sample\npoints on the manifold, generate points along geodesics, and interpolate\nbetween populations across the learned manifold. GAGA shows competitive\nperformance in simulated and real world datasets, including a 30% improvement\nover the state-of-the-art methods in single-cell population-level trajectory\ninference.\n","authors":["Xingzhi Sun","Danqi Liao","Kincaid MacDonald","Yanlei Zhang","Chen Liu","Guillaume Huguet","Guy Wolf","Ian Adelstein","Tim G. J. Rudner","Smita Krishnaswamy"],"pdf_url":"https://arxiv.org/pdf/2410.12779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14033v2","updated":"2024-10-16T17:52:37Z","published":"2024-05-22T22:08:13Z","title":"Adversarial Training of Two-Layer Polynomial and ReLU Activation\n  Networks via Convex Optimization","summary":"  Training neural networks which are robust to adversarial attacks remains an\nimportant problem in deep learning, especially as heavily overparameterized\nmodels are adopted in safety-critical settings. Drawing from recent work which\nreformulates the training problems for two-layer ReLU and polynomial activation\nnetworks as convex programs, we devise a convex semidefinite program (SDP) for\nadversarial training of two-layer polynomial activation networks and prove that\nthe convex SDP achieves the same globally optimal solution as its nonconvex\ncounterpart. The convex SDP is observed to improve robust test accuracy against\n$\\ell_\\infty$ attacks relative to the original convex training formulation on\nmultiple datasets. Additionally, we present scalable implementations of\nadversarial training for two-layer polynomial and ReLU networks which are\ncompatible with standard machine learning libraries and GPU acceleration.\nLeveraging these implementations, we retrain the final two fully connected\nlayers of a Pre-Activation ResNet-18 model on the CIFAR-10 dataset with both\npolynomial and ReLU activations. The two `robustified' models achieve\nsignificantly higher robust test accuracies against $\\ell_\\infty$ attacks than\na Pre-Activation ResNet-18 model trained with sharpness-aware minimization,\ndemonstrating the practical utility of convex adversarial training on\nlarge-scale problems.\n","authors":["Daniel Kuelbs","Sanjay Lall","Mert Pilanci"],"pdf_url":"https://arxiv.org/pdf/2405.14033v2.pdf","comment":"17 pages, 2 figures. Added a proof of the main theorem in the\n  appendix. Expanded numerical results section. Added references"},{"id":"http://arxiv.org/abs/2410.12777v1","updated":"2024-10-16T17:51:25Z","published":"2024-10-16T17:51:25Z","title":"Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned\n  Concepts","summary":"  With the rapid progress of diffusion-based content generation, significant\nefforts are being made to unlearn harmful or copyrighted concepts from\npretrained diffusion models (DMs) to prevent potential model misuse. However,\nit is observed that even when DMs are properly unlearned before release,\nmalicious finetuning can compromise this process, causing DMs to relearn the\nunlearned concepts. This occurs partly because certain benign concepts (e.g.,\n\"skin\") retained in DMs are related to the unlearned ones (e.g., \"nudity\"),\nfacilitating their relearning via finetuning. To address this, we propose\nmeta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an\nunlearned DM when used as is; moreover, if the meta-unlearned DM undergoes\nmalicious finetuning on unlearned concepts, the related benign concepts\nretained within it will be triggered to self-destruct, hindering the relearning\nof unlearned concepts. Our meta-unlearning framework is compatible with most\nexisting unlearning methods, requiring only the addition of an\neasy-to-implement meta objective. We validate our approach through empirical\nexperiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4\nand SDXL), supported by extensive ablation studies. Our code is available at\nhttps://github.com/sail-sg/Meta-Unlearning.\n","authors":["Hongcheng Gao","Tianyu Pang","Chao Du","Taihang Hu","Zhijie Deng","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.12777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12766v1","updated":"2024-10-16T17:41:59Z","published":"2024-10-16T17:41:59Z","title":"The Non-Local Model Merging Problem: Permutation Symmetries and Variance\n  Collapse","summary":"  Model merging aims to efficiently combine the weights of multiple expert\nmodels, each trained on a specific task, into a single multi-task model, with\nstrong performance across all tasks. When applied to all but the last layer of\nweights, existing methods -- such as Task Arithmetic, TIES-merging, and TALL\nmask merging -- work well to combine expert models obtained by fine-tuning a\ncommon foundation model, operating within a \"local\" neighborhood of the\nfoundation model. This work explores the more challenging scenario of\n\"non-local\" merging, which we find arises when an expert model changes\nsignificantly during pretraining or where the expert models do not even share a\ncommon foundation model.\n  We observe that standard merging techniques often fail to generalize\neffectively in this non-local setting, even when accounting for permutation\nsymmetries using standard techniques. We identify that this failure is, in\npart, due to \"variance collapse\", a phenomenon identified also in the setting\nof linear mode connectivity by Jordan et al. (2023). To address this, we\npropose a multi-task technique to re-scale and shift the output activations of\nthe merged model for each task, aligning its output statistics with those of\nthe corresponding task-specific expert models. Our experiments demonstrate that\nthis correction significantly improves the performance of various model merging\napproaches in non-local settings, providing a strong baseline for future\nresearch on this problem.\n","authors":["Ekansh Sharma","Daniel M. Roy","Gintare Karolina Dziugaite"],"pdf_url":"https://arxiv.org/pdf/2410.12766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17563v3","updated":"2024-10-16T17:36:49Z","published":"2024-04-26T17:45:32Z","title":"An exactly solvable model for emergence and scaling laws in the\n  multitask sparse parity problem","summary":"  Deep learning models can exhibit what appears to be a sudden ability to solve\na new problem as training time, training data, or model size increases, a\nphenomenon known as emergence. In this paper, we present a framework where each\nnew ability (a skill) is represented as a basis function. We solve a simple\nmulti-linear model in this skill-basis, finding analytic expressions for the\nemergence of new skills, as well as for scaling laws of the loss with training\ntime, data size, model size, and optimal compute. We compare our detailed\ncalculations to direct simulations of a two-layer neural network trained on\nmultitask sparse parity, where the tasks in the dataset are distributed\naccording to a power-law. Our simple model captures, using a single fit\nparameter, the sigmoidal emergence of multiple new skills as training time,\ndata size or model size increases in the neural network.\n","authors":["Yoonsoo Nam","Nayara Fonseca","Seok Hyeong Lee","Chris Mingard","Ard A. Louis"],"pdf_url":"https://arxiv.org/pdf/2404.17563v3.pdf","comment":"Accepted at NeurIPS 2024 Conference"},{"id":"http://arxiv.org/abs/2410.12761v1","updated":"2024-10-16T17:32:23Z","published":"2024-10-16T17:32:23Z","title":"SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And\n  Video Generation","summary":"  Recent advances in diffusion models have significantly enhanced their ability\nto generate high-quality images and videos, but they have also increased the\nrisk of producing unsafe content. Existing unlearning/editing-based methods for\nsafe generation remove harmful concepts from models but face several\nchallenges: (1) They cannot instantly remove harmful concepts without training.\n(2) Their safe generation capabilities depend on collected training data. (3)\nThey alter model weights, risking degradation in quality for content unrelated\nto toxic concepts. To address these, we propose SAFREE, a novel, training-free\napproach for safe T2I and T2V, that does not alter the model's weights.\nSpecifically, we detect a subspace corresponding to a set of toxic concepts in\nthe text embedding space and steer prompt embeddings away from this subspace,\nthereby filtering out harmful content while preserving intended semantics. To\nbalance the trade-off between filtering toxicity and preserving safe concepts,\nSAFREE incorporates a novel self-validating filtering mechanism that\ndynamically adjusts the denoising steps when applying the filtered embeddings.\nAdditionally, we incorporate adaptive re-attention mechanisms within the\ndiffusion latent space to selectively diminish the influence of features\nrelated to toxic concepts at the pixel level. In the end, SAFREE ensures\ncoherent safety checking, preserving the fidelity, quality, and safety of the\noutput. SAFREE achieves SOTA performance in suppressing unsafe content in T2I\ngeneration compared to training-free baselines and effectively filters targeted\nconcepts while maintaining high-quality images. It also shows competitive\nresults against training-based methods. We extend SAFREE to various T2I\nbackbones and T2V tasks, showcasing its flexibility and generalization. SAFREE\nprovides a robust and adaptable safeguard for ensuring safe visual generation.\n","authors":["Jaehong Yoon","Shoubin Yu","Vaidehi Patil","Huaxiu Yao","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.12761v1.pdf","comment":"The first two authors contributed equally; Project page:\n  https://safree-safe-t2i-t2v.github.io/"},{"id":"http://arxiv.org/abs/2410.12757v1","updated":"2024-10-16T17:25:25Z","published":"2024-10-16T17:25:25Z","title":"StyleDistance: Stronger Content-Independent Style Embeddings with\n  Synthetic Parallel Examples","summary":"  Style representations aim to embed texts with similar writing styles closely\nand texts with different styles far apart, regardless of content. However, the\ncontrastive triplets often used for training these representations may vary in\nboth style and content, leading to potential content leakage in the\nrepresentations. We introduce StyleDistance, a novel approach to training\nstronger content-independent style embeddings. We use a large language model to\ncreate a synthetic dataset of near-exact paraphrases with controlled style\nvariations, and produce positive and negative examples across 40 distinct style\nfeatures for precise contrastive learning. We assess the quality of our\nsynthetic data and embeddings through human and automatic evaluations.\nStyleDistance enhances the content-independence of style embeddings, which\ngeneralize to real-world benchmarks and outperform leading style\nrepresentations in downstream applications. Our model can be found at\nhttps://huggingface.co/StyleDistance/styledistance .\n","authors":["Ajay Patel","Jiacheng Zhu","Justin Qiu","Zachary Horvitz","Marianna Apidianaki","Kathleen McKeown","Chris Callison-Burch"],"pdf_url":"https://arxiv.org/pdf/2410.12757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10267v2","updated":"2024-10-16T17:22:54Z","published":"2023-11-17T01:27:01Z","title":"Energy and Carbon Considerations of Fine-Tuning BERT","summary":"  Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP\ncommunity, existing work quantifying energy costs and associated carbon\nemissions has largely focused on language model pre-training. Although a single\npre-training run draws substantially more energy than fine-tuning, fine-tuning\nis performed more frequently by many more individual actors, and thus must be\naccounted for when considering the energy and carbon footprint of NLP. In order\nto better characterize the role of fine-tuning in the landscape of energy and\ncarbon emissions in NLP, we perform a careful empirical study of the\ncomputational costs of fine-tuning across tasks, datasets, hardware\ninfrastructure and measurement modalities. Our experimental results allow us to\nplace fine-tuning energy and carbon costs into perspective with respect to\npre-training and inference, and outline recommendations to NLP researchers and\npractitioners who wish to improve their fine-tuning energy efficiency.\n","authors":["Xiaorong Wang","Clara Na","Emma Strubell","Sorelle Friedler","Sasha Luccioni"],"pdf_url":"https://arxiv.org/pdf/2311.10267v2.pdf","comment":"EMNLP 2023 Findings; First two authors contributed equally; 12 pages"},{"id":"http://arxiv.org/abs/2410.12747v1","updated":"2024-10-16T17:06:55Z","published":"2024-10-16T17:06:55Z","title":"Initialization Method for Factorization Machine Based on Low-Rank\n  Approximation for Constructing a Corrected Approximate Ising Model","summary":"  This paper presents an initialization method that can approximate a given\napproximate Ising model with a high degree of accuracy using the Factorization\nMachine (FM), a machine learning model. The construction of Ising models using\nFM is applied to the combinatorial optimization problem using the factorization\nmachine with quantum annealing. It is anticipated that the optimization\nperformance of FMQA will be enhanced through the implementation of the\nwarm-start method. Nevertheless, the optimal initialization method for\nleveraging the warm-start approach in FMQA remains undetermined. Consequently,\nthe present study compares a number of initialization methods and identifies\nthe most appropriate for use with a warm-start in FMQA through numerical\nexperimentation. Furthermore, the properties of the proposed FM initialization\nmethod are analyzed using random matrix theory, demonstrating that the\napproximation accuracy of the proposed method is not significantly influenced\nby the specific Ising model under consideration. The findings of this study\nwill facilitate the advancement of combinatorial optimization problem-solving\nthrough the use of Ising machines.\n","authors":["Yuya Seki","Hyakka Nakada","Shu Tanaka"],"pdf_url":"https://arxiv.org/pdf/2410.12747v1.pdf","comment":"25 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.08710v2","updated":"2024-10-16T17:06:41Z","published":"2024-10-11T10:53:38Z","title":"Preferential Normalizing Flows","summary":"  Eliciting a high-dimensional probability distribution from an expert via\nnoisy judgments is notoriously challenging, yet useful for many applications,\nsuch as prior elicitation and reward modeling. We introduce a method for\neliciting the expert's belief density as a normalizing flow based solely on\npreferential questions such as comparing or ranking alternatives. This allows\neliciting in principle arbitrarily flexible densities, but flow estimation is\nsusceptible to the challenge of collapsing or diverging probability mass that\nmakes it difficult in practice. We tackle this problem by introducing a novel\nfunctional prior for the flow, motivated by a decision-theoretic argument, and\nshow empirically that the belief density can be inferred as the function-space\nmaximum a posteriori estimate. We demonstrate our method by eliciting\nmultivariate belief densities of simulated experts, including the prior belief\nof a general-purpose large language model over a real-world dataset.\n","authors":["Petrus Mikkola","Luigi Acerbi","Arto Klami"],"pdf_url":"https://arxiv.org/pdf/2410.08710v2.pdf","comment":"29 pages, 18 figures, Accepted at NeurIPS2024"},{"id":"http://arxiv.org/abs/2307.06541v2","updated":"2024-10-16T16:59:58Z","published":"2023-07-13T03:06:36Z","title":"On the Effective Horizon of Inverse Reinforcement Learning","summary":"  Inverse reinforcement learning (IRL) algorithms often rely on (forward)\nreinforcement learning or planning over a given time horizon to compute an\napproximately optimal policy for a hypothesized reward function and then match\nthis policy with expert demonstrations. The time horizon plays a critical role\nin determining both the accuracy of reward estimates and the computational\nefficiency of IRL algorithms. Interestingly, an \\emph{effective time horizon}\nshorter than the ground-truth value often produces better results faster. This\nwork formally analyzes this phenomenon and provides an explanation: the time\nhorizon controls the complexity of an induced policy class and mitigates\noverfitting with limited data. This analysis serves as a guide for the\nprincipled choice of the effective horizon for IRL. It also prompts us to\nre-examine the classic IRL formulation: it is more natural to learn jointly the\nreward and the effective horizon rather than the reward alone with a given\nhorizon. To validate our findings, we implement a cross-validation extension\nand the experimental results confirm the theoretical analysis.\n","authors":["Yiqing Xu","Finale Doshi-Velez","David Hsu"],"pdf_url":"https://arxiv.org/pdf/2307.06541v2.pdf","comment":"9 pages, under review"},{"id":"http://arxiv.org/abs/2406.14003v3","updated":"2024-10-16T16:51:13Z","published":"2024-06-20T05:13:33Z","title":"Deep Optimal Experimental Design for Parameter Estimation Problems","summary":"  Optimal experimental design is a well studied field in applied science and\nengineering. Techniques for estimating such a design are commonly used within\nthe framework of parameter estimation. Nonetheless, in recent years parameter\nestimation techniques are changing rapidly with the introduction of deep\nlearning techniques to replace traditional estimation methods. This in turn\nrequires the adaptation of optimal experimental design that is associated with\nthese new techniques. In this paper we investigate a new experimental design\nmethodology that uses deep learning. We show that the training of a network as\na Likelihood Free Estimator can be used to significantly simplify the design\nprocess and circumvent the need for the computationally expensive bi-level\noptimization problem that is inherent in optimal experimental design for\nnon-linear systems. Furthermore, deep design improves the quality of the\nrecovery process for parameter estimation problems. As proof of concept we\napply our methodology to two different systems of Ordinary Differential\nEquations.\n","authors":["Md Shahriar Rahim Siddiqui","Arman Rahmim","Eldad Haber"],"pdf_url":"https://arxiv.org/pdf/2406.14003v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12735v1","updated":"2024-10-16T16:51:01Z","published":"2024-10-16T16:51:01Z","title":"CREAM: Consistency Regularized Self-Rewarding Language Models","summary":"  Recent self-rewarding large language models (LLM) have successfully applied\nLLM-as-a-Judge to iteratively improve the alignment performance without the\nneed of human annotations for preference data. These methods commonly utilize\nthe same LLM to act as both the policy model (which generates responses) and\nthe reward model (which scores and ranks those responses). The ranked responses\nare then used as preference pairs to train the LLM via direct alignment\ntechnologies (e.g. DPO). However, it is noteworthy that throughout this\nprocess, there is no guarantee of accuracy in the rewarding and ranking, which\nis critical for ensuring accurate rewards and high-quality preference data.\nEmpirical results from relatively small LLMs (e.g., 7B parameters) also\nindicate that improvements from self-rewarding may diminish after several\niterations in certain situations, which we hypothesize is due to accumulated\nbias in the reward system. This bias can lead to unreliable preference data for\ntraining the LLM. To address this issue, we first formulate and analyze the\ngeneralized iterative preference fine-tuning framework for self-rewarding\nlanguage model. We then introduce the regularization to this generalized\nframework to mitigate the overconfident preference labeling in the\nself-rewarding process. Based on this theoretical insight, we propose a\nConsistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages\nthe rewarding consistency across different iterations to regularize the\nself-rewarding training, helping the model to learn from more reliable\npreference data. With this explicit regularization, our empirical results\ndemonstrate the superiority of CREAM in improving both reward consistency and\nalignment performance. The code is publicly available at\nhttps://github.com/Raibows/CREAM.\n","authors":["Zhaoyang Wang","Weilei He","Zhiyuan Liang","Xuchao Zhang","Chetan Bansal","Ying Wei","Weitong Zhang","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2410.12735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12730v1","updated":"2024-10-16T16:44:12Z","published":"2024-10-16T16:44:12Z","title":"Counterfactual Generative Modeling with Variational Causal Inference","summary":"  Estimating an individual's potential outcomes under counterfactual treatments\nis a challenging task for traditional causal inference and supervised learning\napproaches when the outcome is high-dimensional (e.g. gene expressions, facial\nimages) and covariates are relatively limited. In this case, to predict one's\noutcomes under counterfactual treatments, it is crucial to leverage individual\ninformation contained in its high-dimensional observed outcome in addition to\nthe covariates. Prior works using variational inference in counterfactual\ngenerative modeling have been focusing on neural adaptations and model variants\nwithin the conditional variational autoencoder formulation, which we argue is\nfundamentally ill-suited to the notion of counterfactual in causal inference.\nIn this work, we present a novel variational Bayesian causal inference\nframework and its theoretical backings to properly handle counterfactual\ngenerative modeling tasks, through which we are able to conduct counterfactual\nsupervision end-to-end during training without any counterfactual samples, and\nencourage latent disentanglement that aids the correct identification of causal\neffect in counterfactual generations. In experiments, we demonstrate the\nadvantage of our framework compared to state-of-the-art models in\ncounterfactual generative modeling on multiple benchmarks.\n","authors":["Yulun Wu","Louie McConnell","Claudia Iriondo"],"pdf_url":"https://arxiv.org/pdf/2410.12730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12728v1","updated":"2024-10-16T16:42:20Z","published":"2024-10-16T16:42:20Z","title":"Transformer based super-resolution downscaling for regional reanalysis:\n  Full domain vs tiling approaches","summary":"  Super-resolution (SR) is a promising cost-effective downscaling methodology\nfor producing high-resolution climate information from coarser counterparts. A\nparticular application is downscaling regional reanalysis outputs (predictand)\nfrom the driving global counterparts (predictor). This study conducts an\nintercomparison of various SR downscaling methods focusing on temperature and\nusing the CERRA reanalysis (5.5 km resolution, produced with a regional\natmospheric model driven by ERA5) as example. The method proposed in this work\nis the Swin transformer and two alternative methods are used as benchmark\n(fully convolutional U-Net and convolutional and dense DeepESD) as well as the\nsimple bicubic interpolation. We compare two approaches, the standard one using\nthe full domain as input and a more scalable tiling approach, dividing the full\ndomain into tiles that are used as input. The methods are trained to downscale\nCERRA surface temperature, based on temperature information from the driving\nERA5; in addition, the tiling approach includes static orographic information.\nWe show that the tiling approach, which requires spatial transferability, comes\nat the cost of a lower performance (although it outperforms some full-domain\nbenchmarks), but provides an efficient scalable solution that allows SR\nreduction on a pan-European scale and is valuable for real-time applications.\n","authors":["Antonio Pérez","Mario Santa Cruz","Daniel San Martín","José Manuel Gutiérrez"],"pdf_url":"https://arxiv.org/pdf/2410.12728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06163v2","updated":"2024-10-16T16:40:00Z","published":"2024-10-08T16:08:24Z","title":"Likelihood-based Differentiable Structure Learning","summary":"  Existing approaches to differentiable structure learning of directed acyclic\ngraphs (DAGs) rely on strong identifiability assumptions in order to guarantee\nthat global minimizers of the acyclicity-constrained optimization problem\nidentifies the true DAG. Moreover, it has been observed empirically that the\noptimizer may exploit undesirable artifacts in the loss function. We explain\nand remedy these issues by studying the behavior of differentiable\nacyclicity-constrained programs under general likelihoods with multiple global\nminimizers. By carefully regularizing the likelihood, it is possible to\nidentify the sparsest model in the Markov equivalence class, even in the\nabsence of an identifiable parametrization. We first study the Gaussian case in\ndetail, showing how proper regularization of the likelihood defines a score\nthat identifies the sparsest model. Assuming faithfulness, it also recovers the\nMarkov equivalence class. These results are then generalized to general models\nand likelihoods, where the same claims hold. These theoretical results are\nvalidated empirically, showing how this can be done using standard\ngradient-based optimizers, thus paving the way for differentiable structure\nlearning under general models and losses.\n","authors":["Chang Deng","Kevin Bello","Pradeep Ravikumar","Bryon Aragam"],"pdf_url":"https://arxiv.org/pdf/2410.06163v2.pdf","comment":"38 pages, 14 figures, to appear at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.12725v1","updated":"2024-10-16T16:36:23Z","published":"2024-10-16T16:36:23Z","title":"Optimizing 3D Geometry Reconstruction from Implicit Neural\n  Representations","summary":"  Implicit neural representations have emerged as a powerful tool in learning\n3D geometry, offering unparalleled advantages over conventional representations\nlike mesh-based methods. A common type of INR implicitly encodes a shape's\nboundary as the zero-level set of the learned continuous function and learns a\nmapping from a low-dimensional latent space to the space of all possible shapes\nrepresented by its signed distance function. However, most INRs struggle to\nretain high-frequency details, which are crucial for accurate geometric\ndepiction, and they are computationally expensive. To address these\nlimitations, we present a novel approach that both reduces computational\nexpenses and enhances the capture of fine details. Our method integrates\nperiodic activation functions, positional encodings, and normals into the\nneural network architecture. This integration significantly enhances the\nmodel's ability to learn the entire space of 3D shapes while preserving\nintricate details and sharp features, areas where conventional representations\noften fall short.\n","authors":["Shen Fan","Przemyslaw Musialski"],"pdf_url":"https://arxiv.org/pdf/2410.12725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10759v2","updated":"2024-10-16T16:31:37Z","published":"2024-10-14T17:38:41Z","title":"SplitLLM: Collaborative Inference of LLMs for Model Placement and\n  Throughput Optimization","summary":"  Large language models (LLMs) have been a disruptive innovation in recent\nyears, and they play a crucial role in our daily lives due to their ability to\nunderstand and generate human-like text. Their capabilities include natural\nlanguage understanding, information retrieval and search, translation,\nchatbots, virtual assistance, and many more. However, it is well known that\nLLMs are massive in terms of the number of parameters. Additionally, the\nself-attention mechanism in the underlying architecture of LLMs, Transformers,\nhas quadratic complexity in terms of both computation and memory with respect\nto the input sequence length. For these reasons, LLM inference is\nresource-intensive, and thus, the throughput of LLM inference is limited,\nespecially for the longer sequences. In this report, we design a collaborative\ninference architecture between a server and its clients to alleviate the\nthroughput limit. In this design, we consider the available resources on both\nsides, i.e., the computation and communication costs. We develop a dynamic\nprogramming-based algorithm to optimally allocate computation between the\nserver and the client device to increase the server throughput, while not\nviolating the service level agreement (SLA). We show in the experiments that we\nare able to efficiently distribute the workload allowing for roughly 1/3\nreduction in the server workload, while achieving 19 percent improvement over a\ngreedy method. As a result, we are able to demonstrate that, in an environment\nwith different types of LLM inference requests, the throughput of the server is\nimproved.\n","authors":["Akrit Mudvari","Yuang Jiang","Leandros Tassiulas"],"pdf_url":"https://arxiv.org/pdf/2410.10759v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18567v2","updated":"2024-10-16T16:26:16Z","published":"2024-02-28T18:57:56Z","title":"Diffusion Language Models Are Versatile Protein Learners","summary":"  This paper introduces diffusion protein language model (DPLM), a versatile\nprotein language model that demonstrates strong generative and predictive\ncapabilities for protein sequences. We first pre-train scalable DPLMs from\nevolutionary-scale protein sequences within a generative self-supervised\ndiscrete diffusion probabilistic framework, which generalizes language modeling\nfor proteins in a principled way. After pre-training, DPLM exhibits the ability\nto generate structurally plausible, novel, and diverse protein sequences for\nunconditional generation. We further demonstrate the proposed diffusion\ngenerative pre-training makes DPLM possess a better understanding of proteins,\nmaking it a superior representation learner, which can be fine-tuned for\nvarious predictive tasks, comparing favorably to ESM2 (Lin et al., 2022).\nMoreover, DPLM can be tailored for various needs, which showcases its prowess\nof conditional generation in several ways: (1) conditioning on partial peptide\nsequences, e.g., generating scaffolds for functional motifs with high success\nrate; (2) incorporating other modalities as conditioner, e.g.,\nstructure-conditioned generation for inverse folding; and (3) steering sequence\ngeneration towards desired properties, e.g., satisfying specified secondary\nstructures, through a plug-and-play classifier guidance. Code is released at\n\\url{https://github.com/bytedance/dplm}.\n","authors":["Xinyou Wang","Zaixiang Zheng","Fei Ye","Dongyu Xue","Shujian Huang","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2402.18567v2.pdf","comment":"ICML 2024 camera-ready version"},{"id":"http://arxiv.org/abs/2407.20126v2","updated":"2024-10-16T16:20:44Z","published":"2024-07-29T15:55:52Z","title":"Extreme time extrapolation capabilities and thermodynamic consistency of\n  physics-inspired Neural Networks for the 3D microstructure evolution of\n  materials via Cahn-Hilliard flow","summary":"  A Convolutional Recurrent Neural Network (CRNN) is trained to reproduce the\nevolution of the spinodal decomposition process in three dimensions as\ndescribed by the Cahn-Hilliard equation. A specialized, physics-inspired\narchitecture is proven to provide close accordance between the predicted\nevolutions and the ground truth ones obtained via conventional integration\nschemes. The method can accurately reproduce the evolution of microstructures\nnot represented in the training set at a fraction of the computational costs.\nExtremely long-time extrapolation capabilities are achieved, up to reaching the\ntheoretically expected equilibrium state of the system, consisting of a\nlayered, phase-separated morphology, despite the training set containing only\nrelatively-short, initial phases of the evolution. Quantitative accordance with\nthe decay rate of the Free energy is also demonstrated up to the late\ncoarsening stages, proving that this class of Machine Learning approaches can\nbecome a new and powerful tool for the long timescale and high throughput\nsimulation of materials, while retaining thermodynamic consistency and\nhigh-accuracy.\n","authors":["Daniele Lanzoni","Andrea Fantasia","Roberto Bergamaschini","Olivier Pierre-Louis","Francesco Montalenti"],"pdf_url":"https://arxiv.org/pdf/2407.20126v2.pdf","comment":"12 pages, 6 main text figures, 2 appendix figures, 1 supplementary\n  material figure"},{"id":"http://arxiv.org/abs/2410.12713v1","updated":"2024-10-16T16:20:07Z","published":"2024-10-16T16:20:07Z","title":"How Does Variance Shape the Regret in Contextual Bandits?","summary":"  We consider realizable contextual bandits with general function\napproximation, investigating how small reward variance can lead to\nbetter-than-minimax regret bounds. Unlike in minimax bounds, we show that the\neluder dimension $d_\\text{elu}$$-$a complexity measure of the function\nclass$-$plays a crucial role in variance-dependent bounds. We consider two\ntypes of adversary:\n  (1) Weak adversary: The adversary sets the reward variance before observing\nthe learner's action. In this setting, we prove that a regret of\n$\\Omega(\\sqrt{\\min\\{A,d_\\text{elu}\\}\\Lambda}+d_\\text{elu})$ is unavoidable when\n$d_{\\text{elu}}\\leq\\sqrt{AT}$, where $A$ is the number of actions, $T$ is the\ntotal number of rounds, and $\\Lambda$ is the total variance over $T$ rounds.\nFor the $A\\leq d_\\text{elu}$ regime, we derive a nearly matching upper bound\n$\\tilde{O}(\\sqrt{A\\Lambda}+d_\\text{elu})$ for the special case where the\nvariance is revealed at the beginning of each round.\n  (2) Strong adversary: The adversary sets the reward variance after observing\nthe learner's action. We show that a regret of\n$\\Omega(\\sqrt{d_\\text{elu}\\Lambda}+d_\\text{elu})$ is unavoidable when\n$\\sqrt{d_\\text{elu}\\Lambda}+d_\\text{elu}\\leq\\sqrt{AT}$. In this setting, we\nprovide an upper bound of order\n$\\tilde{O}(d_\\text{elu}\\sqrt{\\Lambda}+d_\\text{elu})$.\n  Furthermore, we examine the setting where the function class additionally\nprovides distributional information of the reward, as studied by Wang et al.\n(2024). We demonstrate that the regret bound\n$\\tilde{O}(\\sqrt{d_\\text{elu}\\Lambda}+d_\\text{elu})$ established in their work\nis unimprovable when $\\sqrt{d_{\\text{elu}}\\Lambda}+d_\\text{elu}\\leq\\sqrt{AT}$.\nHowever, with a slightly different definition of the total variance and with\nthe assumption that the reward follows a Gaussian distribution, one can achieve\na regret of $\\tilde{O}(\\sqrt{A\\Lambda}+d_\\text{elu})$.\n","authors":["Zeyu Jia","Jian Qian","Alexander Rakhlin","Chen-Yu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.12713v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.12712v1","updated":"2024-10-16T16:17:21Z","published":"2024-10-16T16:17:21Z","title":"On the sample complexity of purity and inner product estimation","summary":"  We study the sample complexity of the prototypical tasks quantum purity\nestimation and quantum inner product estimation. In purity estimation, we are\nto estimate $tr(\\rho^2)$ of an unknown quantum state $\\rho$ to additive error\n$\\epsilon$. Meanwhile, for quantum inner product estimation, Alice and Bob are\nto estimate $tr(\\rho\\sigma)$ to additive error $\\epsilon$ given copies of\nunknown quantum state $\\rho$ and $\\sigma$ using classical communication and\nrestricted quantum communication.\n  In this paper, we show a strong connection between the sample complexity of\npurity estimation with bounded quantum memory and inner product estimation with\nbounded quantum communication and unentangled measurements. We propose a\nprotocol that solves quantum inner product estimation with $k$-qubit one-way\nquantum communication and unentangled local measurements using\n$O(median\\{1/\\epsilon^2,2^{n/2}/\\epsilon,2^{n-k}/\\epsilon^2\\})$ copies of\n$\\rho$ and $\\sigma$. Our protocol can be modified to estimate the purity of an\nunknown quantum state $\\rho$ using $k$-qubit quantum memory with the same\ncomplexity. We prove that arbitrary protocols with $k$-qubit quantum memory\nthat estimate purity to error $\\epsilon$ require\n$\\Omega(median\\{1/\\epsilon^2,2^{n/2}/\\sqrt{\\epsilon},2^{n-k}/\\epsilon^2\\})$\ncopies of $\\rho$. This indicates the same lower bound for quantum inner product\nestimation with one-way $k$-qubit quantum communication and classical\ncommunication, and unentangled local measurements. For purity estimation, we\nfurther improve the lower bound to\n$\\Omega(\\max\\{1/\\epsilon^2,2^{n/2}/\\epsilon\\})$ for any protocols using an\nidentical single-copy projection-valued measurement.\n  Additionally, we investigate a decisional variant of quantum distributed\ninner product estimation without quantum communication for mixed state and\nprovide a lower bound on the sample complexity.\n","authors":["Weiyuan Gong","Jonas Haferkamp","Qi Ye","Zhihan Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12712v1.pdf","comment":"33 pages, 1 figure"},{"id":"http://arxiv.org/abs/2407.00463v5","updated":"2024-10-16T16:13:32Z","published":"2024-06-29T15:20:11Z","title":"Open-Source Conversational AI with SpeechBrain 1.0","summary":"  SpeechBrain is an open-source Conversational AI toolkit based on PyTorch,\nfocused particularly on speech processing tasks such as speech recognition,\nspeech enhancement, speaker recognition, text-to-speech, and much more. It\npromotes transparency and replicability by releasing both the pre-trained\nmodels and the complete \"recipes\" of code and algorithms required for training\nthem. This paper presents SpeechBrain 1.0, a significant milestone in the\nevolution of the toolkit, which now has over 200 recipes for speech, audio, and\nlanguage processing tasks, and more than 100 models available on Hugging Face.\nSpeechBrain 1.0 introduces new technologies to support diverse learning\nmodalities, Large Language Model (LLM) integration, and advanced decoding\nstrategies, along with novel models, tasks, and modalities. It also includes a\nnew benchmark repository, offering researchers a unified platform for\nevaluating models across diverse tasks.\n","authors":["Mirco Ravanelli","Titouan Parcollet","Adel Moumen","Sylvain de Langen","Cem Subakan","Peter Plantinga","Yingzhi Wang","Pooneh Mousavi","Luca Della Libera","Artem Ploujnikov","Francesco Paissan","Davide Borra","Salah Zaiem","Zeyu Zhao","Shucong Zhang","Georgios Karakasidis","Sung-Lin Yeh","Pierre Champion","Aku Rouhe","Rudolf Braun","Florian Mai","Juan Zuluaga-Gomez","Seyed Mahed Mousavi","Andreas Nautsch","Xuechen Liu","Sangeet Sagar","Jarod Duret","Salima Mdhaffar","Gaelle Laperriere","Mickael Rouvier","Renato De Mori","Yannick Esteve"],"pdf_url":"https://arxiv.org/pdf/2407.00463v5.pdf","comment":"Accepted to the Journal of Machine Learning research (JMLR), Machine\n  Learning Open Source Software"},{"id":"http://arxiv.org/abs/2410.12707v1","updated":"2024-10-16T16:13:19Z","published":"2024-10-16T16:13:19Z","title":"FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs\n  with Adaptive Compression","summary":"  To alleviate hardware scarcity in training large deep neural networks (DNNs),\nparticularly large language models (LLMs), we present FusionLLM, a\ndecentralized training system designed and implemented for training DNNs using\ngeo-distributed GPUs across different computing clusters or individual devices.\nDecentralized training faces significant challenges regarding system design and\nefficiency, including: 1) the need for remote automatic differentiation (RAD),\n2) support for flexible model definitions and heterogeneous software, 3)\nheterogeneous hardware leading to low resource utilization or the straggler\nproblem, and 4) slow network communication. To address these challenges, in the\nsystem design, we represent the model as a directed acyclic graph of operators\n(OP-DAG). Each node in the DAG represents the operator in the DNNs, while the\nedge represents the data dependency between operators. Based on this design, 1)\nusers are allowed to customize any DNN without caring low-level operator\nimplementation; 2) we enable the task scheduling with the more fine-grained\nsub-tasks, offering more optimization space; 3) a DAG runtime executor can\nimplement RAD withour requiring the consistent low-level ML framework versions.\n  To enhance system efficiency, we implement a workload estimator and design an\nOP-Fence scheduler to cluster devices with similar bandwidths together and\npartition the DAG to increase throughput. Additionally, we propose an AdaTopK\ncompressor to adaptively compress intermediate activations and gradients at the\nslowest communication links. To evaluate the convergence and efficiency of our\nsystem and algorithms, we train ResNet-101 and GPT-2 on three real-world\ntestbeds using 48 GPUs connected with 8 Mbps~10 Gbps networks. Experimental\nresults demonstrate that our system and method can achieve 1.45 - 9.39x speedup\ncompared to baseline methods while ensuring convergence.\n","authors":["Zhenheng Tang","Xueze Kang","Yiming Yin","Xinglin Pan","Yuxin Wang","Xin He","Qiang Wang","Rongfei Zeng","Kaiyong Zhao","Shaohuai Shi","Amelie Chi Zhou","Bo Li","Bingsheng He","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2410.12707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14106v2","updated":"2024-10-16T16:13:01Z","published":"2024-05-23T02:24:52Z","title":"Nearly Tight Black-Box Auditing of Differentially Private Machine\n  Learning","summary":"  This paper presents an auditing procedure for the Differentially Private\nStochastic Gradient Descent (DP-SGD) algorithm in the black-box threat model\nthat is substantially tighter than prior work. The main intuition is to craft\nworst-case initial model parameters, as DP-SGD's privacy analysis is agnostic\nto the choice of the initial model parameters. For models trained on MNIST and\nCIFAR-10 at theoretical $\\varepsilon=10.0$, our auditing procedure yields\nempirical estimates of $\\varepsilon_{emp} = 7.21$ and $6.95$, respectively, on\na 1,000-record sample and $\\varepsilon_{emp}= 6.48$ and $4.96$ on the full\ndatasets. By contrast, previous audits were only (relatively) tight in stronger\nwhite-box models, where the adversary can access the model's inner parameters\nand insert arbitrary gradients. Overall, our auditing procedure can offer\nvaluable insight into how the privacy analysis of DP-SGD could be improved and\ndetect bugs and DP violations in real-world implementations. The source code\nneeded to reproduce our experiments is available at\nhttps://github.com/spalabucr/bb-audit-dpsgd.\n","authors":["Meenatchi Sundaram Muthu Selva Annamalai","Emiliano De Cristofaro"],"pdf_url":"https://arxiv.org/pdf/2405.14106v2.pdf","comment":"To appear in the Proceedings of the Thirty-eighth Annual Conference\n  on Neural Information Processing Systems (NeurIPS 2024). Please cite\n  accordingly"},{"id":"http://arxiv.org/abs/2410.12704v1","updated":"2024-10-16T16:10:59Z","published":"2024-10-16T16:10:59Z","title":"Sarcasm Detection in a Less-Resourced Language","summary":"  The sarcasm detection task in natural language processing tries to classify\nwhether an utterance is sarcastic or not. It is related to sentiment analysis\nsince it often inverts surface sentiment. Because sarcastic sentences are\nhighly dependent on context, and they are often accompanied by various\nnon-verbal cues, the task is challenging. Most of related work focuses on\nhigh-resourced languages like English. To build a sarcasm detection dataset for\na less-resourced language, such as Slovenian, we leverage two modern\ntechniques: a machine translation specific medium-size transformer model, and a\nvery large generative language model. We explore the viability of translated\ndatasets and how the size of a pretrained transformer affects its ability to\ndetect sarcasm. We train ensembles of detection models and evaluate models'\nperformance. The results show that larger models generally outperform smaller\nones and that ensembling can slightly improve sarcasm detection performance.\nOur best ensemble approach achieves an $\\text{F}_1$-score of 0.765 which is\nclose to annotators' agreement in the source language.\n","authors":["Lazar Đoković","Marko Robnik-Šikonja"],"pdf_url":"https://arxiv.org/pdf/2410.12704v1.pdf","comment":"4 pages, published in the Slovenian Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2310.04727v2","updated":"2024-10-16T16:06:17Z","published":"2023-10-07T07:55:22Z","title":"Task Aware Modulation using Representation Learning: An Approach for Few\n  Shot Learning in Environmental Systems","summary":"  We introduce TAM-RL (Task Aware Modulation using Representation Learning), a\nnovel multimodal meta-learning framework for few-shot learning in heterogeneous\nsystems, designed for science and engineering problems where entities share a\ncommon underlying forward model but exhibit heterogeneity due to\nentity-specific characteristics. TAM-RL leverages an amortized training process\nwith a modulation network and a base network to learn task-specific modulation\nparameters, enabling efficient adaptation to new tasks with limited data. We\nevaluate TAM-RL on two real-world environmental datasets: Gross Primary Product\n(GPP) prediction and streamflow forecasting, demonstrating significant\nimprovements over existing meta-learning methods. On the FLUXNET dataset,\nTAM-RL improves RMSE by 18.9\\% over MMAML with just one month of few-shot data,\nwhile for streamflow prediction, it achieves an 8.21\\% improvement with one\nyear of data. Synthetic data experiments further validate TAM-RL's superior\nperformance in heterogeneous task distributions, outperforming the baselines in\nthe most heterogeneous setting. Notably, TAM-RL offers substantial\ncomputational efficiency, with at least 3x faster training times compared to\ngradient-based meta-learning approaches while being much simpler to train due\nto reduced complexity. Ablation studies highlight the importance of pretraining\nand adaptation mechanisms in TAM-RL's performance.\n","authors":["Arvind Renganathan","Rahul Ghosh","Ankush Khandelwal","Vipin Kumar"],"pdf_url":"https://arxiv.org/pdf/2310.04727v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12703v1","updated":"2024-10-16T16:05:46Z","published":"2024-10-16T16:05:46Z","title":"Neural-based Control for CubeSat Docking Maneuvers","summary":"  Autonomous Rendezvous and Docking (RVD) have been extensively studied in\nrecent years, addressing the stringent requirements of spacecraft dynamics\nvariations and the limitations of GNC systems. This paper presents an\ninnovative approach employing Artificial Neural Networks (ANN) trained through\nReinforcement Learning (RL) for autonomous spacecraft guidance and control\nduring the final phase of the rendezvous maneuver. The proposed strategy is\neasily implementable onboard and offers fast adaptability and robustness to\ndisturbances by learning control policies from experience rather than relying\non predefined models. Extensive Monte Carlo simulations within a relevant\nenvironment are conducted in 6DoF settings to validate our approach, along with\nhardware tests that demonstrate deployment feasibility. Our findings highlight\nthe efficacy of RL in assuring the adaptability and efficiency of spacecraft\nRVD, offering insights into future mission expectations.\n","authors":["Matteo Stoisa","Federica Paganelli Azza","Luca Romanelli","Mattia Varile"],"pdf_url":"https://arxiv.org/pdf/2410.12703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12700v1","updated":"2024-10-16T16:03:42Z","published":"2024-10-16T16:03:42Z","title":"Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via\n  Lightweight Value Optimization","summary":"  Recent advancements in diffusion models trained on large-scale data have\nenabled the generation of indistinguishable human-level images, yet they often\nproduce harmful content misaligned with human values, e.g., social bias, and\noffensive content. Despite extensive research on Large Language Models (LLMs),\nthe challenge of Text-to-Image (T2I) model alignment remains largely\nunexplored. Addressing this problem, we propose LiVO (Lightweight Value\nOptimization), a novel lightweight method for aligning T2I models with human\nvalues. LiVO only optimizes a plug-and-play value encoder to integrate a\nspecified value principle with the input prompt, allowing the control of\ngenerated images over both semantics and values. Specifically, we design a\ndiffusion model-tailored preference optimization loss, which theoretically\napproximates the Bradley-Terry model used in LLM alignment but provides a more\nflexible trade-off between image quality and value conformity. To optimize the\nvalue encoder, we also develop a framework to automatically construct a\ntext-image preference dataset of 86k (prompt, aligned image, violating image,\nvalue principle) samples. Without updating most model parameters and through\nadaptive value selection from the input prompt, LiVO significantly reduces\nharmful outputs and achieves faster convergence, surpassing several strong\nbaselines and taking an initial step towards ethically aligned T2I models.\n","authors":["Xingqi Wang","Xiaoyuan Yi","Xing Xie","Jia Jia"],"pdf_url":"https://arxiv.org/pdf/2410.12700v1.pdf","comment":"Accepted by ACM Multimedia 2024. The dataset and code can be found at\n  https://github.com/achernarwang/LiVO"},{"id":"http://arxiv.org/abs/2410.09838v2","updated":"2024-10-16T15:59:19Z","published":"2024-10-13T13:37:36Z","title":"Uncovering, Explaining, and Mitigating the Superficial Safety of\n  Backdoor Defense","summary":"  Backdoor attacks pose a significant threat to Deep Neural Networks (DNNs) as\nthey allow attackers to manipulate model predictions with backdoor triggers. To\naddress these security vulnerabilities, various backdoor purification methods\nhave been proposed to purify compromised models. Typically, these purified\nmodels exhibit low Attack Success Rates (ASR), rendering them resistant to\nbackdoored inputs. However, Does achieving a low ASR through current safety\npurification methods truly eliminate learned backdoor features from the\npretraining phase? In this paper, we provide an affirmative answer to this\nquestion by thoroughly investigating the Post-Purification Robustness of\ncurrent backdoor purification methods. We find that current safety purification\nmethods are vulnerable to the rapid re-learning of backdoor behavior, even when\nfurther fine-tuning of purified models is performed using a very small number\nof poisoned samples. Based on this, we further propose the practical\nQuery-based Reactivation Attack (QRA) which could effectively reactivate the\nbackdoor by merely querying purified models. We find the failure to achieve\nsatisfactory post-purification robustness stems from the insufficient deviation\nof purified models from the backdoored model along the backdoor-connected path.\nTo improve the post-purification robustness, we propose a straightforward\ntuning defense, Path-Aware Minimization (PAM), which promotes deviation along\nbackdoor-connected paths with extra model updates. Extensive experiments\ndemonstrate that PAM significantly improves post-purification robustness while\nmaintaining a good clean accuracy and low ASR. Our work provides a new\nperspective on understanding the effectiveness of backdoor safety tuning and\nhighlights the importance of faithfully assessing the model's safety.\n","authors":["Rui Min","Zeyu Qin","Nevin L. Zhang","Li Shen","Minhao Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.09838v2.pdf","comment":"NeurIPS 2024 Spotlight paper. The first two authors contributed\n  equally"},{"id":"http://arxiv.org/abs/2405.16012v2","updated":"2024-10-16T15:57:03Z","published":"2024-05-25T02:30:46Z","title":"Pessimistic Backward Policy for GFlowNets","summary":"  This paper studies Generative Flow Networks (GFlowNets), which learn to\nsample objects proportionally to a given reward function through the trajectory\nof state transitions. In this work, we observe that GFlowNets tend to\nunder-exploit the high-reward objects due to training on insufficient number of\ntrajectories, which may lead to a large gap between the estimated flow and the\n(known) reward value. In response to this challenge, we propose a pessimistic\nbackward policy for GFlowNets (PBP-GFN), which maximizes the observed flow to\nalign closely with the true reward for the object. We extensively evaluate\nPBP-GFN across eight benchmarks, including hyper-grid environment, bag\ngeneration, structured set generation, molecular generation, and four RNA\nsequence generation tasks. In particular, PBP-GFN enhances the discovery of\nhigh-reward objects, maintains the diversity of the objects, and consistently\noutperforms existing methods.\n","authors":["Hyosoon Jang","Yunhui Jang","Minsu Kim","Jinkyoo Park","Sungsoo Ahn"],"pdf_url":"https://arxiv.org/pdf/2405.16012v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12692v1","updated":"2024-10-16T15:52:32Z","published":"2024-10-16T15:52:32Z","title":"Machine Learning Approach to Brain Tumor Detection and Classification","summary":"  Brain tumor detection and classification are critical tasks in medical image\nanalysis, particularly in early-stage diagnosis, where accurate and timely\ndetection can significantly improve treatment outcomes. In this study, we apply\nvarious statistical and machine learning models to detect and classify brain\ntumors using brain MRI images. We explore a variety of statistical models\nincluding linear, logistic, and Bayesian regressions, and the machine learning\nmodels including decision tree, random forest, single-layer perceptron,\nmulti-layer perceptron, convolutional neural network (CNN), recurrent neural\nnetwork, and long short-term memory. Our findings show that CNN outperforms\nother models, achieving the best performance. Additionally, we confirm that the\nCNN model can also work for multi-class classification, distinguishing between\nfour categories of brain MRI images such as normal, glioma, meningioma, and\npituitary tumor images. This study demonstrates that machine learning\napproaches are suitable for brain tumor detection and classification,\nfacilitating real-world medical applications in assisting radiologists with\nearly and accurate diagnosis.\n","authors":["Alice Oh","Inyoung Noh","Jian Choo","Jihoo Lee","Justin Park","Kate Hwang","Sanghyeon Kim","Soo Min Oh"],"pdf_url":"https://arxiv.org/pdf/2410.12692v1.pdf","comment":"7 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.12690v1","updated":"2024-10-16T15:50:57Z","published":"2024-10-16T15:50:57Z","title":"Local transfer learning Gaussian process modeling, with applications to\n  surrogate modeling of expensive computer simulators","summary":"  A critical bottleneck for scientific progress is the costly nature of\ncomputer simulations for complex systems. Surrogate models provide an appealing\nsolution: such models are trained on simulator evaluations, then used to\nemulate and quantify uncertainty on the expensive simulator at unexplored\ninputs. In many applications, one often has available data on related systems.\nFor example, in designing a new jet turbine, there may be existing studies on\nturbines with similar configurations. A key question is how information from\nsuch \"source\" systems can be transferred for effective surrogate training on\nthe \"target\" system of interest. We thus propose a new LOcal transfer Learning\nGaussian Process (LOL-GP) model, which leverages a carefully-designed Gaussian\nprocess to transfer such information for surrogate modeling. The key novelty of\nthe LOL-GP is a latent regularization model, which identifies regions where\ntransfer should be performed and regions where it should be avoided. This\n\"local transfer\" property is desirable in scientific systems: at certain\nparameters, such systems may behave similarly and thus transfer is beneficial;\nat other parameters, they may behave differently and thus transfer is\ndetrimental. By accounting for local transfer, the LOL-GP can rectify a\ncritical limitation of \"negative transfer\" in existing transfer learning\nmodels, where the transfer of information worsens predictive performance. We\nderive a Gibbs sampling algorithm for efficient posterior predictive sampling\non the LOL-GP, for both the multi-source and multi-fidelity transfer settings.\nWe then show, via a suite of numerical experiments and an application for jet\nturbine design, the improved surrogate performance of the LOL-GP over existing\nmethods.\n","authors":["Xinming Wang","Simon Mak","John Miller","Jianguo Wu"],"pdf_url":"https://arxiv.org/pdf/2410.12690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12689v1","updated":"2024-10-16T15:49:25Z","published":"2024-10-16T15:49:25Z","title":"A distance function for stochastic matrices","summary":"  Motivated by information geometry, a distance function on the space of\nstochastic matrices is advocated. Starting with sequences of Markov chains the\nBhattacharyya angle is advocated as the natural tool for comparing both short\nand long term Markov chain runs. Bounds on the convergence of the distance and\nmixing times are derived. Guided by the desire to compare different Markov\nchain models, especially in the setting of healthcare processes, a new distance\nfunction on the space of stochastic matrices is presented. It is a true\ndistance measure which has a closed form and is efficient to implement for\nnumerical evaluation. In the case of ergodic Markov chains, it is shown that\nconsidering either the Bhattacharyya angle on Markov sequences or the new\nstochastic matrix distance leads to the same distance between models.\n","authors":["Antony Lee","Peter Tino","Iain Bruce Styles"],"pdf_url":"https://arxiv.org/pdf/2410.12689v1.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.12686v1","updated":"2024-10-16T15:48:28Z","published":"2024-10-16T15:48:28Z","title":"Automatic Mapping of Anatomical Landmarks from Free-Text Using Large\n  Language Models: Insights from Llama-2","summary":"  Anatomical landmarks are vital in medical imaging for navigation and anomaly\ndetection. Modern large language models (LLMs), like Llama-2, offer promise for\nautomating the mapping of these landmarks in free-text radiology reports to\ncorresponding positions in image data. Recent studies propose LLMs may develop\ncoherent representations of generative processes. Motivated by these insights,\nwe investigated whether LLMs accurately represent the spatial positions of\nanatomical landmarks. Through experiments with Llama-2 models, we found that\nthey can linearly represent anatomical landmarks in space with considerable\nrobustness to different prompts. These results underscore the potential of LLMs\nto enhance the efficiency and accuracy of medical imaging workflows.\n","authors":["Mohamad Abdi","Gerardo Hemosillo Valadez","Halid Ziya Yerebakan"],"pdf_url":"https://arxiv.org/pdf/2410.12686v1.pdf","comment":"6 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.12683v1","updated":"2024-10-16T15:46:48Z","published":"2024-10-16T15:46:48Z","title":"Generative Neural Reparameterization for Differentiable PDE-constrained\n  Optimization","summary":"  Partial-differential-equation (PDE)-constrained optimization is a well-worn\ntechnique for acquiring optimal parameters of systems governed by PDEs.\nHowever, this approach is limited to providing a single set of optimal\nparameters per optimization. Given a differentiable PDE solver, if the free\nparameters are reparameterized as the output of a neural network, that neural\nnetwork can be trained to learn a map from a probability distribution to the\ndistribution of optimal parameters. This proves useful in the case where there\nare many well performing local minima for the PDE. We apply this technique to\ntrain a neural network that generates optimal parameters that minimize\nlaser-plasma instabilities relevant to laser fusion and show that the neural\nnetwork generates many well performing and diverse minima.\n","authors":["Archis S. Joglekar"],"pdf_url":"https://arxiv.org/pdf/2410.12683v1.pdf","comment":"Accepted to D3S3: Data-driven and Differentiable Simulations,\n  Surrogates, and Solvers - Workshop @ NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.12679v1","updated":"2024-10-16T15:44:15Z","published":"2024-10-16T15:44:15Z","title":"Optimizing Multi-Task Learning for Accurate Spacecraft Pose Estimation","summary":"  Accurate satellite pose estimation is crucial for autonomous guidance,\nnavigation, and control (GNC) systems in in-orbit servicing (IOS) missions.\nThis paper explores the impact of different tasks within a multi-task learning\n(MTL) framework for satellite pose estimation using monocular images. By\nintegrating tasks such as direct pose estimation, keypoint prediction, object\nlocalization, and segmentation into a single network, the study aims to\nevaluate the reciprocal influence between tasks by testing different multi-task\nconfigurations thanks to the modularity of the convolutional neural network\n(CNN) used in this work. The trends of mutual bias between the analyzed tasks\nare found by employing different weighting strategies to further test the\nrobustness of the findings. A synthetic dataset was developed to train and test\nthe MTL network. Results indicate that direct pose estimation and heatmap-based\npose estimation positively influence each other in general, while both the\nbounding box and segmentation tasks do not provide significant contributions\nand tend to degrade the overall estimation accuracy.\n","authors":["Francesco Evangelisti","Francesco Rossi","Tobia Giani","Ilaria Bloise","Mattia Varile"],"pdf_url":"https://arxiv.org/pdf/2410.12679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12677v1","updated":"2024-10-16T15:41:08Z","published":"2024-10-16T15:41:08Z","title":"Efficient Optimization Algorithms for Linear Adversarial Training","summary":"  Adversarial training can be used to learn models that are robust against\nperturbations. For linear models, it can be formulated as a convex optimization\nproblem. Compared to methods proposed in the context of deep learning,\nleveraging the optimization structure allows significantly faster convergence\nrates. Still, the use of generic convex solvers can be inefficient for\nlarge-scale problems. Here, we propose tailored optimization algorithms for the\nadversarial training of linear models, which render large-scale regression and\nclassification problems more tractable. For regression problems, we propose a\nfamily of solvers based on iterative ridge regression and, for classification,\na family of solvers based on projected gradient descent. The methods are based\non extended variable reformulations of the original problem. We illustrate\ntheir efficiency in numerical examples.\n","authors":["Antônio H. RIbeiro","Thomas B. Schön","Dave Zahariah","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2410.12677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12672v1","updated":"2024-10-16T15:36:13Z","published":"2024-10-16T15:36:13Z","title":"Context Matters: Leveraging Contextual Features for Time Series\n  Forecasting","summary":"  Time series forecasts are often influenced by exogenous contextual features\nin addition to their corresponding history. For example, in financial settings,\nit is hard to accurately predict a stock price without considering public\nsentiments and policy decisions in the form of news articles, tweets, etc.\nThough this is common knowledge, the current state-of-the-art (SOTA)\nforecasting models fail to incorporate such contextual information, owing to\nits heterogeneity and multimodal nature. To address this, we introduce\nContextFormer, a novel plug-and-play method to surgically integrate multimodal\ncontextual information into existing pre-trained forecasting models.\nContextFormer effectively distills forecast-specific information from rich\nmultimodal contexts, including categorical, continuous, time-varying, and even\ntextual information, to significantly enhance the performance of existing base\nforecasters. ContextFormer outperforms SOTA forecasting models by up to 30% on\na range of real-world datasets spanning energy, traffic, environmental, and\nfinancial domains.\n","authors":["Sameep Chattopadhyay","Pulkit Paliwal","Sai Shankar Narasimhan","Shubhankar Agarwal","Sandeep P. Chinchali"],"pdf_url":"https://arxiv.org/pdf/2410.12672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12671v1","updated":"2024-10-16T15:36:10Z","published":"2024-10-16T15:36:10Z","title":"New Paradigm of Adversarial Training: Breaking Inherent Trade-Off\n  between Accuracy and Robustness via Dummy Classes","summary":"  Adversarial Training (AT) is one of the most effective methods to enhance the\nrobustness of DNNs. However, existing AT methods suffer from an inherent\ntrade-off between adversarial robustness and clean accuracy, which seriously\nhinders their real-world deployment. While this problem has been widely studied\nwithin the current AT paradigm, existing AT methods still typically experience\na reduction in clean accuracy by over 10% to date, without significant\nimprovements in robustness compared with simple baselines like PGD-AT. This\ninherent trade-off raises a question: whether the current AT paradigm, which\nassumes to learn the corresponding benign and adversarial samples as the same\nclass, inappropriately combines clean and robust objectives that may be\nessentially inconsistent. In this work, we surprisingly reveal that up to 40%\nof CIFAR-10 adversarial samples always fail to satisfy such an assumption\nacross various AT methods and robust models, explicitly indicating the\nimprovement room for the current AT paradigm. Accordingly, to relax the tension\nbetween clean and robust learning derived from this overstrict assumption, we\npropose a new AT paradigm by introducing an additional dummy class for each\noriginal class, aiming to accommodate the hard adversarial samples with shifted\ndistribution after perturbation. The robustness w.r.t. these adversarial\nsamples can be achieved by runtime recovery from the predicted dummy classes to\ntheir corresponding original ones, eliminating the compromise with clean\nlearning. Building on this new paradigm, we propose a novel plug-and-play AT\ntechnology named DUmmy Classes-based Adversarial Training (DUCAT). Extensive\nexperiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that the\nDUCAT concurrently improves clean accuracy and adversarial robustness compared\nwith state-of-the-art benchmarks, effectively breaking the existing inherent\ntrade-off.\n","authors":["Yanyun Wang","Li Liu","Zi Liang","Qingqing Ye","Haibo Hu"],"pdf_url":"https://arxiv.org/pdf/2410.12671v1.pdf","comment":"Preprint. Work in progress. The code is available at\n  https://github.com/FlaAI/DUCAT"},{"id":"http://arxiv.org/abs/2402.07204v4","updated":"2024-10-16T15:28:18Z","published":"2024-02-11T13:30:53Z","title":"ITINERA: Integrating Spatial Optimization with Large Language Models for\n  Open-domain Urban Itinerary Planning","summary":"  Citywalk, a recently popular form of urban travel, requires genuine\npersonalization and understanding of fine-grained requests compared to\ntraditional itinerary planning. In this paper, we introduce the novel task of\nOpen-domain Urban Itinerary Planning (OUIP), which generates personalized urban\nitineraries from user requests in natural language. We then present ITINERA, an\nOUIP system that integrates spatial optimization with large language models to\nprovide customized urban itineraries based on user needs. This involves\ndecomposing user requests, selecting candidate points of interest (POIs),\nordering the POIs based on cluster-aware spatial optimization, and generating\nthe itinerary. Experiments on real-world datasets and the performance of the\ndeployed system demonstrate our system's capacity to deliver personalized and\nspatially coherent itineraries compared to current solutions. Source codes of\nITINERA are available at https://github.com/YihongT/ITINERA.\n","authors":["Yihong Tang","Zhaokai Wang","Ao Qu","Yihao Yan","Zhaofeng Wu","Dingyi Zhuang","Jushi Kai","Kebing Hou","Xiaotong Guo","Jinhua Zhao","Zhan Zhao","Wei Ma"],"pdf_url":"https://arxiv.org/pdf/2402.07204v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12657v1","updated":"2024-10-16T15:18:03Z","published":"2024-10-16T15:18:03Z","title":"Explanation-Preserving Augmentation for Semi-Supervised Graph\n  Representation Learning","summary":"  Graph representation learning (GRL), enhanced by graph augmentation methods,\nhas emerged as an effective technique achieving performance improvements in\nwide tasks such as node classification and graph classification. In\nself-supervised GRL, paired graph augmentations are generated from each graph.\nIts objective is to infer similar representations for augmentations of the same\ngraph, but maximally distinguishable representations for augmentations of\ndifferent graphs. Analogous to image and language domains, the desiderata of an\nideal augmentation method include both (1) semantics-preservation; and (2)\ndata-perturbation; i.e., an augmented graph should preserve the semantics of\nits original graph while carrying sufficient variance. However, most existing\n(un-)/self-supervised GRL methods focus on data perturbation but largely\nneglect semantics preservation. To address this challenge, in this paper, we\npropose a novel method, Explanation-Preserving Augmentation (EPA), that\nleverages graph explanation techniques for generating augmented graphs that can\nbridge the gap between semantics-preservation and data-perturbation. EPA first\nuses a small number of labels to train a graph explainer to infer the\nsub-structures (explanations) that are most relevant to a graph's semantics.\nThese explanations are then used to generate semantics-preserving augmentations\nfor self-supervised GRL, namely EPA-GRL. We demonstrate theoretically, using an\nanalytical example, and through extensive experiments on a variety of benchmark\ndatasets that EPA-GRL outperforms the state-of-the-art (SOTA) GRL methods,\nwhich are built upon semantics-agnostic data augmentations.\n","authors":["Zhuomin Chen","Jingchao Ni","Hojat Allah Salehi","Xu Zheng","Esteban Schafir","Farhad Shirani","Dongsheng Luo"],"pdf_url":"https://arxiv.org/pdf/2410.12657v1.pdf","comment":"16 pages, 7 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.12655v1","updated":"2024-10-16T15:16:50Z","published":"2024-10-16T15:16:50Z","title":"Position Specific Scoring Is All You Need? Revisiting Protein Sequence\n  Classification Tasks","summary":"  Understanding the structural and functional characteristics of proteins are\ncrucial for developing preventative and curative strategies that impact fields\nfrom drug discovery to policy development. An important and popular technique\nfor examining how amino acids make up these characteristics of the protein\nsequences with position-specific scoring (PSS). While the string kernel is\ncrucial in natural language processing (NLP), it is unclear if string kernels\ncan extract biologically meaningful information from protein sequences, despite\nthe fact that they have been shown to be effective in the general sequence\nanalysis tasks. In this work, we propose a weighted PSS kernel matrix (or\nW-PSSKM), that combines a PSS representation of protein sequences, which\nencodes the frequency information of each amino acid in a sequence, with the\nnotion of the string kernel. This results in a novel kernel function that\noutperforms many other approaches for protein sequence classification. We\nperform extensive experimentation to evaluate the proposed method. Our findings\ndemonstrate that the W-PSSKM significantly outperforms existing baselines and\nstate-of-the-art methods and achieves up to 45.1\\% improvement in\nclassification accuracy.\n","authors":["Sarwan Ali","Taslim Murad","Prakash Chourasia","Haris Mansoor","Imdad Ullah Khan","Pin-Yu Chen","Murray Patterson"],"pdf_url":"https://arxiv.org/pdf/2410.12655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12652v1","updated":"2024-10-16T15:16:04Z","published":"2024-10-16T15:16:04Z","title":"Constrained Posterior Sampling: Time Series Generation with Hard\n  Constraints","summary":"  Generating realistic time series samples is crucial for stress-testing models\nand protecting user privacy by using synthetic data. In engineering and\nsafety-critical applications, these samples must meet certain hard constraints\nthat are domain-specific or naturally imposed by physics or nature. Consider,\nfor example, generating electricity demand patterns with constraints on peak\ndemand times. This can be used to stress-test the functioning of power grids\nduring adverse weather conditions. Existing approaches for generating\nconstrained time series are either not scalable or degrade sample quality. To\naddress these challenges, we introduce Constrained Posterior Sampling (CPS), a\ndiffusion-based sampling algorithm that aims to project the posterior mean\nestimate into the constraint set after each denoising update. Notably, CPS\nscales to a large number of constraints (~100) without requiring additional\ntraining. We provide theoretical justifications highlighting the impact of our\nprojection step on sampling. Empirically, CPS outperforms state-of-the-art\nmethods in sample quality and similarity to real time series by around 10% and\n42%, respectively, on real-world stocks, traffic, and air quality datasets.\n","authors":["Sai Shankar Narasimhan","Shubhankar Agarwal","Litu Rout","Sanjay Shakkottai","Sandeep P. Chinchali"],"pdf_url":"https://arxiv.org/pdf/2410.12652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11785v2","updated":"2024-10-16T15:15:44Z","published":"2024-06-17T17:39:10Z","title":"CELL your Model: Contrastive Explanations for Large Language Models","summary":"  The advent of black-box deep neural network classification models has sparked\nthe need to explain their decisions. However, in the case of generative AI,\nsuch as large language models (LLMs), there is no class prediction to explain.\nRather, one can ask why an LLM output a particular response to a given prompt.\nIn this paper, we answer this question by proposing, to the best of our\nknowledge, the first contrastive explanation methods requiring simply\nblack-box/query access. Our explanations suggest that an LLM outputs a reply to\na given prompt because if the prompt was slightly modified, the LLM would have\ngiven a different response that is either less preferable or contradicts the\noriginal response. The key insight is that contrastive explanations simply\nrequire a scoring function that has meaning to the user and not necessarily a\nspecific real valued quantity (viz. class label). We offer two algorithms for\nfinding contrastive explanations: i) A myopic algorithm, which although\neffective in creating contrasts, requires many model calls and ii) A budgeted\nalgorithm, our main algorithmic contribution, which intelligently creates\ncontrasts adhering to a query budget, necessary for longer contexts. We show\nthe efficacy of these methods on diverse natural language tasks such as\nopen-text generation, automated red teaming, and explaining conversational\ndegradation.\n","authors":["Ronny Luss","Erik Miehling","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2406.11785v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11720v2","updated":"2024-10-16T15:10:58Z","published":"2024-10-15T15:52:45Z","title":"Light-Weight Fault Tolerant Attention for Large Language Model Training","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks. However, the training of these\nmodels is computationally intensive and susceptible to faults, particularly in\nthe attention mechanism, which is a critical component of transformer-based\nLLMs. In this paper, we investigate the impact of faults on LLM training,\nfocusing on INF, NaN, and near-INF values in the computation results with\nsystematic fault injection experiments. We observe the propagation patterns of\nthese errors, which can trigger non-trainable states in the model and disrupt\ntraining, forcing the procedure to load from checkpoints. To mitigate the\nimpact of these faults, we propose ATTNChecker, the first Algorithm-Based Fault\nTolerance (ABFT) technique tailored for the attention mechanism in LLMs.\nATTNChecker is designed based on fault propagation patterns of LLM and\nincorporates performance optimization to adapt to both system reliability and\nmodel vulnerability while providing lightweight protection for fast LLM\ntraining. Evaluations on four LLMs show that ATTNChecker on average incurs on\naverage 7% overhead on training while detecting and correcting all extreme\nerrors. Compared with the state-of-the-art checkpoint/restore approach,\nATTNChecker reduces recovery overhead by up to 49x.\n","authors":["Yuhang Liang","Xinyi Li","Jie Ren","Ang Li","Bo Fang","Jieyang Chen"],"pdf_url":"https://arxiv.org/pdf/2410.11720v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12642v1","updated":"2024-10-16T15:03:28Z","published":"2024-10-16T15:03:28Z","title":"Optimization and Application of Cloud-based Deep Learning Architecture\n  for Multi-Source Data Prediction","summary":"  This study develops a cloud-based deep learning system for early prediction\nof diabetes, leveraging the distributed computing capabilities of the AWS cloud\nplatform and deep learning technologies to achieve efficient and accurate risk\nassessment. The system utilizes EC2 p3.8xlarge GPU instances to accelerate\nmodel training, reducing training time by 93.2% while maintaining a prediction\naccuracy of 94.2%. With an automated data processing and model training\npipeline built using Apache Airflow, the system can complete end-to-end updates\nwithin 18.7 hours. In clinical applications, the system demonstrates a\nprediction accuracy of 89.8%, sensitivity of 92.3%, and specificity of 95.1%.\nEarly interventions based on predictions lead to a 37.5% reduction in diabetes\nincidence among the target population. The system's high performance and\nscalability provide strong support for large-scale diabetes prevention and\nmanagement, showcasing significant public health value.\n","authors":["Yang Zhang","Fa Wang","Xin Huang","Xintao Li","Sibei Liu","Hansong Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12642v1.pdf","comment":"6 Pages, 5 Figures, 3 Tables. The final version will be published in\n  the proceedings of the IEEE conference"},{"id":"http://arxiv.org/abs/2202.03023v4","updated":"2024-10-16T15:02:27Z","published":"2022-02-07T09:27:34Z","title":"CECILIA: Comprehensive Secure Machine Learning Framework","summary":"  Since ML algorithms have proven their success in many different applications,\nthere is also a big interest in privacy preserving (PP) ML methods for building\nmodels on sensitive data. Moreover, the increase in the number of data sources\nand the high computational power required by those algorithms force individuals\nto outsource the training and/or the inference of a ML model to the clouds\nproviding such services. To address this, we propose a secure 3-party\ncomputation framework, CECILIA, offering PP building blocks to enable complex\noperations privately. In addition to the adapted and common operations like\naddition and multiplication, it offers multiplexer, most significant bit and\nmodulus conversion. The first two are novel in terms of methodology and the\nlast one is novel in terms of both functionality and methodology. CECILIA also\nhas two complex novel methods, which are the exact exponential of a public base\nraised to the power of a secret value and the inverse square root of a secret\nGram matrix. We use CECILIA to realize the private inference on pre-trained\nRKNs, which require more complex operations than most other DNNs, on the\nstructural classification of proteins as the first study ever accomplishing the\nPP inference on RKNs. In addition to the successful private computation of\nbasic building blocks, the results demonstrate that we perform the exact and\nfully private exponential computation, which is done by approximation in the\nliterature so far. Moreover, they also show that we compute the exact inverse\nsquare root of a secret Gram matrix up to a certain privacy level, which has\nnot been addressed in the literature at all. We also analyze the scalability of\nCECILIA to various settings on a synthetic dataset. The framework shows a great\npromise to make other ML algorithms as well as further computations privately\ncomputable by the building blocks of the framework.\n","authors":["Ali Burak Ünal","Nico Pfeifer","Mete Akgün"],"pdf_url":"https://arxiv.org/pdf/2202.03023v4.pdf","comment":"Preprint version of \"A privacy-preserving approach for cloud-based\n  protein fold recognition\" paper published in Patterns, ~8 pages of the main\n  paper, ~5 pages of Supplement"},{"id":"http://arxiv.org/abs/2408.04969v2","updated":"2024-10-16T14:57:33Z","published":"2024-08-09T09:43:10Z","title":"Towards aerodynamic surrogate modeling based on $β$-variational\n  autoencoders","summary":"  Surrogate models that combine dimensionality reduction and regression\ntechniques are essential to reduce the need for costly high-fidelity\ncomputational fluid dynamics data. New approaches using $\\beta$-Variational\nAutoencoder ($\\beta$-VAE) architectures have shown promise in obtaining\nhigh-quality low-dimensional representations of high-dimensional flow data\nwhile enabling physical interpretation of their latent spaces. We propose a\nsurrogate model based on latent space regression to predict pressure\ndistributions on a transonic wing given the flight conditions: Mach number and\nangle of attack. The $\\beta$-VAE model, enhanced with Principal Component\nAnalysis (PCA), maps high-dimensional data to a low-dimensional latent space,\nshowing a direct correlation with flight conditions. Regularization through\n$\\beta$ requires careful tuning to improve overall performance, while PCA\npreprocessing helps to construct an effective latent space, improving\nautoencoder training and performance. Gaussian Process Regression is used to\npredict latent space variables from flight conditions, showing robust behavior\nindependent of $\\beta$, and the decoder reconstructs the high-dimensional\npressure field data. This pipeline provides insight into unexplored flight\nconditions. Furthermore, a fine-tuning process of the decoder further refines\nthe model, reducing the dependence on $\\beta$ and enhancing accuracy.\nStructured latent space, robust regression performance, and significant\nimprovements in fine-tuning collectively create a highly accurate and efficient\nsurrogate model. Our methodology demonstrates the effectiveness of $\\beta$-VAEs\nfor aerodynamic surrogate modeling, offering a rapid, cost-effective, and\nreliable alternative for aerodynamic data prediction.\n","authors":["Víctor Francés-Belda","Alberto Solera-Rico","Javier Nieto-Centenero","Esther Andrés","Carlos Sanmiguel Vila","Rodrigo Castellanos"],"pdf_url":"https://arxiv.org/pdf/2408.04969v2.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2409.15360v3","updated":"2024-10-16T14:56:15Z","published":"2024-09-18T02:35:41Z","title":"Reward-Robust RLHF in LLMs","summary":"  As Large Language Models (LLMs) continue to progress toward more advanced\nforms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is\nincreasingly seen as a key pathway toward achieving Artificial General\nIntelligence (AGI). However, the reliance on reward-model-based (RM-based)\nalignment methods introduces significant challenges due to the inherent\ninstability and imperfections of Reward Models (RMs), which can lead to\ncritical issues such as reward hacking and misalignment with human intentions.\nIn this paper, we introduce a reward-robust RLHF framework aimed at addressing\nthese fundamental challenges, paving the way for more reliable and resilient\nlearning in LLMs. Our approach introduces a novel optimization objective that\ncarefully balances performance and robustness by incorporating Bayesian Reward\nModel Ensembles (BRME) to model the uncertainty set of reward functions. This\nallows the framework to integrate both nominal performance and minimum reward\nsignals, ensuring more stable learning even with imperfect RMs. Empirical\nresults demonstrate that our framework consistently outperforms baselines\nacross diverse benchmarks, showing improved accuracy and long-term stability.\nWe also provide a theoretical analysis, demonstrating that reward-robust RLHF\napproaches the stability of constant reward settings, which proves to be\nacceptable even in a stochastic-case analysis. Together, these contributions\nhighlight the framework potential to enhance both the performance and stability\nof LLM alignment.\n","authors":["Yuzi Yan","Xingzhou Lou","Jialian Li","Yiping Zhang","Jian Xie","Chao Yu","Yu Wang","Dong Yan","Yuan Shen"],"pdf_url":"https://arxiv.org/pdf/2409.15360v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12636v1","updated":"2024-10-16T14:55:16Z","published":"2024-10-16T14:55:16Z","title":"Towards Arbitrary QUBO Optimization: Analysis of Classical and\n  Quantum-Activated Feedforward Neural Networks","summary":"  Quadratic Unconstrained Binary Optimization (QUBO) sits at the heart of many\nindustries and academic fields such as logistics, supply chain, finance,\npharmaceutical science, chemistry, IT, and energy sectors, among others. These\nproblems typically involve optimizing a large number of binary variables, which\nmakes finding exact solutions exponentially more difficult. Consequently, most\nQUBO problems are classified as NP-hard. To address this challenge, we\ndeveloped a powerful feedforward neural network (FNN) optimizer for arbitrary\nQUBO problems. In this work, we demonstrate that the FNN optimizer can provide\nhigh-quality approximate solutions for large problems, including dense\n80-variable weighted MaxCut and random QUBOs, achieving an average accuracy of\nover 99% in less than 1.1 seconds on an 8-core CPU. Additionally, the FNN\noptimizer outperformed the Gurobi optimizer by 72% on 200-variable random QUBO\nproblems within a 100-second computation time limit, exhibiting strong\npotential for real-time optimization tasks. Building on this model, we explored\nthe novel approach of integrating FNNs with a quantum annealer-based activation\nfunction to create a quantum-classical encoder-decoder (QCED) optimizer, aiming\nto further enhance the performance of FNNs in QUBO optimization.\n","authors":["Chia-Tso Lai","Carsten Blank","Peter Schmelcher","Rick Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2410.12636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12635v1","updated":"2024-10-16T14:55:11Z","published":"2024-10-16T14:55:11Z","title":"An Exact Finite-dimensional Explicit Feature Map for Kernel Functions","summary":"  Kernel methods in machine learning use a kernel function that takes two data\npoints as input and returns their inner product after mapping them to a Hilbert\nspace, implicitly and without actually computing the mapping. For many kernel\nfunctions, such as Gaussian and Laplacian kernels, the feature space is known\nto be infinite-dimensional, making operations in this space possible only\nimplicitly. This implicit nature necessitates algorithms to be expressed using\ndual representations and the kernel trick. In this paper, given an arbitrary\nkernel function, we introduce an explicit, finite-dimensional feature map for\nany arbitrary kernel function that ensures the inner product of data points in\nthe feature space equals the kernel function value, during both training and\ntesting. The existence of this explicit mapping allows for kernelized\nalgorithms to be formulated in their primal form, without the need for the\nkernel trick or the dual representation. As a first application, we demonstrate\nhow to derive kernelized machine learning algorithms directly, without\nresorting to the dual representation, and apply this method specifically to\nPCA. As another application, without any changes to the t-SNE algorithm and its\nimplementation, we use it for visualizing the feature space of kernel\nfunctions.\n","authors":["Kamaledin Ghiasi-Shirazi","Mohammadreza Qaraei"],"pdf_url":"https://arxiv.org/pdf/2410.12635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12631v1","updated":"2024-10-16T14:53:13Z","published":"2024-10-16T14:53:13Z","title":"Explainable Moral Values: a neuro-symbolic approach to value\n  classification","summary":"  This work explores the integration of ontology-based reasoning and Machine\nLearning techniques for explainable value classification. By relying on an\nontological formalization of moral values as in the Moral Foundations Theory,\nrelying on the DnS Ontology Design Pattern, the \\textit{sandra} neuro-symbolic\nreasoner is used to infer values (fomalized as descriptions) that are\n\\emph{satisfied by} a certain sentence. Sentences, alongside their structured\nrepresentation, are automatically generated using an open-source Large Language\nModel. The inferred descriptions are used to automatically detect the value\nassociated with a sentence. We show that only relying on the reasoner's\ninference results in explainable classification comparable to other more\ncomplex approaches. We show that combining the reasoner's inferences with\ndistributional semantics methods largely outperforms all the baselines,\nincluding complex models based on neural network architectures. Finally, we\nbuild a visualization tool to explore the potential of theory-based values\nclassification, which is publicly available at http://xmv.geomeaning.com/.\n","authors":["Nicolas Lazzari","Stefano De Giorgis","Aldo Gangemi","Valentina Presutti"],"pdf_url":"https://arxiv.org/pdf/2410.12631v1.pdf","comment":"Published at ESWC24 Satellite Event"},{"id":"http://arxiv.org/abs/2402.01632v3","updated":"2024-10-16T14:46:34Z","published":"2024-02-02T18:52:16Z","title":"Time-Varying Gaussian Process Bandits with Unknown Prior","summary":"  Bayesian optimisation requires fitting a Gaussian process model, which in\nturn requires specifying prior on the unknown black-box function -- most of the\ntheoretical literature assumes this prior is known. However, it is common to\nhave more than one possible prior for a given black-box function, for example\nsuggested by domain experts with differing opinions. In some cases, the type-II\nmaximum likelihood estimator for selecting prior enjoys the consistency\nguarantee, but it does not universally apply to all types of priors. If the\nproblem is stationary, one could rely on the Regret Balancing scheme to conduct\nthe optimisation, but in the case of time-varying problems, such a scheme\ncannot be used. To address this gap in existing research, we propose a novel\nalgorithm, PE-GP-UCB, which is capable of solving time-varying Bayesian\noptimisation problems even without the exact knowledge of the function's prior.\nThe algorithm relies on the fact that either the observed function values are\nconsistent with some of the priors, in which case it is easy to reject the\nwrong priors, or the observations are consistent with all candidate priors, in\nwhich case it does not matter which prior our model relies on. We provide a\nregret bound on the proposed algorithm. Finally, we empirically evaluate our\nalgorithm on toy and real-world time-varying problems and show that it\noutperforms the maximum likelihood estimator, fully Bayesian treatment of\nunknown prior and Regret Balancing.\n","authors":["Juliusz Ziomek","Masaki Adachi","Michael A. Osborne"],"pdf_url":"https://arxiv.org/pdf/2402.01632v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12621v1","updated":"2024-10-16T14:40:32Z","published":"2024-10-16T14:40:32Z","title":"Weak-to-Strong Generalization beyond Accuracy: a Pilot Study in Safety,\n  Toxicity, and Legal Reasoning","summary":"  As large language models (LLMs) continue to advance, ensuring their alignment\nwith human values becomes increasingly critical. Traditional alignment methods\nheavily rely on human feedback to fine-tune models. With the emergence of\nsuperhuman models whose outputs may surpass human understanding, evaluating and\naligning these models using human judgments poses significant challenges. To\naddress the challenges, recent works use weak supervisors to elicit knowledge\nfrom much stronger models. However, there are important disanalogies between\nthe empirical setup in the existing works and the genuine goal of alignment. We\nremark that existing works investigate the phenomenon of weak-to-strong\ngeneration in analogous setup (i.e., binary classification), rather than\npractical alignment-relevant tasks (e.g., safety). In this paper, we bridge\nthis gap by extending weak-to-strong generation to the context of practical\nalignment. We empirically demonstrate the widespread phenomenon of\nweak-to-strong generation in three complicated alignment tasks: safety,\ntoxicity, and legal reasoning}. Furthermore, we explore efficient strategies\nfor improving alignment performance to enhance the quality of model outcomes.\nLastly, we summarize and analyze the challenges and potential solutions in\nregard to specific alignment tasks, which we hope to catalyze the research\nprogress on the topic of weak-to-strong generalization. Our code is released at\nhttps://github.com/yeruimeng/WTS.git.\n","authors":["Ruimeng Ye","Yang Xiao","Bo Hui"],"pdf_url":"https://arxiv.org/pdf/2410.12621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12613v1","updated":"2024-10-16T14:29:29Z","published":"2024-10-16T14:29:29Z","title":"Exploring Model Kinship for Merging Large Language Models","summary":"  Model merging has become one of the key technologies for enhancing the\ncapabilities and efficiency of Large Language Models (LLMs). However, our\nunderstanding of the expected performance gains and principles when merging any\ntwo models remains limited. In this work, we introduce model kinship, the\ndegree of similarity or relatedness between LLMs, analogous to biological\nevolution. With comprehensive empirical analysis, we find that there is a\ncertain relationship between model kinship and the performance gains after\nmodel merging, which can help guide our selection of candidate models. Inspired\nby this, we propose a new model merging strategy: Top-k Greedy Merging with\nModel Kinship, which can yield better performance on benchmark datasets.\nSpecifically, we discover that using model kinship as a criterion can assist us\nin continuously performing model merging, alleviating the degradation (local\noptima) in model evolution, whereas model kinship can serve as a guide to\nescape these traps. Code is available at\nhttps://github.com/zjunlp/ModelKinship.\n","authors":["Yedi Hu","Yunzhi Yao","Ningyu Zhang","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12613v1.pdf","comment":"Ongoing work"},{"id":"http://arxiv.org/abs/2409.19434v2","updated":"2024-10-16T14:28:32Z","published":"2024-09-28T18:44:39Z","title":"Energy-Efficient Computation with DVFS using Deep Reinforcement Learning\n  for Multi-Task Systems in Edge Computing","summary":"  Periodic soft real-time systems have broad applications in many areas, such\nas IoT. Finding an optimal energy-efficient policy that is adaptable to\nunderlying edge devices while meeting deadlines for tasks has always been\nchallenging. This research studies generalized systems with multi-task,\nmulti-deadline scenarios with reinforcement learning-based DVFS for energy\nsaving. This work addresses the limitation of previous work that models a\nperiodic system as a single task and single-deadline scenario, which is too\nsimplified to cope with complex situations. The method encodes time series\ninformation in the Linux kernel into information that is easy to use for\nreinforcement learning, allowing the system to generate DVFS policies to adapt\nsystem patterns based on the general workload. For encoding, we present two\ndifferent methods for comparison. Both methods use only one performance\ncounter: system utilization and the kernel only needs minimal information from\nthe userspace. Our method is implemented on Jetson Nano Board (2GB) and is\ntested with three fixed multitask workloads, which are three, five, and eight\ntasks in the workload, respectively. For randomness and generalization, we also\ndesigned a random workload generator to build different multitask workloads to\ntest. Based on the test results, our method could save 3%-10% power compared to\nLinux built-in governors.\n","authors":["Xinyi Li","Ti Zhou","Haoyu Wang","Man Lin"],"pdf_url":"https://arxiv.org/pdf/2409.19434v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14463v4","updated":"2024-10-16T14:27:49Z","published":"2023-05-23T18:37:30Z","title":"ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain\n  Readability Assessment","summary":"  We present a comprehensive evaluation of large language models for\nmultilingual readability assessment. Existing evaluation resources lack domain\nand language diversity, limiting the ability for cross-domain and cross-lingual\nanalyses. This paper introduces ReadMe++, a multilingual multi-domain dataset\nwith human annotations of 9757 sentences in Arabic, English, French, Hindi, and\nRussian, collected from 112 different data sources. This benchmark will\nencourage research on developing robust multilingual readability assessment\nmethods. Using ReadMe++, we benchmark multilingual and monolingual language\nmodels in the supervised, unsupervised, and few-shot prompting settings. The\ndomain and language diversity in ReadMe++ enable us to test more effective\nfew-shot prompting, and identify shortcomings in state-of-the-art unsupervised\nmethods. Our experiments also reveal exciting results of superior domain\ngeneralization and enhanced cross-lingual transfer capabilities by models\ntrained on ReadMe++. We will make our data publicly available and release a\npython package tool for multilingual sentence readability prediction using our\ntrained models at: https://github.com/tareknaous/readme\n","authors":["Tarek Naous","Michael J. Ryan","Anton Lavrouk","Mohit Chandra","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2305.14463v4.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.12609v1","updated":"2024-10-16T14:26:08Z","published":"2024-10-16T14:26:08Z","title":"Towards Graph Foundation Models: The Perspective of Zero-shot Reasoning\n  on Knowledge Graphs","summary":"  Inspired by the success of artificial general intelligence, there is a trend\ntowards developing Graph Foundation Models that excel in generalization across\nvarious graph tasks and domains. However, current models often require\nextensive training or fine-tuning to capture structural and semantic insights\non new graphs, which limits their versatility. In this work, we explore graph\nfoundation models from the perspective of zero-shot reasoning on Knowledge\nGraphs (KGs). Our focus is on utilizing KGs as a unified topological structure\nto tackle diverse tasks, while addressing semantic isolation challenges in KG\nreasoning to effectively integrate diverse semantic and structural features.\nThis brings us new methodological insights into KG reasoning, as well as high\ngeneralizability towards foundation models in practice. Methodologically, we\nintroduce SCORE, a unified graph reasoning framework that effectively\ngeneralizes diverse graph tasks using zero-shot learning. At the core of SCORE\nis semantic conditional message passing, a technique designed to capture both\nstructural and semantic invariances in graphs, with theoretical backing for its\nexpressive power. Practically, we evaluate the zero-shot reasoning capability\nof SCORE using 38 diverse graph datasets, covering node-level, link-level, and\ngraph-level tasks across multiple domains. Our experiments reveal a substantial\nperformance improvement over prior foundation models and supervised baselines,\nhighlighting the efficacy and adaptability of our approach.\n","authors":["Kai Wang","Siqiang Luo"],"pdf_url":"https://arxiv.org/pdf/2410.12609v1.pdf","comment":"17 Pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.12607v1","updated":"2024-10-16T14:24:51Z","published":"2024-10-16T14:24:51Z","title":"Low-Rank Adversarial PGD Attack","summary":"  Adversarial attacks on deep neural network models have seen rapid development\nand are extensively used to study the stability of these networks. Among\nvarious adversarial strategies, Projected Gradient Descent (PGD) is a widely\nadopted method in computer vision due to its effectiveness and quick\nimplementation, making it suitable for adversarial training. In this work, we\nobserve that in many cases, the perturbations computed using PGD predominantly\naffect only a portion of the singular value spectrum of the original image,\nsuggesting that these perturbations are approximately low-rank. Motivated by\nthis observation, we propose a variation of PGD that efficiently computes a\nlow-rank attack. We extensively validate our method on a range of standard\nmodels as well as robust models that have undergone adversarial training. Our\nanalysis indicates that the proposed low-rank PGD can be effectively used in\nadversarial training due to its straightforward and fast implementation coupled\nwith competitive performance. Notably, we find that low-rank PGD often performs\ncomparably to, and sometimes even outperforms, the traditional full-rank PGD\nattack, while using significantly less memory.\n","authors":["Dayana Savostianova","Emanuele Zangrando","Francesco Tudisco"],"pdf_url":"https://arxiv.org/pdf/2410.12607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12606v1","updated":"2024-10-16T14:24:44Z","published":"2024-10-16T14:24:44Z","title":"Self-Supervised Learning of Disentangled Representations for\n  Multivariate Time-Series","summary":"  Multivariate time-series data in fields like healthcare and industry are\ninformative but challenging due to high dimensionality and lack of labels.\nRecent self-supervised learning methods excel in learning rich representations\nwithout labels but struggle with disentangled embeddings and inductive bias\nissues like transformation-invariance. To address these challenges, we\nintroduce TimeDRL, a framework for multivariate time-series representation\nlearning with dual-level disentangled embeddings. TimeDRL features: (i)\ndisentangled timestamp-level and instance-level embeddings using a [CLS] token\nstrategy; (ii) timestamp-predictive and instance-contrastive tasks for\nrepresentation learning; and (iii) avoidance of augmentation methods to\neliminate inductive biases. Experiments on forecasting and classification\ndatasets show TimeDRL outperforms existing methods, with further validation in\nsemi-supervised settings with limited labeled data.\n","authors":["Ching Chang","Chiao-Tung Chan","Wei-Yao Wang","Wen-Chih Peng","Tien-Fu Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12606v1.pdf","comment":"NeurIPS 2024 Workshop: Self-Supervised Learning - Theory and Practice"},{"id":"http://arxiv.org/abs/2410.12604v1","updated":"2024-10-16T14:23:36Z","published":"2024-10-16T14:23:36Z","title":"The Bayesian Confidence (BACON) Estimator for Deep Neural Networks","summary":"  This paper introduces the Bayesian Confidence Estimator (BACON) for deep\nneural networks. Current practice of interpreting Softmax values in the output\nlayer as probabilities of outcomes is prone to extreme predictions of class\nprobability. In this work we extend Waagen's method of representing the\nterminal layers with a geometric model, where the probability associated with\nan output vector is estimated with Bayes' Rule using validation data to provide\nlikelihood and normalization values. This estimator provides superior ECE and\nACE calibration error compared to Softmax for ResNet-18 at 85% network\naccuracy, and EfficientNet-B0 at 95% network accuracy, on the CIFAR-10 dataset\nwith an imbalanced test set, except for very high accuracy edge cases. In\naddition, when using the ACE metric, BACON demonstrated improved calibration\nerror when estimating probabilities for the imbalanced test set when using\nactual class distribution fractions.\n","authors":["Patrick D. Kee","Max J. Brown","Jonathan C. Rice","Christian A. Howell"],"pdf_url":"https://arxiv.org/pdf/2410.12604v1.pdf","comment":"14 pages, 15 figures (10 of which include sub-figures)"},{"id":"http://arxiv.org/abs/2410.12598v1","updated":"2024-10-16T14:15:28Z","published":"2024-10-16T14:15:28Z","title":"Dynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach","summary":"  In Deep Reinforcement Learning models trained using gradient-based\ntechniques, the choice of optimizer and its learning rate are crucial to\nachieving good performance: higher learning rates can prevent the model from\nlearning effectively, while lower ones might slow convergence. Additionally,\ndue to the non-stationarity of the objective function, the best-performing\nlearning rate can change over the training steps. To adapt the learning rate, a\nstandard technique consists of using decay schedulers. However, these\nschedulers assume that the model is progressively approaching convergence,\nwhich may not always be true, leading to delayed or premature adjustments. In\nthis work, we propose dynamic Learning Rate for deep Reinforcement Learning\n(LRRL), a meta-learning approach that selects the learning rate based on the\nagent's performance during training. LRRL is based on a multi-armed bandit\nalgorithm, where each arm represents a different learning rate, and the bandit\nfeedback is provided by the cumulative returns of the RL policy to update the\narms' probability distribution. Our empirical results demonstrate that LRRL can\nsubstantially improve the performance of deep RL algorithms.\n","authors":["Henrique Donâncio","Antoine Barrier","Leah F. South","Florence Forbes"],"pdf_url":"https://arxiv.org/pdf/2410.12598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12597v1","updated":"2024-10-16T14:15:01Z","published":"2024-10-16T14:15:01Z","title":"Personalized Prediction Models for Changes in Knee Pain among Patients\n  with Osteoarthritis Participating in Supervised Exercise and Education","summary":"  Knee osteoarthritis (OA) is a widespread chronic condition that impairs\nmobility and diminishes quality of life. Despite the proven benefits of\nexercise therapy and patient education in managing the OA symptoms pain and\nfunctional limitations, these strategies are often underutilized. Personalized\noutcome prediction models can help motivate and engage patients, but the\naccuracy of existing models in predicting changes in knee pain remains\ninsufficiently examined. To validate existing models and introduce a concise\npersonalized model predicting changes in knee pain before to after\nparticipating in a supervised education and exercise therapy program (GLA:D)\nfor knee OA patients. Our models use self-reported patient information and\nfunctional measures. To refine the number of variables, we evaluated the\nvariable importance and applied clinical reasoning. We trained random forest\nregression models and compared the rate of true predictions of our models with\nthose utilizing average values. We evaluated the performance of a full,\ncontinuous, and concise model including all 34, all 11 continuous, and the six\nmost predictive variables respectively. All three models performed similarly\nand were comparable to the existing model, with R-squares of 0.31-0.32 and\nRMSEs of 18.65-18.85 - despite our increased sample size. Allowing a deviation\nof 15 VAS points from the true change in pain, our concise model and utilizing\nthe average values estimated the change in pain at 58% and 51% correctly,\nrespectively. Our supplementary analysis led to similar outcomes. Our concise\npersonalized prediction model more accurately predicts changes in knee pain\nfollowing the GLA:D program compared to average pain improvement values.\nNeither the increase in sample size nor the inclusion of additional variables\nimproved previous models. To improve predictions, new variables beyond those in\nthe GLA:D are required.\n","authors":["M. Rafiei","S. Das","M. Bakhtiari","E. M. Roos","S. T. Skou","D. T. Grønne","J. Baumbach","L. Baumbach"],"pdf_url":"https://arxiv.org/pdf/2410.12597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08770v3","updated":"2024-10-16T14:14:41Z","published":"2024-09-13T12:24:12Z","title":"Increasing Both Batch Size and Learning Rate Accelerates Stochastic\n  Gradient Descent","summary":"  The performance of mini-batch stochastic gradient descent (SGD) strongly\ndepends on setting the batch size and learning rate to minimize the empirical\nloss in training the deep neural network. In this paper, we present theoretical\nanalyses of mini-batch SGD with four schedulers: (i) constant batch size and\ndecaying learning rate scheduler, (ii) increasing batch size and decaying\nlearning rate scheduler, (iii) increasing batch size and increasing learning\nrate scheduler, and (iv) increasing batch size and warm-up decaying learning\nrate scheduler. We show that mini-batch SGD using scheduler (i) does not always\nminimize the expectation of the full gradient norm of the empirical loss,\nwhereas it does using any of schedulers (ii), (iii), and (iv). Furthermore,\nschedulers (iii) and (iv) accelerate mini-batch SGD. The paper also provides\nnumerical results of supporting analyses showing that using scheduler (iii) or\n(iv) minimizes the full gradient norm of the empirical loss faster than using\nscheduler (i) or (ii).\n","authors":["Hikaru Umeda","Hideaki Iiduka"],"pdf_url":"https://arxiv.org/pdf/2409.08770v3.pdf","comment":"28 pages, 18 figures"},{"id":"http://arxiv.org/abs/2403.11894v4","updated":"2024-10-16T14:14:27Z","published":"2024-03-18T15:53:33Z","title":"From Explainable to Interpretable Deep Learning for Natural Language\n  Processing in Healthcare: How Far from Reality?","summary":"  Deep learning (DL) has substantially enhanced natural language processing\n(NLP) in healthcare research. However, the increasing complexity of DL-based\nNLP necessitates transparent model interpretability, or at least\nexplainability, for reliable decision-making. This work presents a thorough\nscoping review of explainable and interpretable DL in healthcare NLP. The term\n\"eXplainable and Interpretable Artificial Intelligence\" (XIAI) is introduced to\ndistinguish XAI from IAI. Different models are further categorized based on\ntheir functionality (model-, input-, output-based) and scope (local, global).\nOur analysis shows that attention mechanisms are the most prevalent emerging\nIAI technique. The use of IAI is growing, distinguishing it from XAI. The major\nchallenges identified are that most XIAI does not explore \"global\" modelling\nprocesses, the lack of best practices, and the lack of systematic evaluation\nand benchmarks. One important opportunity is to use attention mechanisms to\nenhance multi-modal XIAI for personalized medicine. Additionally, combining DL\nwith causal logic holds promise. Our discussion encourages the integration of\nXIAI in Large Language Models (LLMs) and domain-specific smaller models. In\nconclusion, XIAI adoption in healthcare requires dedicated in-house expertise.\nCollaboration with domain experts, end-users, and policymakers can lead to\nready-to-use XIAI methods across NLP and medical tasks. While challenges exist,\nXIAI techniques offer a valuable foundation for interpretable NLP algorithms in\nhealthcare.\n","authors":["Guangming Huang","Yingya Li","Shoaib Jameel","Yunfei Long","Giorgos Papanastasiou"],"pdf_url":"https://arxiv.org/pdf/2403.11894v4.pdf","comment":"This paper has been accepted by Computational and Structural\n  Biotechnology Journal"},{"id":"http://arxiv.org/abs/2410.12593v1","updated":"2024-10-16T14:12:11Z","published":"2024-10-16T14:12:11Z","title":"Expand and Compress: Exploring Tuning Principles for Continual\n  Spatio-Temporal Graph Forecasting","summary":"  The widespread deployment of sensing devices leads to a surge in data for\nspatio-temporal forecasting applications such as traffic flow, air quality, and\nwind energy. Although spatio-temporal graph neural networks have achieved\nsuccess in modeling various static spatio-temporal forecasting scenarios,\nreal-world spatio-temporal data are typically received in a streaming manner,\nand the network continuously expands with the installation of new sensors.\nThus, spatio-temporal forecasting in streaming scenarios faces dual challenges:\nthe inefficiency of retraining models over newly arrived data and the\ndetrimental effects of catastrophic forgetting over long-term history. To\naddress these challenges, we propose a novel prompt tuning-based continuous\nforecasting method, following two fundamental tuning principles guided by\nempirical and theoretical analysis: expand and compress, which effectively\nresolve the aforementioned problems with lightweight tuning parameters.\nSpecifically, we integrate the base spatio-temporal graph neural network with a\ncontinuous prompt pool, utilizing stored prompts (i.e., few learnable\nparameters) in memory, and jointly optimize them with the base spatio-temporal\ngraph neural network. This method ensures that the model sequentially learns\nfrom the spatio-temporal data stream to accomplish tasks for corresponding\nperiods. Extensive experimental results on multiple real-world datasets\ndemonstrate the multi-faceted superiority of our method over the\nstate-of-the-art baselines, including effectiveness, efficiency, universality,\netc.\n","authors":["Wei Chen","Yuxuan Liang"],"pdf_url":"https://arxiv.org/pdf/2410.12593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03293v3","updated":"2024-10-16T14:11:21Z","published":"2024-10-04T10:06:55Z","title":"Five Years of COVID-19 Discourse on Instagram: A Labeled Instagram\n  Dataset of Over Half a Million Posts for Multilingual Sentiment Analysis","summary":"  The work presented in this paper makes three scientific contributions with a\nspecific focus on mining and analysis of COVID-19-related posts on Instagram.\nFirst, it presents a multilingual dataset of 500,153 Instagram posts about\nCOVID-19 published between January 2020 and September 2024. This dataset,\navailable at https://dx.doi.org/10.21227/d46p-v480, contains Instagram posts in\n161 different languages as well as 535,021 distinct hashtags. After the\ndevelopment of this dataset, multilingual sentiment analysis was performed,\nwhich involved classifying each post as positive, negative, or neutral. The\nresults of sentiment analysis are presented as a separate attribute in this\ndataset. Second, it presents the results of performing sentiment analysis per\nyear from 2020 to 2024. The findings revealed the trends in sentiment related\nto COVID-19 on Instagram since the beginning of the pandemic. For instance,\nbetween 2020 and 2024, the sentiment trends show a notable shift, with positive\nsentiment decreasing from 38.35% to 28.69%, while neutral sentiment rising from\n44.19% to 58.34%. Finally, the paper also presents findings of\nlanguage-specific sentiment analysis. This analysis highlighted similar and\ncontrasting trends of sentiment across posts published in different languages\non Instagram. For instance, out of all English posts, 49.68% were positive,\n14.84% were negative, and 35.48% were neutral. In contrast, among Hindi posts,\n4.40% were positive, 57.04% were negative, and 38.56% were neutral, reflecting\ndistinct differences in the sentiment distribution between these two languages.\n","authors":["Nirmalya Thakur"],"pdf_url":"https://arxiv.org/pdf/2410.03293v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12592v1","updated":"2024-10-16T14:10:53Z","published":"2024-10-16T14:10:53Z","title":"Cocoon: Robust Multi-Modal Perception with Uncertainty-Aware Sensor\n  Fusion","summary":"  An important paradigm in 3D object detection is the use of multiple\nmodalities to enhance accuracy in both normal and challenging conditions,\nparticularly for long-tail scenarios. To address this, recent studies have\nexplored two directions of adaptive approaches: MoE-based adaptive fusion,\nwhich struggles with uncertainties arising from distinct object configurations,\nand late fusion for output-level adaptive fusion, which relies on separate\ndetection pipelines and limits comprehensive understanding. In this work, we\nintroduce Cocoon, an object- and feature-level uncertainty-aware fusion\nframework. The key innovation lies in uncertainty quantification for\nheterogeneous representations, enabling fair comparison across modalities\nthrough the introduction of a feature aligner and a learnable surrogate ground\ntruth, termed feature impression. We also define a training objective to ensure\nthat their relationship provides a valid metric for uncertainty quantification.\nCocoon consistently outperforms existing static and adaptive methods in both\nnormal and challenging conditions, including those with natural and artificial\ncorruptions. Furthermore, we show the validity and efficacy of our uncertainty\nmetric across diverse datasets.\n","authors":["Minkyoung Cho","Yulong Cao","Jiachen Sun","Qingzhao Zhang","Marco Pavone","Jeong Joon Park","Heng Yang","Z. Morley Mao"],"pdf_url":"https://arxiv.org/pdf/2410.12592v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2410.12589v1","updated":"2024-10-16T14:10:15Z","published":"2024-10-16T14:10:15Z","title":"From Lab to Pocket: A Novel Continual Learning-based Mobile Application\n  for Screening COVID-19","summary":"  Artificial intelligence (AI) has emerged as a promising tool for predicting\nCOVID-19 from medical images. In this paper, we propose a novel continual\nlearning-based approach and present the design and implementation of a mobile\napplication for screening COVID-19. Our approach demonstrates the ability to\nadapt to evolving datasets, including data collected from different locations\nor hospitals, varying virus strains, and diverse clinical presentations,\nwithout retraining from scratch. We have evaluated state-of-the-art continual\nlearning methods for detecting COVID-19 from chest X-rays and selected the\nbest-performing model for our mobile app. We evaluated various deep learning\narchitectures to select the best-performing one as a foundation model for\ncontinual learning. Both regularization and memory-based methods for continual\nlearning were tested, using different memory sizes to develop the optimal\ncontinual learning model for our app. DenseNet161 emerged as the best\nfoundation model with 96.87\\% accuracy, and Learning without Forgetting (LwF)\nwas the top continual learning method with an overall performance of 71.99\\%.\nThe mobile app design considers both patient and doctor perspectives. It\nincorporates the continual learning DenseNet161 LwF model on a cloud server,\nenabling the model to learn from new instances of chest X-rays and their\nclassifications as they are submitted. The app is designed, implemented, and\nevaluated to ensure it provides an efficient tool for COVID-19 screening. The\napp is available to download from\nhttps://github.com/DannyFGitHub/COVID-19PneumoCheckApp.\n","authors":["Danny Falero","Muhammad Ashad Kabir","Nusrat Homaira"],"pdf_url":"https://arxiv.org/pdf/2410.12589v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2410.08469v2","updated":"2024-10-16T14:09:14Z","published":"2024-10-11T02:42:13Z","title":"Semantic Token Reweighting for Interpretable and Controllable Text\n  Embeddings in CLIP","summary":"  A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial\nrole in translating textual input into an embedding space shared with images,\nthereby facilitating the interpretative analysis of vision tasks through\nnatural language. Despite the varying significance of different textual\nelements within a sentence depending on the context, efforts to account for\nvariation of importance in constructing text embeddings have been lacking. We\npropose a framework of Semantic Token Reweighting to build Interpretable text\nembeddings (SToRI), which incorporates controllability as well. SToRI refines\nthe text encoding process in CLIP by differentially weighting semantic elements\nbased on contextual importance, enabling finer control over emphasis responsive\nto data-driven insights and user preferences. The efficacy of SToRI is\ndemonstrated through comprehensive experiments on few-shot image classification\nand image retrieval tailored to user preferences.\n","authors":["Eunji Kim","Kyuhong Shim","Simyung Chang","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2410.08469v2.pdf","comment":"Accepted at EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.11031v2","updated":"2024-10-16T14:05:57Z","published":"2024-10-14T19:33:46Z","title":"NAR-*ICP: Neural Execution of Classical ICP-based Pointcloud\n  Registration Algorithms","summary":"  This study explores the intersection of neural networks and classical\nrobotics algorithms through the Neural Algorithmic Reasoning (NAR) framework,\nallowing to train neural networks to effectively reason like classical robotics\nalgorithms by learning to execute them. Algorithms are integral to robotics and\nsafety-critical applications due to their predictable and consistent\nperformance through logical and mathematical principles. In contrast, while\nneural networks are highly adaptable, handling complex, high-dimensional data\nand generalising across tasks, they often lack interpretability and\ntransparency in their internal computations. We propose a Graph Neural Network\n(GNN)-based learning framework, NAR-*ICP, which learns the intermediate\nalgorithmic steps of classical ICP-based pointcloud registration algorithms,\nand extend the CLRS Algorithmic Reasoning Benchmark with classical robotics\nperception algorithms. We evaluate our approach across diverse datasets, from\nreal-world to synthetic, demonstrating its flexibility in handling complex and\nnoisy inputs, along with its potential to be used as part of a larger learning\nsystem. Our results indicate that our method achieves superior performance\nacross all benchmarks and datasets, consistently surpassing even the algorithms\nit has been trained on, further demonstrating its ability to generalise beyond\nthe capabilities of traditional algorithms.\n","authors":["Efimia Panagiotaki","Daniele De Martini","Lars Kunze","Petar Veličković"],"pdf_url":"https://arxiv.org/pdf/2410.11031v2.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.12584v1","updated":"2024-10-16T14:04:06Z","published":"2024-10-16T14:04:06Z","title":"Self-DenseMobileNet: A Robust Framework for Lung Nodule Classification\n  using Self-ONN and Stacking-based Meta-Classifier","summary":"  In this study, we propose a novel and robust framework, Self-DenseMobileNet,\ndesigned to enhance the classification of nodules and non-nodules in chest\nradiographs (CXRs). Our approach integrates advanced image standardization and\nenhancement techniques to optimize the input quality, thereby improving\nclassification accuracy. To enhance predictive accuracy and leverage the\nstrengths of multiple models, the prediction probabilities from\nSelf-DenseMobileNet were transformed into tabular data and used to train eight\nclassical machine learning (ML) models; the top three performers were then\ncombined via a stacking algorithm, creating a robust meta-classifier that\nintegrates their collective insights for superior classification performance.\nTo enhance the interpretability of our results, we employed class activation\nmapping (CAM) to visualize the decision-making process of the best-performing\nmodel. Our proposed framework demonstrated remarkable performance on internal\nvalidation data, achieving an accuracy of 99.28\\% using a Meta-Random Forest\nClassifier. When tested on an external dataset, the framework maintained strong\ngeneralizability with an accuracy of 89.40\\%. These results highlight a\nsignificant improvement in the classification of CXRs with lung nodules.\n","authors":["Md. Sohanur Rahman","Muhammad E. H. Chowdhury","Hasib Ryan Rahman","Mosabber Uddin Ahmed","Muhammad Ashad Kabir","Sanjiban Sekhar Roy","Rusab Sarmun"],"pdf_url":"https://arxiv.org/pdf/2410.12584v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2308.06686v4","updated":"2024-10-16T13:55:36Z","published":"2023-08-13T05:22:49Z","title":"TorchQL: A Programming Framework for Integrity Constraints in Machine\n  Learning","summary":"  Finding errors in machine learning applications requires a thorough\nexploration of their behavior over data. Existing approaches used by\npractitioners are often ad-hoc and lack the abstractions needed to scale this\nprocess. We present TorchQL, a programming framework to evaluate and improve\nthe correctness of machine learning applications. TorchQL allows users to write\nqueries to specify and check integrity constraints over machine learning models\nand datasets. It seamlessly integrates relational algebra with functional\nprogramming to allow for highly expressive queries using only eight intuitive\noperators. We evaluate TorchQL on diverse use-cases including finding critical\ntemporal inconsistencies in objects detected across video frames in autonomous\ndriving, finding data imputation errors in time-series medical records, finding\ndata labeling errors in real-world images, and evaluating biases and\nconstraining outputs of language models. Our experiments show that TorchQL\nenables up to 13x faster query executions than baselines like Pandas and\nMongoDB, and up to 40% shorter queries than native Python. We also conduct a\nuser study and find that TorchQL is natural enough for developers familiar with\nPython to specify complex integrity constraints.\n","authors":["Aaditya Naik","Adam Stein","Yinjun Wu","Mayur Naik","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2308.06686v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12572v1","updated":"2024-10-16T13:50:04Z","published":"2024-10-16T13:50:04Z","title":"On the Role of Activation Functions in EEG-To-Text Decoder","summary":"  In recent years, much interdisciplinary research has been conducted exploring\npotential use cases of neuroscience to advance the field of information\nretrieval. Initial research concentrated on the use of fMRI data, but fMRI was\ndeemed to be not suitable for real-world applications, and soon, research\nshifted towards using EEG data. In this paper, we try to improve the original\nperformance of a first attempt at generating text using EEG by focusing on the\nless explored area of optimising neural network performance. We test a set of\ndifferent activation functions and compare their performance. Our results show\nthat introducing a higher degree polynomial activation function can enhance\nmodel performance without changing the model architecture. We also show that\nthe learnable 3rd-degree activation function performs better on the 1-gram\nevaluation compared to a 3rd-degree non-learnable function. However, when\nevaluating the model on 2-grams and above, the polynomial function lacks in\nperformance, whilst the leaky ReLU activation function outperforms the\nbaseline.\n","authors":["Zenon Lamprou","Iakovos Tenedios","Yashar Moshfeghi"],"pdf_url":"https://arxiv.org/pdf/2410.12572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12593v2","updated":"2024-10-16T13:45:54Z","published":"2024-06-18T13:25:18Z","title":"PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval","summary":"  Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)\nfor efficient document retrieval without relying on external indexes. However,\nDSI needs full re-training to handle updates in dynamic corpora, causing\nsignificant computational inefficiencies. We introduce PromptDSI, a\nprompt-based rehearsal-free approach for instance-wise incremental learning\ndocument retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of\nDSI, leveraging its powerful representation to efficiently index new corpora\nwhile maintaining a balance between stability and plasticity. We eliminate the\ninitial forward pass of prompt-based continual learning methods that doubles\ntraining and inference time. Moreover, we propose a topic-aware prompt pool\nthat employs neural topic embeddings as fixed keys. This strategy ensures\ndiverse and effective prompt usage, addressing the challenge of parameter\nunderutilization caused by the collapse of the query-key matching mechanism.\nOur empirical evaluations demonstrate that BERT-based PromptDSI matches IncDSI\nin managing forgetting while improving new corpora performance by more than 4%\nHits@10 on NQ320k and upto 3% MRR@10 on MS MARCO 300k.\n","authors":["Tuan-Luc Huynh","Thuy-Trang Vu","Weiqing Wang","Yinwei Wei","Trung Le","Dragan Gasevic","Yuan-Fang Li","Thanh-Toan Do"],"pdf_url":"https://arxiv.org/pdf/2406.12593v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2408.13452v3","updated":"2024-10-16T13:43:08Z","published":"2024-08-24T03:43:35Z","title":"Data Augmentation for Continual RL via Adversarial Gradient Episodic\n  Memory","summary":"  Data efficiency of learning, which plays a key role in the Reinforcement\nLearning (RL) training process, becomes even more important in continual RL\nwith sequential environments. In continual RL, the learner interacts with\nnon-stationary, sequential tasks and is required to learn new tasks without\nforgetting previous knowledge. However, there is little work on implementing\ndata augmentation for continual RL. In this paper, we investigate the efficacy\nof data augmentation for continual RL. Specifically, we provide benchmarking\ndata augmentations for continual RL, by (1) summarising existing data\naugmentation methods and (2) including a new augmentation method for continual\nRL: Adversarial Augmentation with Gradient Episodic Memory (Adv-GEM). Extensive\nexperiments show that data augmentations, such as random amplitude scaling,\nstate-switch, mixup, adversarial augmentation, and Adv-GEM, can improve\nexisting continual RL algorithms in terms of their average performance,\ncatastrophic forgetting, and forward transfer, on robot control tasks. All data\naugmentation methods are implemented as plug-in modules for trivial integration\ninto continual RL methods.\n","authors":["Sihao Wu","Xingyu Zhao","Xiaowei Huang"],"pdf_url":"https://arxiv.org/pdf/2408.13452v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07762v2","updated":"2024-10-16T13:42:33Z","published":"2024-02-12T16:28:52Z","title":"Scalable Structure Learning for Sparse Context-Specific Systems","summary":"  Several approaches to graphically representing context-specific relations\namong jointly distributed categorical variables have been proposed, along with\nstructure learning algorithms. While existing optimization-based methods have\nlimited scalability due to the large number of context-specific models, the\nconstraint-based methods are more prone to error than even constraint-based\ndirected acyclic graph learning algorithms since more relations must be tested.\nWe present an algorithm for learning context-specific models that scales to\nhundreds of variables. Scalable learning is achieved through a combination of\nan order-based Markov chain Monte-Carlo search and a novel, context-specific\nsparsity assumption that is analogous to those typically invoked for directed\nacyclic graphical models. Unlike previous Markov chain Monte-Carlo search\nmethods, our Markov chain is guaranteed to have the true posterior of the\nvariable orderings as the stationary distribution. To implement the method, we\nsolve a first case of an open problem recently posed by Alon and Balogh. Future\nwork solving increasingly general instances of this problem would allow our\nmethods to learn increasingly dense models. The method is shown to perform well\non synthetic data and real world examples, in terms of both accuracy and\nscalability.\n","authors":["Felix Leopoldo Rios","Alex Markham","Liam Solus"],"pdf_url":"https://arxiv.org/pdf/2402.07762v2.pdf","comment":"34 pages, 6 figures; for associated code, see\n  https://cstrees.readthedocs.io"},{"id":"http://arxiv.org/abs/2410.12557v1","updated":"2024-10-16T13:34:40Z","published":"2024-10-16T13:34:40Z","title":"One Step Diffusion via Shortcut Models","summary":"  Diffusion models and flow-matching models have enabled generating diverse and\nrealistic images by learning to transfer noise to data. However, sampling from\nthese models involves iterative denoising over many neural network passes,\nmaking generation slow and expensive. Previous approaches for speeding up\nsampling require complex training regimes, such as multiple training phases,\nmultiple networks, or fragile scheduling. We introduce shortcut models, a\nfamily of generative models that use a single network and training phase to\nproduce high-quality samples in a single or multiple sampling steps. Shortcut\nmodels condition the network not only on the current noise level but also on\nthe desired step size, allowing the model to skip ahead in the generation\nprocess. Across a wide range of sampling step budgets, shortcut models\nconsistently produce higher quality samples than previous approaches, such as\nconsistency models and reflow. Compared to distillation, shortcut models reduce\ncomplexity to a single network and training phase and additionally allow\nvarying step budgets at inference time.\n","authors":["Kevin Frans","Danijar Hafner","Sergey Levine","Pieter Abbeel"],"pdf_url":"https://arxiv.org/pdf/2410.12557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12555v1","updated":"2024-10-16T13:32:35Z","published":"2024-10-16T13:32:35Z","title":"Investigating Sensitive Directions in GPT-2: An Improved Baseline and\n  Comparative Analysis of SAEs","summary":"  Sensitive directions experiments attempt to understand the computational\nfeatures of Language Models (LMs) by measuring how much the next token\nprediction probabilities change by perturbing activations along specific\ndirections. We extend the sensitive directions work by introducing an improved\nbaseline for perturbation directions. We demonstrate that KL divergence for\nSparse Autoencoder (SAE) reconstruction errors are no longer pathologically\nhigh compared to the improved baseline. We also show that feature directions\nuncovered by SAEs have varying impacts on model outputs depending on the SAE's\nsparsity, with lower L0 SAE feature directions exerting a greater influence.\nAdditionally, we find that end-to-end SAE features do not exhibit stronger\neffects on model outputs compared to traditional SAEs.\n","authors":["Daniel J. Lee","Stefan Heimersheim"],"pdf_url":"https://arxiv.org/pdf/2410.12555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12542v1","updated":"2024-10-16T13:20:57Z","published":"2024-10-16T13:20:57Z","title":"Evaluating Utility of Memory Efficient Medical Image Generation: A Study\n  on Lung Nodule Segmentation","summary":"  The scarcity of publicly available medical imaging data limits the\ndevelopment of effective AI models. This work proposes a memory-efficient\npatch-wise denoising diffusion probabilistic model (DDPM) for generating\nsynthetic medical images, focusing on CT scans with lung nodules. Our approach\ngenerates high-utility synthetic images with nodule segmentation while\nefficiently managing memory constraints, enabling the creation of training\ndatasets. We evaluate the method in two scenarios: training a segmentation\nmodel exclusively on synthetic data, and augmenting real-world training data\nwith synthetic images. In the first case, models trained solely on synthetic\ndata achieve Dice scores comparable to those trained on real-world data\nbenchmarks. In the second case, augmenting real-world data with synthetic\nimages significantly improves segmentation performance. The generated images\ndemonstrate their potential to enhance medical image datasets in scenarios with\nlimited real-world data.\n","authors":["Kathrin Khadra","Utku Türkbey"],"pdf_url":"https://arxiv.org/pdf/2410.12542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12537v1","updated":"2024-10-16T13:19:03Z","published":"2024-10-16T13:19:03Z","title":"Is Complex Query Answering Really Complex?","summary":"  Complex query answering (CQA) on knowledge graphs (KGs) is gaining momentum\nas a challenging reasoning task. In this paper, we show that the current\nbenchmarks for CQA are not really complex, and the way they are built distorts\nour perception of progress in this field. For example, we find that in these\nbenchmarks, most queries (up to 98% for some query types) can be reduced to\nsimpler problems, e.g., link prediction, where only one link needs to be\npredicted. The performance of state-of-the-art CQA models drops significantly\nwhen such models are evaluated on queries that cannot be reduced to easier\ntypes. Thus, we propose a set of more challenging benchmarks, composed of\nqueries that require models to reason over multiple hops and better reflect the\nconstruction of real-world KGs. In a systematic empirical investigation, the\nnew benchmarks show that current methods leave much to be desired from current\nCQA methods.\n","authors":["Cosimo Gregucci","Bo Xiong","Daniel Hernandez","Lorenzo Loconte","Pasquale Minervini","Steffen Staab","Antonio Vergari"],"pdf_url":"https://arxiv.org/pdf/2410.12537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12536v1","updated":"2024-10-16T13:18:45Z","published":"2024-10-16T13:18:45Z","title":"SiFiSinger: A High-Fidelity End-to-End Singing Voice Synthesizer based\n  on Source-filter Model","summary":"  This paper presents an advanced end-to-end singing voice synthesis (SVS)\nsystem based on the source-filter mechanism that directly translates lyrical\nand melodic cues into expressive and high-fidelity human-like singing.\nSimilarly to VISinger 2, the proposed system also utilizes training paradigms\nevolved from VITS and incorporates elements like the fundamental pitch (F0)\npredictor and waveform generation decoder. To address the issue that the\ncoupling of mel-spectrogram features with F0 information may introduce errors\nduring F0 prediction, we consider two strategies. Firstly, we leverage\nmel-cepstrum (mcep) features to decouple the intertwined mel-spectrogram and F0\ncharacteristics. Secondly, inspired by the neural source-filter models, we\nintroduce source excitation signals as the representation of F0 in the SVS\nsystem, aiming to capture pitch nuances more accurately. Meanwhile,\ndifferentiable mcep and F0 losses are employed as the waveform decoder\nsupervision to fortify the prediction accuracy of speech envelope and pitch in\nthe generated speech. Experiments on the Opencpop dataset demonstrate efficacy\nof the proposed model in synthesis quality and intonation accuracy.\n","authors":["Jianwei Cui","Yu Gu","Chao Weng","Jie Zhang","Liping Chen","Lirong Dai"],"pdf_url":"https://arxiv.org/pdf/2410.12536v1.pdf","comment":"Accepted by ICASSP 2024, Synthesized audio samples are available at:\n  https://sounddemos.github.io/sifisinger"},{"id":"http://arxiv.org/abs/2410.12530v1","updated":"2024-10-16T13:10:04Z","published":"2024-10-16T13:10:04Z","title":"Disentangling data distribution for Federated Learning","summary":"  Federated Learning (FL) facilitates collaborative training of a global model\nwhose performance is boosted by private data owned by distributed clients,\nwithout compromising data privacy. Yet the wide applicability of FL is hindered\nby entanglement of data distributions across different clients. This paper\ndemonstrates for the first time that by disentangling data distributions FL can\nin principle achieve efficiencies comparable to those of distributed systems,\nrequiring only one round of communication. To this end, we propose a novel\nFedDistr algorithm, which employs stable diffusion models to decouple and\nrecover data distributions. Empirical results on the CIFAR100 and DomainNet\ndatasets show that FedDistr significantly enhances model utility and efficiency\nin both disentangled and near-disentangled scenarios while ensuring privacy,\noutperforming traditional federated learning methods.\n","authors":["Xinyuan Zhao","Hanlin Gu","Lixin Fan","Qiang Yang","Yuxing Han"],"pdf_url":"https://arxiv.org/pdf/2410.12530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12522v1","updated":"2024-10-16T13:02:02Z","published":"2024-10-16T13:02:02Z","title":"MING: A Functional Approach to Learning Molecular Generative Models","summary":"  Traditional molecule generation methods often rely on sequence or graph-based\nrepresentations, which can limit their expressive power or require complex\npermutation-equivariant architectures. This paper introduces a novel paradigm\nfor learning molecule generative models based on functional representations.\nSpecifically, we propose Molecular Implicit Neural Generation (MING), a\ndiffusion-based model that learns molecular distributions in function space.\nUnlike standard diffusion processes in data space, MING employs a novel\nfunctional denoising probabilistic process, which jointly denoises the\ninformation in both the function's input and output spaces by leveraging an\nexpectation-maximization procedure for latent implicit neural representations\nof data. This approach allows for a simple yet effective model design that\naccurately captures underlying function distributions. Experimental results on\nmolecule-related datasets demonstrate MING's superior performance and ability\nto generate plausible molecular samples, surpassing state-of-the-art data-space\nmethods while offering a more streamlined architecture and significantly faster\ngeneration times.\n","authors":["Van Khoa Nguyen","Maciej Falkiewicz","Giangiacomo Mercatali","Alexandros Kalousis"],"pdf_url":"https://arxiv.org/pdf/2410.12522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19014v3","updated":"2024-10-16T12:55:55Z","published":"2024-09-24T01:40:50Z","title":"FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark","summary":"  Text-to-SQL systems have become crucial for translating natural language into\nSQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, the Execution Accuracy\n(EX), the most prevalent evaluation metric, still shows many false positives\nand negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our metric improves\nagreement with human experts (from 62 to 87.04 in Cohen's kappa) with\ncomprehensive context and sophisticated criteria. Our extensive experiments\nyield several key insights: (1) Models' performance increases by over 2.6\npoints on average, substantially affecting rankings on Spider and BIRD\nbenchmarks; (2) The underestimation of models in EX primarily stems from\nannotation quality issues; and (3) Model performance on particularly\nchallenging questions tends to be overestimated. This work contributes to a\nmore accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field.\n","authors":["Heegyu Kim","Taeyang Jeon","Seunghwan Choi","Seungtaek Choi","Hyunsouk Cho"],"pdf_url":"https://arxiv.org/pdf/2409.19014v3.pdf","comment":"preprint, under review"},{"id":"http://arxiv.org/abs/2410.10114v2","updated":"2024-10-16T12:30:53Z","published":"2024-10-14T03:05:12Z","title":"Mixture of Experts Made Personalized: Federated Prompt Learning for\n  Vision-Language Models","summary":"  Prompt learning for pre-trained Vision-Language Models (VLMs) like CLIP has\ndemonstrated potent applicability across diverse downstream tasks. This\nlightweight approach has quickly gained traction from federated learning (FL)\nresearchers who seek to efficiently adapt VLMs to heterogeneous scenarios.\nHowever, current federated prompt learning methods are habitually restricted to\nthe traditional FL paradigm, where the participating clients are generally only\nallowed to download a single globally aggregated model from the server. While\njustifiable for training full-sized models under federated settings, in this\nwork, we argue that this paradigm is ill-suited for lightweight prompts. By\nfacilitating the clients to download multiple pre-aggregated prompts as fixed\nnon-local experts, we propose Personalized Federated Mixture of Adaptive\nPrompts (pFedMoAP), a novel FL framework that personalizes the prompt learning\nprocess through the lens of Mixture of Experts (MoE). pFedMoAP implements a\nlocal attention-based gating network that learns to generate enhanced text\nfeatures for better alignment with local image data on the client, benefiting\nfrom both local and downloaded non-local adaptive prompt experts. The non-local\nexperts are sparsely selected from a server-maintained pool, fostering\ncollaborative learning across clients. To evaluate the proposed algorithm, we\nconduct extensive experiments across 9 datasets under various heterogeneous\nfederated settings. The results show that pFedMoAP consistently outperforms the\nstate-of-the-art alternatives, underscoring its efficacy in personalizing\nprompt learning for CLIP within the federated learning paradigm.\n","authors":["Jun Luo","Chen Chen","Shandong Wu"],"pdf_url":"https://arxiv.org/pdf/2410.10114v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2311.07454v5","updated":"2024-10-16T12:28:25Z","published":"2023-11-13T16:35:34Z","title":"Causal Discovery under Latent Class Confounding","summary":"  An acyclic causal structure can be described with directed acyclic graph\n(DAG), where arrows indicate the possibility of direct causation. The task of\nlearning this structure from data is known as \"causal discovery.\" Diverse\npopulations or changing environments can sometimes give rise to data that is\nheterogeneous in the following sense: each population/environment is a \"source\"\nwhich idiosyncratically determines the forms of those direct causal effects.\nFrom this perspective, the source is a latent common cause for every observed\nvariable. While some methods for causal discovery are able to work around\nlatent confounding in special cases, especially when only few observables are\nconfounded, a global confounder is a difficult challenge. The only known ways\nto deal with latent global confounding involve assumptions that limit the\nstructural equations and/or noise functions. We demonstrate that globally\nconfounded causal structures can still be identifiable with arbitrary\nstructural equations and noise functions, so long as the number of latent\nclasses remains small relative to the size and sparsity of the underlying DAG.\n","authors":["Bijan Mazaheri","Spencer Gordon","Yuval Rabani","Leonard Schulman"],"pdf_url":"https://arxiv.org/pdf/2311.07454v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11929v4","updated":"2024-10-16T12:20:35Z","published":"2024-01-22T13:15:40Z","title":"Parsimony or Capability? Decomposition Delivers Both in Long-term Time\n  Series Forecasting","summary":"  Long-term time series forecasting (LTSF) represents a critical frontier in\ntime series analysis, characterized by extensive input sequences, as opposed to\nthe shorter spans typical of traditional approaches. While longer sequences\ninherently offer richer information for enhanced predictive precision,\nprevailing studies often respond by escalating model complexity. These\nintricate models can inflate into millions of parameters, resulting in\nprohibitive parameter scales. Our study demonstrates, through both analytical\nand empirical evidence, that decomposition is key to containing excessive model\ninflation while achieving uniformly superior and robust results across various\ndatasets. Remarkably, by tailoring decomposition to the intrinsic dynamics of\ntime series data, our proposed model outperforms existing benchmarks, using\nover 99 \\% fewer parameters than the majority of competing methods. Through\nthis work, we aim to unleash the power of a restricted set of parameters by\ncapitalizing on domain characteristics--a timely reminder that in the realm of\nLTSF, bigger is not invariably better.\n","authors":["Jinliang Deng","Feiyang Ye","Du Yin","Xuan Song","Ivor W. Tsang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2401.11929v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12492v1","updated":"2024-10-16T12:14:29Z","published":"2024-10-16T12:14:29Z","title":"End-to-end Planner Training for Language Modeling","summary":"  Through end-to-end training to predict the next token, LLMs have become\nvaluable tools for various tasks. Enhancing their core training in language\nmodeling can improve numerous downstream applications. A successful approach to\nenhance language modeling uses a separate planning module to predict abstract\nlabels of future sentences and conditions the LM on these predictions. However,\nthis method is non-differentiable, preventing joint end-to-end tuning of the\nplanner with the LM. We propose an effective method to improve this approach by\nenabling joint fine-tuning of the planner and the LM. We show that a naive way\nof approximating the gradient of selecting a label via the straight-through\nestimator is not effective. Instead, we propose to use the predicted label\nprobabilities as mixing weights to condition the LM on a weighted average of\nlabel embeddings in a differentiable manner. This not only enables joint\nfine-tuning of the planner and the LM, but also allows the LM to draw on the\nfull label distribution predicted by the planner, retaining more information.\nOur experimental results show consistent improvements in perplexity.\n","authors":["Nathan Cornille","Florian Mai","Jingyuan Sun","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2410.12492v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2410.04492v3","updated":"2024-10-16T12:07:27Z","published":"2024-10-06T14:11:39Z","title":"Interpret Your Decision: Logical Reasoning Regularization for\n  Generalization in Visual Classification","summary":"  Vision models excel in image classification but struggle to generalize to\nunseen data, such as classifying images from unseen domains or discovering\nnovel categories. In this paper, we explore the relationship between logical\nreasoning and deep learning generalization in visual classification. A logical\nregularization termed L-Reg is derived which bridges a logical analysis\nframework to image classification. Our work reveals that L-Reg reduces the\ncomplexity of the model in terms of the feature distribution and classifier\nweights. Specifically, we unveil the interpretability brought by L-Reg, as it\nenables the model to extract the salient features, such as faces to persons,\nfor classification. Theoretical analysis and experiments demonstrate that L-Reg\nenhances generalization across various scenarios, including multi-domain\ngeneralization and generalized category discovery. In complex real-world\nscenarios where images span unknown classes and unseen domains, L-Reg\nconsistently improves generalization, highlighting its practical efficacy.\n","authors":["Zhaorui Tan","Xi Yang","Qiufeng Wang","Anh Nguyen","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2410.04492v3.pdf","comment":"Accepted by NeurIPS2024 as Spotlight"},{"id":"http://arxiv.org/abs/2410.12485v1","updated":"2024-10-16T12:03:37Z","published":"2024-10-16T12:03:37Z","title":"Data-Driven Gyroscope Calibration","summary":"  Gyroscopes are inertial sensors that measure the angular velocity of the\nplatforms to which they are attached. To estimate the gyroscope deterministic\nerror terms prior mission start, a calibration procedure is performed. When\nconsidering low-cost gyroscopes, the calibration requires a turntable as the\ngyros are incapable of sensing the Earth turn rate. In this paper, we propose a\ndata-driven framework to estimate the scale factor and bias of a gyroscope. To\ntrain and validate our approach, a dataset of 56 minutes was recorded using a\nturntable. We demonstrated that our proposed approach outperforms the\nmodel-based approach, in terms of accuracy and convergence time. Specifically,\nwe improved the scale factor and bias estimation by an average of 72% during\nsix seconds of calibration time, demonstrating an average of 75% calibration\ntime improvement. That is, instead of minutes, our approach requires only\nseveral seconds for the calibration.\n","authors":["Zeev Yampolsky","Itzik Klein"],"pdf_url":"https://arxiv.org/pdf/2410.12485v1.pdf","comment":"19 Pages, 5 Figures, 3 Tables"},{"id":"http://arxiv.org/abs/2406.16535v2","updated":"2024-10-16T12:00:46Z","published":"2024-06-24T11:16:26Z","title":"Token-based Decision Criteria Are Suboptimal in In-context Learning","summary":"  In-Context Learning (ICL) typically utilizes classification criteria from\noutput probabilities of manually selected label tokens. However, we argue that\nsuch token-based classification criteria lead to suboptimal decision\nboundaries, despite delicate calibrations through translation and constrained\nrotation applied. To address this problem, we propose Hidden Calibration, which\nrenounces token probabilities and uses the nearest centroid classifier on the\nLM's last hidden states. In detail, we assign the label of the nearest centroid\npreviously estimated from a calibration set to the test sample as the predicted\nlabel. Our experiments on 6 models and 10 classification datasets indicate that\nHidden Calibration consistently outperforms current token-based baselines by\nabout 20%~50%, achieving a strong state-of-the-art in ICL. Our further analysis\ndemonstrates that Hidden Calibration finds better classification criteria with\nless inter-class overlap, and LMs provide linearly separable intra-class\nclusters with the help of demonstrations, which supports Hidden Calibration and\ngives new insights into the principle of ICL.\n","authors":["Hakaze Cho","Yoshihiro Sakai","Mariko Kato","Kenshiro Tanaka","Akira Ishii","Naoya Inoue"],"pdf_url":"https://arxiv.org/pdf/2406.16535v2.pdf","comment":"24 pages, 15 figures, 13 tables"},{"id":"http://arxiv.org/abs/2410.12481v1","updated":"2024-10-16T11:59:27Z","published":"2024-10-16T11:59:27Z","title":"SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and\n  Hindsight Relabeling","summary":"  The past years have seen Large Language Models (LLMs) strive not only as\ngenerative models but also as agents solving textual sequential decision-making\ntasks. When facing complex environments where their zero-shot abilities are\ninsufficient, recent work showed online Reinforcement Learning (RL) could be\nused for the LLM agent to discover and learn efficient strategies\ninteractively. However, most prior work sticks to on-policy algorithms, which\ngreatly reduces the scope of methods such agents could use for both exploration\nand exploitation, such as experience replay and hindsight relabeling. Yet, such\nmethods may be key for LLM learning agents, and in particular when designing\nautonomous intrinsically motivated agents sampling and pursuing their own goals\n(i.e. autotelic agents). This paper presents and studies an adaptation of Soft\nActor-Critic and hindsight relabeling to LLM agents. Our method not only paves\nthe path towards autotelic LLM agents that learn online but can also outperform\non-policy methods in more classic multi-goal RL environments.\n","authors":["Loris Gaven","Clement Romac","Thomas Carta","Sylvain Lamprier","Olivier Sigaud","Pierre-Yves Oudeyer"],"pdf_url":"https://arxiv.org/pdf/2410.12481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11224v2","updated":"2024-10-16T11:56:57Z","published":"2024-10-15T03:09:06Z","title":"DeltaDock: A Unified Framework for Accurate, Efficient, and Physically\n  Reliable Molecular Docking","summary":"  Molecular docking, a technique for predicting ligand binding poses, is\ncrucial in structure-based drug design for understanding protein-ligand\ninteractions. Recent advancements in docking methods, particularly those\nleveraging geometric deep learning (GDL), have demonstrated significant\nefficiency and accuracy advantages over traditional sampling methods. Despite\nthese advancements, current methods are often tailored for specific docking\nsettings, and limitations such as the neglect of protein side-chain structures,\ndifficulties in handling large binding pockets, and challenges in predicting\nphysically valid structures exist. To accommodate various docking settings and\nachieve accurate, efficient, and physically reliable docking, we propose a\nnovel two-stage docking framework, DeltaDock, consisting of pocket prediction\nand site-specific docking. We innovatively reframe the pocket prediction task\nas a pocket-ligand alignment problem rather than direct prediction in the first\nstage. Then we follow a bi-level coarse-to-fine iterative refinement process to\nperform site-specific docking. Comprehensive experiments demonstrate the\nsuperior performance of DeltaDock. Notably, in the blind docking setting,\nDeltaDock achieves a 31\\% relative improvement over the docking success rate\ncompared with the previous state-of-the-art GDL model. With the consideration\nof physical validity, this improvement increases to about 300\\%.\n","authors":["Jiaxian Yan","Zaixi Zhang","Jintao Zhu","Kai Zhang","Jianfeng Pei","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2410.11224v2.pdf","comment":"Accepted by NeurIPS'24"},{"id":"http://arxiv.org/abs/2410.12480v1","updated":"2024-10-16T11:50:02Z","published":"2024-10-16T11:50:02Z","title":"KcMF: A Knowledge-compliant Framework for Schema and Entity Matching\n  with Fine-tuning-free LLMs","summary":"  Schema and entity matching tasks are crucial for data integration and\nmanagement. While large language models (LLMs) have shown promising results in\nthese tasks, they suffer from hallucinations and confusion about task\ninstructions. In this paper, we present the Knowledge-Compliant Matching\nFramework (KcMF), an LLM-based approach that addresses these issues without the\nneed for domain-specific fine-tuning. KcMF employs a pseudo-code-based task\ndecomposition strategy to adopt task-specific natural language statements that\nguide LLM reasoning and reduce confusion. We also propose two mechanisms,\nDataset as Knowledge (DaK) and Example as Knowledge (EaK), to build domain\nknowledge sets when unstructured domain knowledge is lacking. Additionally, we\nintroduce a result-ensembling strategy to leverage multiple knowledge sources\nand suppress poorly formatted outputs. Comprehensive evaluations on schema and\nentity matching tasks demonstrate that KcMF outperforms previous non-LLM\nstate-of-the-art (SOTA) methods by an average F1 score of 22.9% and competes\neffectively with SOTA fine-tuned LLMs. Moreover, KcMF generalizes well across\ndifferent LLMs.\n","authors":["Yongqin Xu","Huan Li","Ke Chen","Lidan Shou"],"pdf_url":"https://arxiv.org/pdf/2410.12480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12476v1","updated":"2024-10-16T11:46:32Z","published":"2024-10-16T11:46:32Z","title":"Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial\n  Generation","summary":"  Machine learning (ML) exhibits promise in the clinical domain. However, it is\nconstrained by data scarcity and ethical considerations, as the generation of\nclinical trials presents significant challenges due to stringent privacy\nregulations, high costs, and the extended duration required for conducting\nstudies with human participants. Despite the advancements of large language\nmodels (LLMs) in general generation tasks, their potential in facilitating the\ngeneration of synthetic clinical trials is under-explored. To address this gap,\nwe introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs\nto generate artificial yet realistic and diverse clinical trials with binary\nsuccess/failure labels. Experiments conducted on real clinical trials from the\n\\url{ClinicalTrials.gov} database demonstrate that our synthetic data can\neffectively augment real datasets. Furthermore, by fine-tuning a pre-trained\nmodel as a binary classifier on synthetic clinical trial datasets, we\ndemonstrate that this augmentation enhances model training for downstream tasks\nsuch as trial outcome prediction. Our findings suggest that LLMs for synthetic\nclinical trial generation hold promise for accelerating clinical research and\nupholding ethical standards for patient privacy. The code is publicly available\nat\nhttps://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.\n","authors":["Zerui Xu","Fang Wu","Tianfan Fu","Yue Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.12476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.12146v8","updated":"2024-10-16T11:45:22Z","published":"2021-11-23T20:41:06Z","title":"Sharing to learn and learning to share; Fitting together Meta-Learning,\n  Multi-Task Learning, and Transfer Learning: A meta review","summary":"  Integrating knowledge across different domains is an essential feature of\nhuman learning. Learning paradigms such as transfer learning, meta-learning,\nand multi-task learning reflect the human learning process by exploiting the\nprior knowledge for new tasks, encouraging faster learning and good\ngeneralization for new tasks. This article gives a detailed view of these\nlearning paradigms and their comparative analysis. The weakness of one learning\nalgorithm turns out to be a strength of another, and thus, merging them is a\nprevalent trait in the literature. Numerous research papers focus on each of\nthese learning paradigms separately and provide a comprehensive overview of\nthem. However, this article reviews research studies that combine (two of)\nthese learning algorithms. This survey describes how these techniques are\ncombined to solve problems in many different fields of research, including\ncomputer vision, natural language processing, hyper-spectral imaging, and many\nmore, in a supervised setting only. Based on the knowledge accumulated from the\nliterature, we hypothesize a generic task-agnostic and model-agnostic learning\nnetwork - an ensemble of meta-learning, transfer learning, and multi-task\nlearning, termed Multi-modal Multi-task Meta Transfer Learning. We also present\nsome open research questions, limitations, and future research directions for\nthis proposed network. The aim of this article is to spark interest among\nscholars in effectively merging existing learning algorithms with the intention\nof advancing research in this field. Instead of presenting experimental\nresults, we invite readers to explore and contemplate techniques for merging\nalgorithms while navigating through their limitations.\n","authors":["Richa Upadhyay","Ronald Phlypo","Rajkumar Saini","Marcus Liwicki"],"pdf_url":"https://arxiv.org/pdf/2111.12146v8.pdf","comment":"This article has been accepted for publication in IEEE Access. This\n  is the author's version which has not been fully edited and content may\n  slightly change prior to final publication. Citation information: DOI\n  10.1109/ACCESS.2024.3478805"},{"id":"http://arxiv.org/abs/2410.12474v1","updated":"2024-10-16T11:42:11Z","published":"2024-10-16T11:42:11Z","title":"Mind the Gap Between Prototypes and Images in Cross-domain Finetuning","summary":"  In cross-domain few-shot classification (CFC), recent works mainly focus on\nadapting a simple transformation head on top of a frozen pre-trained backbone\nwith few labeled data to project embeddings into a task-specific metric space\nwhere classification can be performed by measuring similarities between image\ninstance and prototype representations. Technically, an assumption implicitly\nadopted in such a framework is that the prototype and image instance embeddings\nshare the same representation transformation. However, in this paper, we find\nthat there naturally exists a gap, which resembles the modality gap, between\nthe prototype and image instance embeddings extracted from the frozen\npre-trained backbone, and simply applying the same transformation during the\nadaptation phase constrains exploring the optimal representations and shrinks\nthe gap between prototype and image representations. To solve this problem, we\npropose a simple yet effective method, contrastive prototype-image adaptation\n(CoPA), to adapt different transformations respectively for prototypes and\nimages similarly to CLIP by treating prototypes as text prompts. Extensive\nexperiments on Meta-Dataset demonstrate that CoPA achieves the state-of-the-art\nperformance more efficiently. Meanwhile, further analyses also indicate that\nCoPA can learn better representation clusters, enlarge the gap, and achieve\nminimal validation loss at the enlarged gap.\n","authors":["Hongduan Tian","Feng Liu","Zhanke Zhou","Tongliang Liu","Chengqi Zhang","Bo Han"],"pdf_url":"https://arxiv.org/pdf/2410.12474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12461v1","updated":"2024-10-16T11:21:07Z","published":"2024-10-16T11:21:07Z","title":"Challenges, Methods, Data -- a Survey of Machine Learning in Water\n  Distribution Networks","summary":"  Research on methods for planning and controlling water distribution networks\ngains increasing relevance as the availability of drinking water will decrease\nas a consequence of climate change. So far, the majority of approaches is based\non hydraulics and engineering expertise. However, with the increasing\navailability of sensors, machine learning techniques constitute a promising\ntool. This work presents the main tasks in water distribution networks,\ndiscusses how they relate to machine learning and analyses how the\nparticularities of the domain pose challenges to and can be leveraged by\nmachine learning approaches. Besides, it provides a technical toolkit by\npresenting evaluation benchmarks and a structured survey of the exemplary task\nof leakage detection and localization.\n","authors":["Valerie Vaquet","Fabian Hinder","André Artelt","Inaam Ashraf","Janine Strotherm","Jonas Vaquet","Johannes Brinkrolf","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2410.12461v1.pdf","comment":"This preprint has not undergone any post-submission improvements or\n  corrections. The Version of Record of this contribution is published in\n  Artificial Neural Networks and Machine Learning -- ICANN 2024"},{"id":"http://arxiv.org/abs/2410.11190v2","updated":"2024-10-16T11:19:56Z","published":"2024-10-15T02:10:45Z","title":"Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex\n  Capabilities","summary":"  GPT-4o, an all-encompassing model, represents a milestone in the development\nof large multi-modal language models. It can understand visual, auditory, and\ntextual modalities, directly output audio, and support flexible duplex\ninteraction. Models from the open-source community often achieve some\nfunctionalities of GPT-4o, such as visual understanding and voice chat.\nNevertheless, training a unified model that incorporates all modalities is\nchallenging due to the complexities of multi-modal data, intricate model\narchitectures, and training processes. In this paper, we introduce Mini-Omni2,\na visual-audio assistant capable of providing real-time, end-to-end voice\nresponses to visoin and audio queries. By integrating pretrained visual and\nauditory encoders, Mini-Omni2 maintains performance in individual modalities.\nWe propose a three-stage training process to align modalities, allowing the\nlanguage model to handle multi-modal inputs and outputs after training on a\nlimited dataset. For interaction, we introduce a command-based interruption\nmechanism, enabling more flexible interaction with users. To the best of our\nknowledge, Mini-Omni2 is one of the closest reproductions of GPT-4o, which have\nsimilar form of functionality, and we hope it can offer valuable insights for\nsubsequent research.\n","authors":["Zhifei Xie","Changqiao Wu"],"pdf_url":"https://arxiv.org/pdf/2410.11190v2.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.12459v1","updated":"2024-10-16T11:16:47Z","published":"2024-10-16T11:16:47Z","title":"HELM: Hierarchical Encoding for mRNA Language Modeling","summary":"  Messenger RNA (mRNA) plays a crucial role in protein synthesis, with its\ncodon structure directly impacting biological properties. While Language Models\n(LMs) have shown promise in analyzing biological sequences, existing approaches\nfail to account for the hierarchical nature of mRNA's codon structure. We\nintroduce Hierarchical Encoding for mRNA Language Modeling (HELM), a novel\npre-training strategy that incorporates codon-level hierarchical structure into\nlanguage model training. HELM modulates the loss function based on codon\nsynonymity, aligning the model's learning process with the biological reality\nof mRNA sequences. We evaluate HELM on diverse mRNA datasets and tasks,\ndemonstrating that HELM outperforms standard language model pre-training as\nwell as existing foundation model baselines on six diverse downstream property\nprediction tasks and an antibody region annotation tasks on average by around\n8\\%. Additionally, HELM enhances the generative capabilities of language model,\nproducing diverse mRNA sequences that better align with the underlying true\ndata distribution compared to non-hierarchical baselines.\n","authors":["Mehdi Yazdani-Jahromi","Mangal Prakash","Tommaso Mansi","Artem Moskalev","Rui Liao"],"pdf_url":"https://arxiv.org/pdf/2410.12459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12457v1","updated":"2024-10-16T11:08:06Z","published":"2024-10-16T11:08:06Z","title":"Sharpness-Aware Black-Box Optimization","summary":"  Black-box optimization algorithms have been widely used in various machine\nlearning problems, including reinforcement learning and prompt fine-tuning.\nHowever, directly optimizing the training loss value, as commonly done in\nexisting black-box optimization methods, could lead to suboptimal model quality\nand generalization performance. To address those problems in black-box\noptimization, we propose a novel Sharpness-Aware Black-box Optimization (SABO)\nalgorithm, which applies a sharpness-aware minimization strategy to improve the\nmodel generalization. Specifically, the proposed SABO method first\nreparameterizes the objective function by its expectation over a Gaussian\ndistribution. Then it iteratively updates the parameterized distribution by\napproximated stochastic gradients of the maximum objective value within a small\nneighborhood around the current solution in the Gaussian distribution space.\nTheoretically, we prove the convergence rate and generalization bound of the\nproposed SABO algorithm. Empirically, extensive experiments on the black-box\nprompt fine-tuning tasks demonstrate the effectiveness of the proposed SABO\nmethod in improving model generalization performance.\n","authors":["Feiyang Ye","Yueming Lyu","Xuehao Wang","Masashi Sugiyama","Yu Zhang","Ivor Tsang"],"pdf_url":"https://arxiv.org/pdf/2410.12457v1.pdf","comment":"27 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.12456v1","updated":"2024-10-16T11:08:02Z","published":"2024-10-16T11:08:02Z","title":"Training Neural Samplers with Reverse Diffusive KL Divergence","summary":"  Training generative models to sample from unnormalized density functions is\nan important and challenging task in machine learning. Traditional training\nmethods often rely on the reverse Kullback-Leibler (KL) divergence due to its\ntractability. However, the mode-seeking behavior of reverse KL hinders\neffective approximation of multi-modal target distributions. To address this,\nwe propose to minimize the reverse KL along diffusion trajectories of both\nmodel and target densities. We refer to this objective as the reverse diffusive\nKL divergence, which allows the model to capture multiple modes. Leveraging\nthis objective, we train neural samplers that can efficiently generate samples\nfrom the target distribution in one step. We demonstrate that our method\nenhances sampling performance across various Boltzmann distributions, including\nboth synthetic multi-modal densities and n-body particle systems.\n","authors":["Jiajun He","Wenlin Chen","Mingtian Zhang","David Barber","José Miguel Hernández-Lobato"],"pdf_url":"https://arxiv.org/pdf/2410.12456v1.pdf","comment":"23 pages, 6 figures, 3 tables, 1 algorithm"},{"id":"http://arxiv.org/abs/2410.12455v1","updated":"2024-10-16T11:05:43Z","published":"2024-10-16T11:05:43Z","title":"Loss Landscape Characterization of Neural Networks without\n  Over-Parametrziation","summary":"  Optimization methods play a crucial role in modern machine learning, powering\nthe remarkable empirical achievements of deep learning models. These successes\nare even more remarkable given the complex non-convex nature of the loss\nlandscape of these models. Yet, ensuring the convergence of optimization\nmethods requires specific structural conditions on the objective function that\nare rarely satisfied in practice. One prominent example is the widely\nrecognized Polyak-Lojasiewicz (PL) inequality, which has gained considerable\nattention in recent years. However, validating such assumptions for deep neural\nnetworks entails substantial and often impractical levels of\nover-parametrization. In order to address this limitation, we propose a novel\nclass of functions that can characterize the loss landscape of modern deep\nmodels without requiring extensive over-parametrization and can also include\nsaddle points. Crucially, we prove that gradient-based optimizers possess\ntheoretical guarantees of convergence under this assumption. Finally, we\nvalidate the soundness of our new function class through both theoretical\nanalysis and empirical experimentation across a diverse range of deep learning\nmodels.\n","authors":["Rustem Islamov","Niccolò Ajroldi","Antonio Orvieto","Aurelien Lucchi"],"pdf_url":"https://arxiv.org/pdf/2410.12455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12452v1","updated":"2024-10-16T11:00:25Z","published":"2024-10-16T11:00:25Z","title":"FairGLVQ: Fairness in Partition-Based Classification","summary":"  Fairness is an important objective throughout society. From the distribution\nof limited goods such as education, over hiring and payment, to taxes,\nlegislation, and jurisprudence. Due to the increasing importance of machine\nlearning approaches in all areas of daily life including those related to\nhealth, security, and equity, an increasing amount of research focuses on fair\nmachine learning. In this work, we focus on the fairness of partition- and\nprototype-based models. The contribution of this work is twofold: 1) we develop\na general framework for fair machine learning of partition-based models that\ndoes not depend on a specific fairness definition, and 2) we derive a fair\nversion of learning vector quantization (LVQ) as a specific instantiation. We\ncompare the resulting algorithm against other algorithms from the literature on\ntheoretical and real-world data showing its practical relevance.\n","authors":["Felix Störck","Fabian Hinder","Johannes Brinkrolf","Benjamin Paassen","Valerie Vaquet","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2410.12452v1.pdf","comment":"This preprint has not undergone any post-submission improvements or\n  corrections. The Version of Record of this contribution is published in\n  Advances in Self-Organizing Maps, Learning Vector Quantization, Interpretable\n  Machine Learning, and Beyond"},{"id":"http://arxiv.org/abs/2410.11267v2","updated":"2024-10-16T11:00:09Z","published":"2024-10-15T04:44:21Z","title":"FedCCRL: Federated Domain Generalization with Cross-Client\n  Representation Learning","summary":"  Domain Generalization (DG) aims to train models that can effectively\ngeneralize to unseen domains. However, in the context of Federated Learning\n(FL), where clients collaboratively train a model without directly sharing\ntheir data, most existing DG algorithms are not directly applicable to the FL\nsetting due to privacy constraints, as well as the limited data quantity and\ndomain diversity at each client. To tackle these challenges, we propose\nFedCCRL, a novel federated domain generalization method that significantly\nimproves the model's ability to generalize to unseen domains without\ncompromising privacy or incurring excessive computational and communication\ncosts. Specifically, we adapt MixStyle to the federated setting to transfer\ndomain-specific features while AugMix is employed to perturb domain-invariant\nfeatures. Furthermore, we leverage supervised contrastive loss for\nrepresentation alignment and utilize Jensen-Shannon divergence to ensure\nconsistent predictions between original and augmented samples. Extensive\nexperimental results demonstrate that FedCCRL achieves the state-of-the-art\nperformances on the PACS, OfficeHome and miniDomainNet datasets across varying\nnumbers of clients. Code is available at\nhttps://github.com/SanphouWang/FedCCRL.\n","authors":["Xinpeng Wang","Xiaoying Tang"],"pdf_url":"https://arxiv.org/pdf/2410.11267v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12443v1","updated":"2024-10-16T10:41:17Z","published":"2024-10-16T10:41:17Z","title":"Reconstruction of Differentially Private Text Sanitization via Large\n  Language Models","summary":"  Differential privacy (DP) is the de facto privacy standard against privacy\nleakage attacks, including many recently discovered ones against large language\nmodels (LLMs). However, we discovered that LLMs could reconstruct the\naltered/removed privacy from given DP-sanitized prompts. We propose two attacks\n(black-box and white-box) based on the accessibility to LLMs and show that LLMs\ncould connect the pair of DP-sanitized text and the corresponding private\ntraining data of LLMs by giving sample text pairs as instructions (in the\nblack-box attacks) or fine-tuning data (in the white-box attacks). To\nillustrate our findings, we conduct comprehensive experiments on modern LLMs\n(e.g., LLaMA-2, LLaMA-3, ChatGPT-3.5, ChatGPT-4, ChatGPT-4o, Claude-3,\nClaude-3.5, OPT, GPT-Neo, GPT-J, Gemma-2, and Pythia) using commonly used\ndatasets (such as WikiMIA, Pile-CC, and Pile-Wiki) against both word-level and\nsentence-level DP. The experimental results show promising recovery rates,\ne.g., the black-box attacks against the word-level DP over WikiMIA dataset gave\n72.18% on LLaMA-2 (70B), 82.39% on LLaMA-3 (70B), 75.35% on Gemma-2, 91.2% on\nChatGPT-4o, and 94.01% on Claude-3.5 (Sonnet). More urgently, this study\nindicates that these well-known LLMs have emerged as a new security risk for\nexisting DP text sanitization approaches in the current environment.\n","authors":["Shuchao Pang","Zhigang Lu","Haichen Wang","Peng Fu","Yongbin Zhou","Minhui Xue","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2410.12443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12439v1","updated":"2024-10-16T10:34:11Z","published":"2024-10-16T10:34:11Z","title":"ConLUX: Concept-Based Local Unified Explanations","summary":"  With the rapid advancements of various machine learning models, there is a\nsignificant demand for model-agnostic explanation techniques, which can explain\nthese models across different architectures. Mainstream model-agnostic\nexplanation techniques generate local explanations based on basic features\n(e.g., words for text models and (super-)pixels for image models). However,\nthese explanations often do not align with the decision-making processes of the\ntarget models and end-users, resulting in explanations that are unfaithful and\ndifficult for users to understand. On the other hand, concept-based techniques\nprovide explanations based on high-level features (e.g., topics for text models\nand objects for image models), but most are model-specific or require\nadditional pre-defined external concept knowledge. To address this limitation,\nwe propose \\toolname, a general framework to provide concept-based local\nexplanations for any machine learning models. Our key insight is that we can\nautomatically extract high-level concepts from large pre-trained models, and\nuniformly extend existing local model-agnostic techniques to provide unified\nconcept-based explanations. We have instantiated \\toolname on four different\ntypes of explanation techniques: LIME, Kernel SHAP, Anchor, and LORE, and\napplied these techniques to text and image models. Our evaluation results\ndemonstrate that 1) compared to the vanilla versions, \\toolname offers more\nfaithful explanations and makes them more understandable to users, and 2) by\noffering multiple forms of explanations, \\toolname outperforms state-of-the-art\nconcept-based explanation techniques specifically designed for text and image\nmodels, respectively.\n","authors":["Junhao Liu","Haonan Yu","Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12435v1","updated":"2024-10-16T10:28:22Z","published":"2024-10-16T10:28:22Z","title":"Approaching Metaheuristic Deep Learning Combos for Automated Data Mining","summary":"  Lack of data on which to perform experimentation is a recurring issue in many\nareas of research, particularly in machine learning. The inability of most\nautomated data mining techniques to be generalized to all types of data is\ninherently related with their dependency on those types which deems them\nineffective against anything slightly different. Meta-heuristics are algorithms\nwhich attempt to optimize some solution independently of the type of data used,\nwhilst classifiers or neural networks focus on feature extrapolation and\ndimensionality reduction to fit some model onto data arranged in a particular\nway. These two algorithmic fields encompass a group of characteristics which\nwhen combined are seemingly capable of achieving data mining regardless of how\nit is arranged. To this end, this work proposes a means of combining\nmeta-heuristic methods with conventional classifiers and neural networks in\norder to perform automated data mining. Experiments on the MNIST dataset for\nhandwritten digit recognition were performed and it was empirically observed\nthat using a ground truth labeled dataset's validation accuracy is inadequate\nfor correcting labels of other previously unseen data instances.\n","authors":["Gustavo Assunção","Paulo Menezes"],"pdf_url":"https://arxiv.org/pdf/2410.12435v1.pdf","comment":"Tentative submission for data mining and knowledge discovery"},{"id":"http://arxiv.org/abs/2406.03402v2","updated":"2024-10-16T10:14:36Z","published":"2024-06-04T09:07:45Z","title":"Mixed-Precision Federated Learning via Multi-Precision Over-The-Air\n  Aggregation","summary":"  Over-the-Air Federated Learning (OTA-FL) is a privacy-preserving distributed\nlearning mechanism, by aggregating updates in the electromagnetic channel\nrather than at the server. A critical research gap in existing OTA-FL research\nis the assumption of homogeneous client computational bit precision. While in\nreal world application, clients with varying hardware resources may exploit\napproximate computing (AxC) to operate at different bit precisions optimized\nfor energy and computational efficiency. And model updates of various\nprecisions amongst clients poses an open challenge for OTA-FL, as it is\nincompatible in the wireless modulation superposition. Here, we propose an\nmixed-precision OTA-FL framework of clients with multiple bit precisions,\ndemonstrating the following innovations: (i) the superior trade-off for both\nserver and clients within the constraints of varying edge computing\ncapabilities, energy efficiency, and learning accuracy requirements comparing\nto homogeneous client bit precision, and (ii) a multi-precision gradient\nmodulation scheme to ensure compatibility with OTA aggregation and eliminate\nthe overheads of precision conversion. Through case study with real world data,\nwe validate our modulation scheme that enables AxC based mixed-precision\nOTA-FL. In comparison to homogeneous standard precision of 32-bit and 16-bit,\nour framework presents more than 10% in 4-bit ultra low precision client\nperformance and over 65%and 13% of energy savings respectively. This\ndemonstrates the great potential of our mixed-precision OTA-FL approach in\nheterogeneous edge computing environments.\n","authors":["Jinsheng Yuan","Zhuangkun Wei","Weisi Guo"],"pdf_url":"https://arxiv.org/pdf/2406.03402v2.pdf","comment":"Submitted to WCNC 2025"},{"id":"http://arxiv.org/abs/2410.12425v1","updated":"2024-10-16T10:08:02Z","published":"2024-10-16T10:08:02Z","title":"Perseus: Leveraging Common Data Patterns with Curriculum Learning for\n  More Robust Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) excel at handling graph data but remain\nvulnerable to adversarial attacks. Existing defense methods typically rely on\nassumptions like graph sparsity and homophily to either preprocess the graph or\nguide structure learning. However, preprocessing methods often struggle to\naccurately distinguish between normal edges and adversarial perturbations,\nleading to suboptimal results due to the loss of valuable edge information.\nRobust graph neural network models train directly on graph data affected by\nadversarial perturbations, without preprocessing. This can cause the model to\nget stuck in poor local optima, negatively affecting its performance. To\naddress these challenges, we propose Perseus, a novel adversarial defense\nmethod based on curriculum learning. Perseus assesses edge difficulty using\nglobal homophily and applies a curriculum learning strategy to adjust the\nlearning order, guiding the model to learn the full graph structure while\nadaptively focusing on common data patterns. This approach mitigates the impact\nof adversarial perturbations. Experiments show that models trained with Perseus\nachieve superior performance and are significantly more robust to adversarial\nattacks.\n","authors":["Kaiwen Xia","Huijun Wu","Duanyu Li","Min Xie","Ruibo Wang","Wenzhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12424v1","updated":"2024-10-16T10:07:07Z","published":"2024-10-16T10:07:07Z","title":"Nonlinear bayesian tomography of ion temperature and velocity for\n  Doppler coherence imaging spectroscopy in RT-1","summary":"  We present a novel Bayesian tomography approach for Coherence Imaging\nSpectroscopy (CIS) that simultaneously reconstructs ion temperature and\nvelocity distributions in plasmas. Utilizing nonlinear Gaussian Process\nTomography (GPT) with the Laplace approximation, we model prior distributions\nof log-emissivity, temperature, and velocity as Gaussian processes. This\nframework rigorously incorporates nonlinear effects and temperature\ndependencies often neglected in conventional CIS tomography, enabling robust\nreconstruction even in the region of high temperature and velocity. By applying\na log-Gaussian process, we also address issues like velocity divergence in\nlow-emissivity regions. Validated with phantom simulations and experimental\ndata from the RT-1 device, our method reveals detailed spatial structures of\nion temperature and toroidal ion flow characteristic of magnetospheric plasma.\nThis work significantly broadens the scope of CIS tomography, offering a robust\ntool for plasma diagnostics and facilitating integration with complementary\nmeasurement techniques.\n","authors":["Kenji Ueda","Masaki. Nishiura"],"pdf_url":"https://arxiv.org/pdf/2410.12424v1.pdf","comment":"13 page, 9 figures"},{"id":"http://arxiv.org/abs/2403.00853v2","updated":"2024-10-16T09:39:02Z","published":"2024-02-29T18:03:03Z","title":"Parallel Momentum Methods Under Biased Gradient Estimations","summary":"  Parallel stochastic gradient methods are gaining prominence in solving\nlarge-scale machine learning problems that involve data distributed across\nmultiple nodes. However, obtaining unbiased stochastic gradients, which have\nbeen the focus of most theoretical research, is challenging in many distributed\nmachine learning applications. The gradient estimations easily become biased,\nfor example, when gradients are compressed or clipped, when data is shuffled,\nand in meta-learning and reinforcement learning. In this work, we establish\nworst-case bounds on parallel momentum methods under biased gradient estimation\non both general non-convex and $\\mu$-PL problems. Our analysis covers general\ndistributed optimization problems, and we work out the implications for special\ncases where gradient estimates are biased, i.e. in meta-learning and when the\ngradients are compressed or clipped. Our numerical experiments verify our\ntheoretical findings and show faster convergence performance of momentum\nmethods than traditional biased gradient descent.\n","authors":["Ali Beikmohammadi","Sarit Khirirat","Sindri Magnússon"],"pdf_url":"https://arxiv.org/pdf/2403.00853v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2308.01472v2","updated":"2024-10-16T09:37:11Z","published":"2023-08-02T23:39:29Z","title":"Reverse Stable Diffusion: What prompt was used to generate this image?","summary":"  Text-to-image diffusion models have recently attracted the interest of many\nresearchers, and inverting the diffusion process can play an important role in\nbetter understanding the generative process and how to engineer prompts in\norder to obtain the desired images. To this end, we study the task of\npredicting the prompt embedding given an image generated by a generative\ndiffusion model. We consider a series of white-box and black-box models (with\nand without access to the weights of the diffusion network) to deal with the\nproposed task. We propose a novel learning framework comprising a joint prompt\nregression and multi-label vocabulary classification objective that generates\nimproved prompts. To further improve our method, we employ a curriculum\nlearning procedure that promotes the learning of image-prompt pairs with lower\nlabeling noise (i.e. that are better aligned). We conduct experiments on the\nDiffusionDB data set, predicting text prompts from images generated by Stable\nDiffusion. In addition, we make an interesting discovery: training a diffusion\nmodel on the prompt generation task can make the model generate images that are\nmuch better aligned with the input prompts, when the model is directly reused\nfor text-to-image generation. Our code is publicly available for download at\nhttps://github.com/CroitoruAlin/Reverse-Stable-Diffusion.\n","authors":["Florinel-Alin Croitoru","Vlad Hondru","Radu Tudor Ionescu","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2308.01472v2.pdf","comment":"Accepted for publication in Computer Vision and Image Understanding"},{"id":"http://arxiv.org/abs/2405.19783v2","updated":"2024-10-16T09:28:22Z","published":"2024-05-30T07:48:32Z","title":"Instruction-Guided Visual Masking","summary":"  Instruction following is crucial in contemporary LLM. However, when extended\nto multimodal setting, it often suffers from misalignment between specific\ntextual instruction and targeted local region of an image. To achieve more\naccurate and nuanced multimodal instruction following, we introduce\nInstruction-guided Visual Masking (IVM), a new versatile visual grounding model\nthat is compatible with diverse multimodal models, such as LMM and robot model.\nBy constructing visual masks for instruction-irrelevant regions, IVM-enhanced\nmultimodal models can effectively focus on task-relevant image regions to\nbetter align with complex instructions. Specifically, we design a visual\nmasking data generation pipeline and create an IVM-Mix-1M dataset with 1\nmillion image-instruction pairs. We further introduce a new learning technique,\nDiscriminator Weighted Supervised Learning (DWSL) for preferential IVM training\nthat prioritizes high-quality data samples. Experimental results on generic\nmultimodal tasks such as VQA and embodied robotic control demonstrate the\nversatility of IVM, which as a plug-and-play tool, significantly boosts the\nperformance of diverse multimodal models, yielding new state-of-the-art results\nacross challenging multimodal benchmarks. Code, model and data are available at\nhttps://github.com/2toinf/IVM.\n","authors":["Jinliang Zheng","Jianxiong Li","Sijie Cheng","Yinan Zheng","Jiaming Li","Jihao Liu","Yu Liu","Jingjing Liu","Xianyuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2405.19783v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2401.02300v3","updated":"2024-10-16T09:20:47Z","published":"2024-01-04T14:42:29Z","title":"Collocation-based Robust Variational Physics-Informed Neural Networks\n  (CRVPINN)","summary":"  Physics-Informed Neural Networks (PINNs) have been successfully applied to\nsolve Partial Differential Equations (PDEs). Their loss function is founded on\na strong residual minimization scheme. Variational Physics-Informed Neural\nNetworks (VPINNs) are their natural extension to weak variational settings. In\nthis context, the recent work of Robust Variational Physics-Informed Neural\nNetworks (RVPINNs) highlights the importance of conveniently translating the\nnorms of the underlying continuum-level spaces to the discrete level.\nOtherwise, VPINNs might become unrobust, implying that residual minimization\nmight be highly uncorrelated with a desired minimization of the error in the\nenergy norm. However, applying this robustness to VPINNs typically entails\ndealing with the inverse of a Gram matrix, usually producing slow convergence\nspeeds during training. In this work, we accelerate the implementation of\nRVPINN, establishing a LU factorization of sparse Gram matrix in a kind of\npoint-collocation scheme with the same spirit as original PINNs. We call out\nmethod the Collocation-based Robust Variational Physics Informed Neural\nNetworks (CRVPINN). We test our efficient CRVPINN algorithm on Laplace,\nadvection-diffusion, and Stokes problems in two spatial dimensions.\n","authors":["Marcin Łoś","Tomasz Służalec","Paweł Maczuga","Askold Vilkha","Carlos Uriarte","Maciej Paszyński"],"pdf_url":"https://arxiv.org/pdf/2401.02300v3.pdf","comment":"39 pages, 16 figures"},{"id":"http://arxiv.org/abs/2410.12391v1","updated":"2024-10-16T09:18:39Z","published":"2024-10-16T09:18:39Z","title":"Tracking Universal Features Through Fine-Tuning and Model Merging","summary":"  We study how features emerge, disappear, and persist across models fine-tuned\non different domains of text. More specifically, we start from a base one-layer\nTransformer language model that is trained on a combination of the BabyLM\ncorpus, and a collection of Python code from The Stack. This base model is\nadapted to two new domains of text: TinyStories, and the Lua programming\nlanguage, respectively; and then these two models are merged using these two\nmodels using spherical linear interpolation. Our exploration aims to provide\ndeeper insights into the stability and transformation of features across\ntypical transfer-learning scenarios using small-scale models and sparse\nauto-encoders.\n","authors":["Niels Horn","Desmond Elliott"],"pdf_url":"https://arxiv.org/pdf/2410.12391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05898v3","updated":"2024-10-16T09:10:54Z","published":"2024-10-08T10:55:40Z","title":"Manifolds, Random Matrices and Spectral Gaps: The geometric phases of\n  generative diffusion","summary":"  In this paper, we investigate the latent geometry of generative diffusion\nmodels under the manifold hypothesis. To this purpose, we analyze the spectrum\nof eigenvalues (and singular values) of the Jacobian of the score function,\nwhose discontinuities (gaps) reveal the presence and dimensionality of distinct\nsub-manifolds. Using a statistical physics approach, we derive the spectral\ndistributions and formulas for the spectral gaps under several distributional\nassumptions and we compare these theoretical predictions with the spectra\nestimated from trained networks. Our analysis reveals the existence of three\ndistinct qualitative phases during the generative process: a trivial phase; a\nmanifold coverage phase where the diffusion process fits the distribution\ninternal to the manifold; a consolidation phase where the score becomes\northogonal to the manifold and all particles are projected on the support of\nthe data. This `division of labor' between different timescales provides an\nelegant explanation on why generative diffusion models are not affected by the\nmanifold overfitting phenomenon that plagues likelihood-based models, since the\ninternal distribution and the manifold geometry are produced at different time\npoints during generation.\n","authors":["Enrico Ventura","Beatrice Achilli","Gianluigi Silvestri","Carlo Lucibello","Luca Ambrogioni"],"pdf_url":"https://arxiv.org/pdf/2410.05898v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12367v1","updated":"2024-10-16T08:39:40Z","published":"2024-10-16T08:39:40Z","title":"Adaptive and Stratified Subsampling Techniques for High Dimensional\n  Non-Standard Data Environments","summary":"  This paper addresses the challenge of estimating high-dimensional parameters\nin non-standard data environments, where traditional methods often falter due\nto issues such as heavy-tailed distributions, data contamination, and dependent\nobservations. We propose robust subsampling techniques, specifically Adaptive\nImportance Sampling (AIS) and Stratified Subsampling, designed to enhance the\nreliability and efficiency of parameter estimation. Under some clearly outlined\nconditions, we establish consistency and asymptotic normality for the proposed\nestimators, providing non-asymptotic error bounds that quantify their\nperformance. Our theoretical foundations are complemented by controlled\nexperiments demonstrating the superiority of our methods over conventional\napproaches. By bridging the gap between theory and practice, this work offers\nsignificant contributions to robust statistical estimation, paving the way for\nadvancements in various applied domains.\n","authors":["Prateek Mittal","Jai Dalmotra","Joohi Chauhan"],"pdf_url":"https://arxiv.org/pdf/2410.12367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11769v2","updated":"2024-10-16T08:30:57Z","published":"2024-10-15T16:44:40Z","title":"Can Search-Based Testing with Pareto Optimization Effectively Cover\n  Failure-Revealing Test Inputs?","summary":"  Search-based software testing (SBST) is a widely adopted technique for\ntesting complex systems with large input spaces, such as Deep Learning-enabled\n(DL-enabled) systems. Many SBST techniques focus on Pareto-based optimization,\nwhere multiple objectives are optimized in parallel to reveal failures.\nHowever, it is important to ensure that identified failures are spread\nthroughout the entire failure-inducing area of a search domain and not\nclustered in a sub-region. This ensures that identified failures are\nsemantically diverse and reveal a wide range of underlying causes. In this\npaper, we present a theoretical argument explaining why testing based on Pareto\noptimization is inadequate for covering failure-inducing areas within a search\ndomain. We support our argument with empirical results obtained by applying two\nwidely used types of Pareto-based optimization techniques, namely NSGA-II (an\nevolutionary algorithm) and OMOPSO (a swarm-based Pareto-optimization\nalgorithm), to two DL-enabled systems: an industrial Automated Valet Parking\n(AVP) system and a system for classifying handwritten digits. We measure the\ncoverage of failure-revealing test inputs in the input space using a metric\nthat we refer to as the Coverage Inverted Distance quality indicator. Our\nresults show that NSGA-II-based search and OMOPSO are not more effective than a\nna\\\"ive random search baseline in covering test inputs that reveal failures.\nThe replication package for this study is available in a GitHub repository.\n","authors":["Lev Sorokin","Damir Safin","Shiva Nejati"],"pdf_url":"https://arxiv.org/pdf/2410.11769v2.pdf","comment":"Accepted for publication by Empirical Software Engineering Journal\n  (EMSE) (in October 2024)"},{"id":"http://arxiv.org/abs/2410.12360v1","updated":"2024-10-16T08:23:39Z","published":"2024-10-16T08:23:39Z","title":"Towards Neural Scaling Laws for Time Series Foundation Models","summary":"  Scaling laws offer valuable insights into the design of time series\nfoundation models (TSFMs). However, previous research has largely focused on\nthe scaling laws of TSFMs for in-distribution (ID) data, leaving their\nout-of-distribution (OOD) scaling behavior and the influence of model\narchitectures less explored. In this work, we examine two common TSFM\narchitectures, encoder-only and decoder-only Transformers, and investigate\ntheir scaling behavior on both ID and OOD data. These models are trained and\nevaluated across varying parameter counts, compute budgets, and dataset sizes.\nOur experiments reveal that the log-likelihood loss of TSFMs exhibits similar\nscaling behavior in both OOD and ID settings. We further compare the scaling\nproperties across different architectures, incorporating two state-of-the-art\nTSFMs as case studies, showing that model architecture plays a significant role\nin scaling. The encoder-only Transformers demonstrate better scalability than\nthe decoder-only Transformers, while the architectural enhancements in the two\nadvanced TSFMs primarily improve ID performance but reduce OOD scalability.\nWhile scaling up TSFMs is expected to drive performance breakthroughs, the lack\nof a comprehensive understanding of TSFM scaling laws has hindered the\ndevelopment of a robust framework to guide model scaling. We fill this gap in\nthis work by synthesizing our findings and providing practical guidelines for\ndesigning and scaling larger TSFMs with enhanced model capabilities.\n","authors":["Qingren Yao","Chao-Han Huck Yang","Renhe Jiang","Yuxuan Liang","Ming Jin","Shirui Pan"],"pdf_url":"https://arxiv.org/pdf/2410.12360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09858v3","updated":"2024-10-16T08:23:11Z","published":"2024-09-15T20:41:18Z","title":"A Survey of Out-of-distribution Generalization for Graph Machine\n  Learning from a Causal View","summary":"  Graph machine learning (GML) has been successfully applied across a wide\nrange of tasks. Nonetheless, GML faces significant challenges in generalizing\nover out-of-distribution (OOD) data, which raises concerns about its wider\napplicability. Recent advancements have underscored the crucial role of\ncausality-driven approaches in overcoming these generalization challenges.\nDistinct from traditional GML methods that primarily rely on statistical\ndependencies, causality-focused strategies delve into the underlying causal\nmechanisms of data generation and model prediction, thus significantly\nimproving the generalization of GML across different environments. This paper\noffers a thorough review of recent progress in causality-involved GML\ngeneralization. We elucidate the fundamental concepts of employing causality to\nenhance graph model generalization and categorize the various approaches,\nproviding detailed descriptions of their methodologies and the connections\namong them. Furthermore, we explore the incorporation of causality in other\nrelated important areas of trustworthy GML, such as explanation, fairness, and\nrobustness. Concluding with a discussion on potential future research\ndirections, this review seeks to articulate the continuing development and\nfuture potential of causality in enhancing the trustworthiness of graph machine\nlearning.\n","authors":["Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2409.09858v3.pdf","comment":"15 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.10394v2","updated":"2024-10-16T08:20:44Z","published":"2024-10-14T11:30:18Z","title":"PIVOT-R: Primitive-Driven Waypoint-Aware World Model for Robotic\n  Manipulation","summary":"  Language-guided robotic manipulation is a challenging task that requires an\nembodied agent to follow abstract user instructions to accomplish various\ncomplex manipulation tasks. Previous work trivially fitting the data without\nrevealing the relation between instruction and low-level executable actions,\nthese models are prone to memorizing the surficial pattern of the data instead\nof acquiring the transferable knowledge, and thus are fragile to dynamic\nenvironment changes. To address this issue, we propose a PrIrmitive-driVen\nwaypOinT-aware world model for Robotic manipulation (PIVOT-R) that focuses\nsolely on the prediction of task-relevant waypoints. Specifically, PIVOT-R\nconsists of a Waypoint-aware World Model (WAWM) and a lightweight action\nprediction module. The former performs primitive action parsing and\nprimitive-driven waypoint prediction, while the latter focuses on decoding\nlow-level actions. Additionally, we also design an asynchronous hierarchical\nexecutor (AHE), which can use different execution frequencies for different\nmodules of the model, thereby helping the model reduce computational redundancy\nand improve model execution efficiency. Our PIVOT-R outperforms\nstate-of-the-art (SoTA) open-source models on the SeaWave benchmark, achieving\nan average relative improvement of 19.45% across four levels of instruction\ntasks. Moreover, compared to the synchronously executed PIVOT-R, the execution\nefficiency of PIVOT-R with AHE is increased by 28-fold, with only a 2.9% drop\nin performance. These results provide compelling evidence that our PIVOT-R can\nsignificantly improve both the performance and efficiency of robotic\nmanipulation.\n","authors":["Kaidong Zhang","Pengzhen Ren","Bingqian Lin","Junfan Lin","Shikui Ma","Hang Xu","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2410.10394v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.02701v2","updated":"2024-10-16T08:12:42Z","published":"2024-02-05T03:27:52Z","title":"Understanding What Affects the Generalization Gap in Visual\n  Reinforcement Learning: Theory and Empirical Evidence","summary":"  Recently, there are many efforts attempting to learn useful policies for\ncontinuous control in visual reinforcement learning (RL). In this scenario, it\nis important to learn a generalizable policy, as the testing environment may\ndiffer from the training environment, e.g., there exist distractors during\ndeployment. Many practical algorithms are proposed to handle this problem.\nHowever, to the best of our knowledge, none of them provide a theoretical\nunderstanding of what affects the generalization gap and why their proposed\nmethods work. In this paper, we bridge this issue by theoretically answering\nthe key factors that contribute to the generalization gap when the testing\nenvironment has distractors. Our theories indicate that minimizing the\nrepresentation distance between training and testing environments, which aligns\nwith human intuition, is the most critical for the benefit of reducing the\ngeneralization gap. Our theoretical results are supported by the empirical\nevidence in the DMControl Generalization Benchmark (DMC-GB).\n","authors":["Jiafei Lyu","Le Wan","Xiu Li","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2402.02701v2.pdf","comment":"Accepted by Journal of Artificial Intelligence Research (JAIR)"},{"id":"http://arxiv.org/abs/2410.12343v1","updated":"2024-10-16T08:04:57Z","published":"2024-10-16T08:04:57Z","title":"Federated Temporal Graph Clustering","summary":"  Temporal graph clustering is a complex task that involves discovering\nmeaningful structures in dynamic graphs where relationships and entities change\nover time. Existing methods typically require centralized data collection,\nwhich poses significant privacy and communication challenges. In this work, we\nintroduce a novel Federated Temporal Graph Clustering (FTGC) framework that\nenables decentralized training of graph neural networks (GNNs) across multiple\nclients, ensuring data privacy throughout the process. Our approach\nincorporates a temporal aggregation mechanism to effectively capture the\nevolution of graph structures over time and a federated optimization strategy\nto collaboratively learn high-quality clustering representations. By preserving\ndata privacy and reducing communication overhead, our framework achieves\ncompetitive performance on temporal graph datasets, making it a promising\nsolution for privacy-sensitive, real-world applications involving dynamic data.\n","authors":["Yang Liu","Zihao Zhou","Xianghong Xu","Qian Li"],"pdf_url":"https://arxiv.org/pdf/2410.12343v1.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.12330v1","updated":"2024-10-16T07:52:26Z","published":"2024-10-16T07:52:26Z","title":"MAX: Masked Autoencoder for X-ray Fluorescence in Geological\n  Investigation","summary":"  Pre-training foundation models has become the de-facto procedure for deep\nlearning approaches, yet its application remains limited in the geological\nstudies, where in needs of the model transferability to break the shackle of\ndata scarcity. Here we target on the X-ray fluorescence (XRF) scanning data, a\nstandard high-resolution measurement in extensive scientific drilling projects.\nWe propose a scalable self-supervised learner, masked autoencoders on XRF\nspectra (MAX), to pre-train a foundation model covering geological records from\nmultiple regions of the Pacific and Southern Ocean. In pre-training, we find\nthat masking a high proportion of the input spectrum (50\\%) yields a nontrivial\nand meaningful self-supervisory task. For downstream tasks, we select the\nquantification of XRF spectra into two costly geochemical measurements,\nCaCO$_3$ and total organic carbon, due to their importance in understanding the\npaleo-oceanic carbon system. Our results show that MAX, requiring only\none-third of the data, outperforms models without pre-training in terms of\nquantification accuracy. Additionally, the model's generalizability improves by\nmore than 60\\% in zero-shot tests on new materials, with explainability further\nensuring its robustness. Thus, our approach offers a promising pathway to\novercome data scarcity in geological discovery by leveraging the\nself-supervised foundation model and fast-acquired XRF scanning data.\n","authors":["An-Sheng Lee","Yu-Wen Pao","Hsuan-Tien Lin","Sofia Ya Hsuan Liou"],"pdf_url":"https://arxiv.org/pdf/2410.12330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12328v1","updated":"2024-10-16T07:48:53Z","published":"2024-10-16T07:48:53Z","title":"Improved Anomaly Detection through Conditional Latent Space VAE\n  Ensembles","summary":"  We propose a novel Conditional Latent space Variational Autoencoder (CL-VAE)\nto perform improved pre-processing for anomaly detection on data with known\ninlier classes and unknown outlier classes. This proposed variational\nautoencoder (VAE) improves latent space separation by conditioning on\ninformation within the data. The method fits a unique prior distribution to\neach class in the dataset, effectively expanding the classic prior distribution\nfor VAEs to include a Gaussian mixture model. An ensemble of these VAEs are\nmerged in the latent spaces to form a group consensus that greatly improves the\naccuracy of anomaly detection across data sets. Our approach is compared\nagainst the capabilities of a typical VAE, a CNN, and a PCA, with regards AUC\nfor anomaly detection. The proposed model shows increased accuracy in anomaly\ndetection, achieving an AUC of 97.4% on the MNIST dataset compared to 95.7% for\nthe second best model. In addition, the CL-VAE shows increased benefits from\nensembling, a more interpretable latent space, and an increased ability to\nlearn patterns in complex data with limited model sizes.\n","authors":["Oskar Åström","Alexandros Sopasakis"],"pdf_url":"https://arxiv.org/pdf/2410.12328v1.pdf","comment":"13 pages of main article, 19 pages including references and appendix,\n  4 figures"},{"id":"http://arxiv.org/abs/2410.12326v1","updated":"2024-10-16T07:47:31Z","published":"2024-10-16T07:47:31Z","title":"Revisited Large Language Model for Time Series Analysis through Modality\n  Alignment","summary":"  Large Language Models have demonstrated impressive performance in many\npivotal web applications such as sensor data analysis. However, since LLMs are\nnot designed for time series tasks, simpler models like linear regressions can\noften achieve comparable performance with far less complexity. In this study,\nwe perform extensive experiments to assess the effectiveness of applying LLMs\nto key time series tasks, including forecasting, classification, imputation,\nand anomaly detection. We compare the performance of LLMs against simpler\nbaseline models, such as single-layer linear models and randomly initialized\nLLMs. Our results reveal that LLMs offer minimal advantages for these core time\nseries tasks and may even distort the temporal structure of the data. In\ncontrast, simpler models consistently outperform LLMs while requiring far fewer\nparameters. Furthermore, we analyze existing reprogramming techniques and show,\nthrough data manifold analysis, that these methods fail to effectively align\ntime series data with language and display pseudo-alignment behaviour in\nembedding space. Our findings suggest that the performance of LLM-based methods\nin time series tasks arises from the intrinsic characteristics and structure of\ntime series data, rather than any meaningful alignment with the language model\narchitecture.\n","authors":["Liangwei Nathan Zheng","Chang George Dong","Wei Emma Zhang","Lin Yue","Miao Xu","Olaf Maennel","Weitong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.14522v3","updated":"2024-10-16T07:44:04Z","published":"2023-06-26T08:54:46Z","title":"Nonconvex Stochastic Bregman Proximal Gradient Method with Application\n  to Deep Learning","summary":"  Stochastic gradient methods for minimizing nonconvex composite objective\nfunctions typically rely on the Lipschitz smoothness of the differentiable\npart, but this assumption fails in many important problem classes like\nquadratic inverse problems and neural network training, leading to instability\nof the algorithms in both theory and practice. To address this, we propose a\nfamily of stochastic Bregman proximal gradient (SBPG) methods that only require\nsmooth adaptivity. SBPG replaces the quadratic approximation in SGD with a\nBregman proximity measure, offering a better approximation model that handles\nnon-Lipschitz gradients in nonconvex objectives. We establish the convergence\nproperties of vanilla SBPG and show it achieves optimal sample complexity in\nthe nonconvex setting. Experimental results on quadratic inverse problems\ndemonstrate SBPG's robustness in terms of stepsize selection and sensitivity to\nthe initial point. Furthermore, we introduce a momentum-based variant, MSBPG,\nwhich enhances convergence by relaxing the mini-batch size requirement while\npreserving the optimal oracle complexity. We apply MSBPG to the training of\ndeep neural networks, utilizing a polynomial kernel function to ensure smooth\nadaptivity of the loss function. Experimental results on benchmark datasets\nconfirm the effectiveness and robustness of MSBPG in training neural networks.\nGiven its negligible additional computational cost compared to SGD in\nlarge-scale optimization, MSBPG shows promise as a universal open-source\noptimizer for future applications.\n","authors":["Kuangyu Ding","Jingyang Li","Kim-Chuan Toh"],"pdf_url":"https://arxiv.org/pdf/2306.14522v3.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2405.18749v3","updated":"2024-10-16T07:35:31Z","published":"2024-05-29T04:22:18Z","title":"A SARS-CoV-2 Interaction Dataset and VHH Sequence Corpus for Antibody\n  Language Models","summary":"  Antibodies are crucial proteins produced by the immune system to eliminate\nharmful foreign substances and have become pivotal therapeutic agents for\ntreating human diseases. To accelerate the discovery of antibody therapeutics,\nthere is growing interest in constructing language models using antibody\nsequences. However, the applicability of pre-trained language models for\nantibody discovery has not been thoroughly evaluated due to the scarcity of\nlabeled datasets. To overcome these limitations, we introduce AVIDa-SARS-CoV-2,\na dataset featuring the antigen-variable domain of heavy chain of heavy chain\nantibody (VHH) interactions obtained from two alpacas immunized with severe\nacute respiratory syndrome coronavirus 2 (SARS-CoV-2) spike proteins.\nAVIDa-SARS-CoV-2 includes binary labels indicating the binding or non-binding\nof diverse VHH sequences to 12 SARS-CoV-2 mutants, such as the Delta and\nOmicron variants. Furthermore, we release VHHCorpus-2M, a pre-training dataset\nfor antibody language models, containing over two million VHH sequences. We\nreport benchmark results for predicting SARS-CoV-2-VHH binding using VHHBERT\npre-trained on VHHCorpus-2M and existing general protein and antibody-specific\npre-trained language models. These results confirm that AVIDa-SARS-CoV-2\nprovides valuable benchmarks for evaluating the representation capabilities of\nantibody language models for binding prediction, thereby facilitating the\ndevelopment of AI-driven antibody discovery. The datasets are available at\nhttps://datasets.cognanous.com.\n","authors":["Hirofumi Tsuruta","Hiroyuki Yamazaki","Ryota Maeda","Ryotaro Tamura","Akihiro Imura"],"pdf_url":"https://arxiv.org/pdf/2405.18749v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12316v1","updated":"2024-10-16T07:33:29Z","published":"2024-10-16T07:33:29Z","title":"TPFL: A Trustworthy Personalized Federated Learning Framework via\n  Subjective Logic","summary":"  Federated learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy. Despite its widespread\nadoption, most FL approaches focusing solely on privacy protection fall short\nin scenarios where trustworthiness is crucial, necessitating advancements in\nsecure training, dependable decision-making mechanisms, robustness on\ncorruptions, and enhanced performance with Non-IID data. To bridge this gap, we\nintroduce Trustworthy Personalized Federated Learning (TPFL) framework designed\nfor classification tasks via subjective logic in this paper. Specifically, TPFL\nadopts a unique approach by employing subjective logic to construct federated\nmodels, providing probabilistic decisions coupled with an assessment of\nuncertainty rather than mere probability assignments. By incorporating a\ntrainable heterogeneity prior to the local training phase, TPFL effectively\nmitigates the adverse effects of data heterogeneity. Model uncertainty and\ninstance uncertainty are further utilized to ensure the safety and reliability\nof the training and inference stages. Through extensive experiments on widely\nrecognized federated learning benchmarks, we demonstrate that TPFL not only\nachieves competitive performance compared with advanced methods but also\nexhibits resilience against prevalent malicious attacks, robustness on domain\nshifts, and reliability in high-stake scenarios.\n","authors":["Jinqian Chen","Jihua Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.12316v1.pdf","comment":"17 Pages with Appendix"},{"id":"http://arxiv.org/abs/2410.12307v1","updated":"2024-10-16T07:18:36Z","published":"2024-10-16T07:18:36Z","title":"DAT: Improving Adversarial Robustness via Generative Amplitude Mix-up in\n  Frequency Domain","summary":"  To protect deep neural networks (DNNs) from adversarial attacks, adversarial\ntraining (AT) is developed by incorporating adversarial examples (AEs) into\nmodel training. Recent studies show that adversarial attacks disproportionately\nimpact the patterns within the phase of the sample's frequency spectrum --\ntypically containing crucial semantic information -- more than those in the\namplitude, resulting in the model's erroneous categorization of AEs. We find\nthat, by mixing the amplitude of training samples' frequency spectrum with\nthose of distractor images for AT, the model can be guided to focus on phase\npatterns unaffected by adversarial perturbations. As a result, the model's\nrobustness can be improved. Unfortunately, it is still challenging to select\nappropriate distractor images, which should mix the amplitude without affecting\nthe phase patterns. To this end, in this paper, we propose an optimized\nAdversarial Amplitude Generator (AAG) to achieve a better tradeoff between\nimproving the model's robustness and retaining phase patterns. Based on this\ngenerator, together with an efficient AE production procedure, we design a new\nDual Adversarial Training (DAT) strategy. Experiments on various datasets show\nthat our proposed DAT leads to significantly improved robustness against\ndiverse adversarial attacks.\n","authors":["Fengpeng Li","Kemou Li","Haiwei Wu","Jinyu Tian","Jiantao Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.12307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00733v2","updated":"2024-10-16T07:14:57Z","published":"2024-09-01T14:49:48Z","title":"Benign Overfitting under Learning Rate Conditions for $α$\n  Sub-exponential Input","summary":"  This paper investigates the phenomenon of benign overfitting in binary\nclassification problems with heavy-tailed input distributions, extending the\nanalysis of maximum margin classifiers to $\\alpha$ sub-exponential\ndistributions ($\\alpha \\in (0, 2]$). This generalizes previous work focused on\nsub-gaussian inputs. We provide generalization error bounds for linear\nclassifiers trained using gradient descent on unregularized logistic loss in\nthis heavy-tailed setting. Our results show that, under certain conditions on\nthe dimensionality $p$ and the distance between the centers of the\ndistributions, the misclassification error of the maximum margin classifier\nasymptotically approaches the noise level, the theoretical optimal value.\nMoreover, we derive an upper bound on the learning rate $\\beta$ for benign\noverfitting to occur and show that as the tail heaviness of the input\ndistribution $\\alpha$ increases, the upper bound on the learning rate\ndecreases. These results demonstrate that benign overfitting persists even in\nsettings with heavier-tailed inputs than previously studied, contributing to a\ndeeper understanding of the phenomenon in more realistic data environments.\n","authors":["Kota Okudo","Kei Kobayashi"],"pdf_url":"https://arxiv.org/pdf/2409.00733v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13993v2","updated":"2024-10-16T07:14:20Z","published":"2024-06-20T04:44:20Z","title":"Exploring Changes in Nation Perception with Nationality-Assigned\n  Personas in LLMs","summary":"  Persona assignment has become a common strategy for customizing LLM use to\nparticular tasks and contexts. In this study, we explore how evaluation of\ndifferent nations change when LLMs are assigned specific nationality personas.\nWe assign 193 different nationality personas (e.g., an American person) to four\nLLMs and examine how the LLM evaluations (or ''perceptions'')of countries\nchange. We find that all LLM-persona combinations tend to favor Western\nEuropean nations, though nation-personas push LLM behaviors to focus more on\nand treat the nation-persona's own region more favorably. Eastern European,\nLatin American, and African nations are treated more negatively by different\nnationality personas. We additionally find that evaluations by nation-persona\nLLMs of other nations correlate with human survey responses but fail to match\nthe values closely. Our study provides insight into how biases and stereotypes\nare realized within LLMs when adopting different national personas. In line\nwith the ''Blueprint for an AI Bill of Rights'', our findings underscore the\ncritical need for developing mechanisms to ensure that LLM outputs promote\nfairness and avoid over-generalization.\n","authors":["Mahammed Kamruzzaman","Gene Louis Kim"],"pdf_url":"https://arxiv.org/pdf/2406.13993v2.pdf","comment":"Pre-print, Under review"},{"id":"http://arxiv.org/abs/2311.17137v3","updated":"2024-10-16T07:08:57Z","published":"2023-11-28T18:59:02Z","title":"Generative Models: What Do They Know? Do They Know Things? Let's Find\n  Out!","summary":"  Generative models excel at mimicking real scenes, suggesting they might\ninherently encode important intrinsic scene properties. In this paper, we aim\nto explore the following key questions: (1) What intrinsic knowledge do\ngenerative models like GANs, Autoregressive models, and Diffusion models\nencode? (2) Can we establish a general framework to recover intrinsic\nrepresentations from these models, regardless of their architecture or model\ntype? (3) How minimal can the required learnable parameters and labeled data be\nto successfully recover this knowledge? (4) Is there a direct link between the\nquality of a generative model and the accuracy of the recovered scene\nintrinsics?\n  Our findings indicate that a small Low-Rank Adaptators (LoRA) can recover\nintrinsic images-depth, normals, albedo and shading-across different generators\n(Autoregressive, GANs and Diffusion) while using the same decoder head that\ngenerates the image. As LoRA is lightweight, we introduce very few learnable\nparameters (as few as 0.04% of Stable Diffusion model weights for a rank of 2),\nand we find that as few as 250 labeled images are enough to generate intrinsic\nimages with these LoRA modules. Finally, we also show a positive correlation\nbetween the generative model's quality and the accuracy of the recovered\nintrinsics through control experiments.\n","authors":["Xiaodan Du","Nicholas Kolkin","Greg Shakhnarovich","Anand Bhattad"],"pdf_url":"https://arxiv.org/pdf/2311.17137v3.pdf","comment":"https://intrinsic-lora.github.io/"},{"id":"http://arxiv.org/abs/2410.05581v2","updated":"2024-10-16T07:07:20Z","published":"2024-10-08T00:37:16Z","title":"Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes\n  Fail to Improve?","summary":"  In the last decade, the generalization and adaptation abilities of deep\nlearning models were typically evaluated on fixed training and test\ndistributions. Contrary to traditional deep learning, large language models\n(LLMs) are (i) even more overparameterized, (ii) trained on unlabeled text\ncorpora curated from the Internet with minimal human intervention, and (iii)\ntrained in an online fashion. These stark contrasts prevent researchers from\ntransferring lessons learned on model generalization and adaptation in deep\nlearning contexts to LLMs. To this end, our short paper introduces empirical\nobservations that aim to shed light on further training of already pretrained\nlanguage models. Specifically, we demonstrate that training a model on a text\ndomain could degrade its perplexity on the test portion of the same domain. We\nobserve with our subsequent analysis that the performance degradation is\npositively correlated with the similarity between the additional and the\noriginal pretraining dataset of the LLM. Our further token-level perplexity\nobservations reveals that the perplexity degradation is due to a handful of\ntokens that are not informative about the domain. We hope these findings will\nguide us in determining when to adapt a model vs when to rely on its\nfoundational capabilities.\n","authors":["Fırat Öncel","Matthias Bethge","Beyza Ermis","Mirco Ravanelli","Cem Subakan","Çağatay Yıldız"],"pdf_url":"https://arxiv.org/pdf/2410.05581v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.12303v1","updated":"2024-10-16T07:05:06Z","published":"2024-10-16T07:05:06Z","title":"Continuous Pupillography: A Case for Visual Health Ecosystem","summary":"  This article aims to cover pupillography, and its potential use in a number\nof ophthalmological diagnostic applications in biomedical space. With the\never-increasing incorporation of technology within our daily lives and an\never-growing active research into smart devices and technologies, we try to\nmake a case for a health ecosystem that revolves around continuous eye\nmonitoring. We tend to summarize the design constraints & requirements for an\nIoT-based continuous pupil detection system, with an attempt at developing a\npipeline for wearable pupillographic device, while comparing two compact\nmini-camera modules currently available in the market. We use a light algorithm\nthat can be directly adopted to current micro-controllers, and share our\nresults for different lighting conditions, and scenarios. Lastly, we present\nour findings, along with an analysis on the challenges faced and a way ahead\ntowards successfully building this ecosystem.\n","authors":["Usama Younus","Nirupam Roy"],"pdf_url":"https://arxiv.org/pdf/2410.12303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12302v1","updated":"2024-10-16T07:02:51Z","published":"2024-10-16T07:02:51Z","title":"Two Birds with One Stone: Multi-Task Semantic Communications Systems\n  over Relay Channel","summary":"  In this paper, we propose a novel multi-task, multi-link relay semantic\ncommunications (MTML-RSC) scheme that enables the destination node to\nsimultaneously perform image reconstruction and classification with one\ntransmission from the source node. In the MTML-RSC scheme, the source node\nbroadcasts a signal using semantic communications, and the relay node forwards\nthe signal to the destination. We analyze the coupling relationship between the\ntwo tasks and the two links (source-to-relay and source-to-destination) and\ndesign a semantic-focused forward method for the relay node, where it\nselectively forwards only the semantics of the relevant class while ignoring\nothers. At the destination, the node combines signals from both the source node\nand the relay node to perform classification, and then uses the classification\nresult to assist in decoding the signal from the relay node for image\nreconstructing. Experimental results demonstrate that the proposed MTML-RSC\nscheme achieves significant performance gains, e.g., $1.73$ dB improvement in\npeak-signal-to-noise ratio (PSNR) for image reconstruction and increasing the\naccuracy from $64.89\\%$ to $70.31\\%$ for classification.\n","authors":["Yujie Cao","Tong Wu","Zhiyong Chen","Yin Xu","Meixia Tao","Wenjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12302v1.pdf","comment":"submitted to IEEE WCNC"},{"id":"http://arxiv.org/abs/2410.12297v1","updated":"2024-10-16T06:56:53Z","published":"2024-10-16T06:56:53Z","title":"Conjunction Subspaces Test for Conformal and Selective Classification","summary":"  In this paper, we present a new classifier, which integrates significance\ntesting results over different random subspaces to yield consensus p-values for\nquantifying the uncertainty of classification decision. The null hypothesis is\nthat the test sample has no association with the target class on a randomly\nchosen subspace, and hence the classification problem can be formulated as a\nproblem of testing for the conjunction of hypotheses. The proposed classifier\ncan be easily deployed for the purpose of conformal prediction and selective\nclassification with reject and refine options by simply thresholding the\nconsensus p-values. The theoretical analysis on the generalization error bound\nof the proposed classifier is provided and empirical studies on real data sets\nare conducted as well to demonstrate its effectiveness.\n","authors":["Zengyou He","Zerun Li","Junjie Dong","Xinying Liu","Mudi Jiang","Lianyu Hu"],"pdf_url":"https://arxiv.org/pdf/2410.12297v1.pdf","comment":"36 pages, 9 figures"},{"id":"http://arxiv.org/abs/2405.05219v2","updated":"2024-10-16T06:55:47Z","published":"2024-05-08T17:11:38Z","title":"Conv-Basis: A New Paradigm for Efficient Attention Inference and\n  Gradient Computation in Transformers","summary":"  The self-attention mechanism is the key to the success of transformers in\nrecent Large Language Models (LLMs). However, the quadratic computational cost\n$O(n^2)$ in the input sequence length $n$ is a notorious obstacle for further\nimprovement and scalability in longer contexts. In this work, we leverage the\nconvolution-like structure of attention matrices to develop an efficient\napproximation method for attention computation using convolution matrices. We\npropose a $\\mathsf{conv}$ basis system, analogous to the rank basis, and show\nthat any lower triangular matrix can always be decomposed as a sum of\nstructured convolution matrices in this basis. We then design a fast algorithm\nto approximate the attention matrix via a sum of such $k$ convolution matrices.\nThis allows us to compute the attention {\\it inference} via Fast Fourier\nTransforms (FFT) in $O(knd \\log n)$ time, where $d$ is the hidden dimension,\nand thus achieve almost linear time $n^{1+o(1)}$ in the practical scenario\nwhere $kd = n^{o(1)}$. Furthermore, the attention {\\it training forward} and\n{\\it backward gradient} can be computed in $n^{1+o(1)}$ as well. We provide\ntheoretical guarantees on the run time and approximation error and conduct\npreliminary experiments to evaluate its effectiveness. We hope our new paradigm\nfor accelerating attention computation in transformer models can help their\napplication to longer contexts.\n","authors":["Yingyu Liang","Heshan Liu","Zhenmei Shi","Zhao Song","Zhuoyan Xu","Junze Yin"],"pdf_url":"https://arxiv.org/pdf/2405.05219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12295v1","updated":"2024-10-16T06:55:02Z","published":"2024-10-16T06:55:02Z","title":"Consistency Calibration: Improving Uncertainty Calibration via\n  Consistency among Perturbed Neighbors","summary":"  Calibration is crucial in deep learning applications, especially in fields\nlike healthcare and autonomous driving, where accurate confidence estimates are\nvital for decision-making. However, deep neural networks often suffer from\nmiscalibration, with reliability diagrams and Expected Calibration Error (ECE)\nbeing the only standard perspective for evaluating calibration performance. In\nthis paper, we introduce the concept of consistency as an alternative\nperspective on model calibration, inspired by uncertainty estimation literature\nin large language models (LLMs). We highlight its advantages over the\ntraditional reliability-based view. Building on this concept, we propose a\npost-hoc calibration method called Consistency Calibration (CC), which adjusts\nconfidence based on the model's consistency across perturbed inputs. CC is\nparticularly effective in locally uncertainty estimation, as it requires no\nadditional data samples or label information, instead generating input\nperturbations directly from the source data. Moreover, we show that performing\nperturbations at the logit level significantly improves computational\nefficiency. We validate the effectiveness of CC through extensive comparisons\nwith various post-hoc and training-time calibration methods, demonstrating\nstate-of-the-art performance on standard datasets such as CIFAR-10, CIFAR-100,\nand ImageNet, as well as on long-tailed datasets like ImageNet-LT.\n","authors":["Linwei Tao","Haolan Guo","Minjing Dong","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2410.12295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03920v4","updated":"2024-10-16T06:51:39Z","published":"2024-06-06T10:02:49Z","title":"Towards Physically Consistent Deep Learning For Climate Model\n  Parameterizations","summary":"  Climate models play a critical role in understanding and projecting climate\nchange. Due to their complexity, their horizontal resolution of about 40-100 km\nremains too coarse to resolve processes such as clouds and convection, which\nneed to be approximated via parameterizations. These parameterizations are a\nmajor source of systematic errors and large uncertainties in climate\nprojections. Deep learning (DL)-based parameterizations, trained on data from\ncomputationally expensive short, high-resolution simulations, have shown great\npromise for improving climate models in that regard. However, their lack of\ninterpretability and tendency to learn spurious non-physical correlations\nresult in reduced trust in the climate simulation. We propose an efficient\nsupervised learning framework for DL-based parameterizations that leads to\nphysically consistent models with improved interpretability and negligible\ncomputational overhead compared to standard supervised training. First, key\nfeatures determining the target physical processes are uncovered. Subsequently,\nthe neural network is fine-tuned using only those relevant features. We show\nempirically that our method robustly identifies a small subset of the inputs as\nactual physical drivers, therefore removing spurious non-physical\nrelationships. This results in by design physically consistent and\ninterpretable neural networks while maintaining the predictive performance of\nunconstrained black-box DL-based parameterizations.\n","authors":["Birgit Kühbacher","Fernando Iglesias-Suarez","Niki Kilbertus","Veronika Eyring"],"pdf_url":"https://arxiv.org/pdf/2406.03920v4.pdf","comment":"Accepted at ICMLA 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.12700v1","updated":"2024-10-16T16:03:42Z","published":"2024-10-16T16:03:42Z","title":"Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via\n  Lightweight Value Optimization","summary":"  Recent advancements in diffusion models trained on large-scale data have\nenabled the generation of indistinguishable human-level images, yet they often\nproduce harmful content misaligned with human values, e.g., social bias, and\noffensive content. Despite extensive research on Large Language Models (LLMs),\nthe challenge of Text-to-Image (T2I) model alignment remains largely\nunexplored. Addressing this problem, we propose LiVO (Lightweight Value\nOptimization), a novel lightweight method for aligning T2I models with human\nvalues. LiVO only optimizes a plug-and-play value encoder to integrate a\nspecified value principle with the input prompt, allowing the control of\ngenerated images over both semantics and values. Specifically, we design a\ndiffusion model-tailored preference optimization loss, which theoretically\napproximates the Bradley-Terry model used in LLM alignment but provides a more\nflexible trade-off between image quality and value conformity. To optimize the\nvalue encoder, we also develop a framework to automatically construct a\ntext-image preference dataset of 86k (prompt, aligned image, violating image,\nvalue principle) samples. Without updating most model parameters and through\nadaptive value selection from the input prompt, LiVO significantly reduces\nharmful outputs and achieves faster convergence, surpassing several strong\nbaselines and taking an initial step towards ethically aligned T2I models.\n","authors":["Xingqi Wang","Xiaoyuan Yi","Xing Xie","Jia Jia"],"pdf_url":"https://arxiv.org/pdf/2410.12700v1.pdf","comment":"Accepted by ACM Multimedia 2024. The dataset and code can be found at\n  https://github.com/achernarwang/LiVO"},{"id":"http://arxiv.org/abs/2410.12407v1","updated":"2024-10-16T09:42:29Z","published":"2024-10-16T09:42:29Z","title":"Beyond Coarse-Grained Matching in Video-Text Retrieval","summary":"  Video-text retrieval has seen significant advancements, yet the ability of\nmodels to discern subtle differences in captions still requires verification.\nIn this paper, we introduce a new approach for fine-grained evaluation. Our\napproach can be applied to existing datasets by automatically generating hard\nnegative test captions with subtle single-word variations across nouns, verbs,\nadjectives, adverbs, and prepositions. We perform comprehensive experiments\nusing four state-of-the-art models across two standard benchmarks (MSR-VTT and\nVATEX) and two specially curated datasets enriched with detailed descriptions\n(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our\nanalyses show that the current evaluation benchmarks fall short in detecting a\nmodel's ability to perceive subtle single-word differences, 2) our fine-grained\nevaluation highlights the difficulty models face in distinguishing such subtle\nvariations. To enhance fine-grained understanding, we propose a new baseline\nthat can be easily combined with current methods. Experiments on our\nfine-grained evaluations demonstrate that this approach enhances a model's\nability to understand fine-grained differences.\n","authors":["Aozhu Chen","Hazel Doughty","Xirong Li","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12407v1.pdf","comment":"Accepted to ACCV 2024"},{"id":"http://arxiv.org/abs/2410.12220v1","updated":"2024-10-16T04:31:25Z","published":"2024-10-16T04:31:25Z","title":"Rethinking Bjøntegaard Delta for Compression Efficiency Evaluation:\n  Are We Calculating It Precisely and Reliably?","summary":"  For decades, the Bj{\\o}ntegaard Delta (BD) has been the metric for evaluating\ncodec Rate-Distortion (R-D) performance. Yet, in most studies, BD is determined\nusing just 4-5 R-D data points, could this be sufficient? As codecs and quality\nmetrics advance, does the conventional BD estimation still hold up? Crucially,\nare the performance improvements of new codecs and tools genuine, or merely\nartifacts of estimation flaws? This paper addresses these concerns by\nreevaluating BD estimation. We present a novel approach employing a\nparameterized deep neural network to model R-D curves with high precision\nacross various metrics, accompanied by a comprehensive R-D dataset. This\napproach both assesses the reliability of BD calculations and serves as a\nprecise BD estimator. Our findings advocate for the adoption of rigorous R-D\nsampling and reliability metrics in future compression research to ensure the\nvalidity and reliability of results.\n","authors":["Xinyu Hang","Shenpeng Song","Zhimeng Huang","Chuanmin Jia","Siwei Ma","Wen Gao"],"pdf_url":"https://arxiv.org/pdf/2410.12220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12219v1","updated":"2024-10-16T04:29:46Z","published":"2024-10-16T04:29:46Z","title":"OmnixR: Evaluating Omni-modality Language Models on Reasoning across\n  Modalities","summary":"  We introduce OmnixR, an evaluation suite designed to benchmark SoTA\nOmni-modality Language Models, such as GPT-4o and Gemini. Evaluating OLMs,\nwhich integrate multiple modalities such as text, vision, and audio, presents\nunique challenges. Particularly, the user message might often consist of\nmultiple modalities, such that OLMs have to establish holistic understanding\nand reasoning across modalities to accomplish the task. Existing benchmarks are\nlimited to single modality or dual-modality tasks, overlooking comprehensive\nmulti-modal assessments of model reasoning. To address this, OmnixR offers two\nevaluation variants: (1)synthetic subset: a synthetic dataset generated\nautomatically by translating text into multiple modalities--audio, images,\nvideo, and hybrids (Omnify). (2)realistic subset: a real-world dataset,\nmanually curated and annotated by experts, for evaluating cross-modal reasoning\nin natural settings. OmnixR presents a unique evaluation towards assessing OLMs\nover a diverse mix of modalities, such as a question that involves video,\naudio, and text, providing a rigorous cross-modal reasoning testbed unlike any\nexisting benchmarks. Our experiments find that all state-of-the-art OLMs\nstruggle with OmnixR questions that require integrating information from\nmultiple modalities to answer. Further analysis highlights differences in\nreasoning behavior, underscoring the challenges of omni-modal AI alignment.\n","authors":["Lichang Chen","Hexiang Hu","Mingda Zhang","Yiwen Chen","Zifeng Wang","Yandong Li","Pranav Shyam","Tianyi Zhou","Heng Huang","Ming-Hsuan Yang","Boqing Gong"],"pdf_url":"https://arxiv.org/pdf/2410.12219v1.pdf","comment":"19 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2407.07464v2","updated":"2024-10-16T03:44:41Z","published":"2024-07-10T08:40:39Z","title":"Video-to-Audio Generation with Hidden Alignment","summary":"  Generating semantically and temporally aligned audio content in accordance\nwith video input has become a focal point for researchers, particularly\nfollowing the remarkable breakthrough in text-to-video generation. In this\nwork, we aim to offer insights into the video-to-audio generation paradigm,\nfocusing on three crucial aspects: vision encoders, auxiliary embeddings, and\ndata augmentation techniques. Beginning with a foundational model built on a\nsimple yet surprisingly effective intuition, we explore various vision encoders\nand auxiliary embeddings through ablation studies. Employing a comprehensive\nevaluation pipeline that emphasizes generation quality and video-audio\nsynchronization alignment, we demonstrate that our model exhibits\nstate-of-the-art video-to-audio generation capabilities. Furthermore, we\nprovide critical insights into the impact of different data augmentation\nmethods on enhancing the generation framework's overall capacity. We showcase\npossibilities to advance the challenge of generating synchronized audio from\nsemantic and temporal perspectives. We hope these insights will serve as a\nstepping stone toward developing more realistic and accurate audio-visual\ngeneration models.\n","authors":["Manjie Xu","Chenxing Li","Xinyi Tu","Yong Ren","Rilin Chen","Yu Gu","Wei Liang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2407.07464v2.pdf","comment":"https://sites.google.com/view/vta-ldm"},{"id":"http://arxiv.org/abs/2410.12191v1","updated":"2024-10-16T03:25:16Z","published":"2024-10-16T03:25:16Z","title":"Test-time adaptation for image compression with distribution\n  regularization","summary":"  Current test- or compression-time adaptation image compression (TTA-IC)\napproaches, which leverage both latent and decoder refinements as a two-step\nadaptation scheme, have potentially enhanced the rate-distortion (R-D)\nperformance of learned image compression models on cross-domain compression\ntasks, \\textit{e.g.,} from natural to screen content images. However, compared\nwith the emergence of various decoder refinement variants, the latent\nrefinement, as an inseparable ingredient, is barely tailored to cross-domain\nscenarios. To this end, we aim to develop an advanced latent refinement method\nby extending the effective hybrid latent refinement (HLR) method, which is\ndesigned for \\textit{in-domain} inference improvement but shows noticeable\ndegradation of the rate cost in \\textit{cross-domain} tasks. Specifically, we\nfirst provide theoretical analyses, in a cue of marginalization approximation\nfrom in- to cross-domain scenarios, to uncover that the vanilla HLR suffers\nfrom an underlying mismatch between refined Gaussian conditional and hyperprior\ndistributions, leading to deteriorated joint probability approximation of\nmarginal distribution with increased rate consumption. To remedy this issue, we\nintroduce a simple Bayesian approximation-endowed \\textit{distribution\nregularization} to encourage learning a better joint probability approximation\nin a plug-and-play manner. Extensive experiments on six in- and cross-domain\ndatasets demonstrate that our proposed method not only improves the R-D\nperformance compared with other latent refinement counterparts, but also can be\nflexibly integrated into existing TTA-IC methods with incremental benefits.\n","authors":["Kecheng Chen","Pingping Zhang","Tiexin Qin","Shiqi Wang","Hong Yan","Haoliang Li"],"pdf_url":"https://arxiv.org/pdf/2410.12191v1.pdf","comment":null}]},"2024-10-15T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.12109v1","updated":"2024-10-15T23:16:28Z","published":"2024-10-15T23:16:28Z","title":"OMCAT: Omni Context Aware Transformer","summary":"  Large Language Models (LLMs) have made significant strides in text generation\nand comprehension, with recent advancements extending into multimodal LLMs that\nintegrate visual and audio inputs. However, these models continue to struggle\nwith fine-grained, cross-modal temporal understanding, particularly when\ncorrelating events across audio and video streams. We address these challenges\nwith two key contributions: a new dataset and model, called OCTAV and OMCAT\nrespectively. OCTAV (Omni Context and Temporal Audio Video) is a novel dataset\ndesigned to capture event transitions across audio and video. Second, OMCAT\n(Omni Context Aware Transformer) is a powerful model that leverages RoTE\n(Rotary Time Embeddings), an innovative extension of RoPE, to enhance temporal\ngrounding and computational efficiency in time-anchored tasks. Through a robust\nthree-stage training pipeline-feature alignment, instruction tuning, and\nOCTAV-specific training-OMCAT excels in cross-modal temporal understanding. Our\nmodel demonstrates state-of-the-art performance on Audio-Visual Question\nAnswering (AVQA) tasks and the OCTAV benchmark, showcasing significant gains in\ntemporal reasoning and cross-modal alignment, as validated through\ncomprehensive experiments and ablation studies. Our dataset and code will be\nmade publicly available. The link to our demo page is https://om-cat.github.io.\n","authors":["Arushi Goel","Karan Sapra","Matthieu Le","Rafael Valle","Andrew Tao","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2410.12109v1.pdf","comment":"Demo page: https://om-cat.github.io"},{"id":"http://arxiv.org/abs/2310.12976v2","updated":"2024-10-15T22:26:58Z","published":"2023-10-19T17:59:37Z","title":"Exploring Invariance in Images through One-way Wave Equations","summary":"  In this paper, we empirically reveal an invariance over images-images share a\nset of one-way wave equations with latent speeds. Each image is uniquely\nassociated with a solution to these wave equations, allowing for its\nreconstruction with high fidelity from an initial condition. We demonstrate it\nusing an intuitive encoder-decoder framework where each image is encoded into\nits corresponding initial condition (a single vector). Subsequently, the\ninitial condition undergoes a specialized decoder, transforming the one-way\nwave equations into a first-order norm + linear autoregressive process. This\nprocess propagates the initial condition along the x and y directions,\ngenerating a high-resolution feature map (up to the image resolution), followed\nby a few convolutional layers to reconstruct image pixels. The revealed\ninvariance, rooted in the shared wave equations, offers a fresh perspective for\ncomprehending images, establishing a promising avenue for further exploration.\n","authors":["Yinpeng Chen","Dongdong Chen","Xiyang Dai","Mengchen Liu","Yinan Feng","Youzuo Lin","Lu Yuan","Zicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2310.12976v2.pdf","comment":"This is an improvement version, which fuses some parts from the\n  preliminary work arXiv:2305.16319"},{"id":"http://arxiv.org/abs/2410.12080v1","updated":"2024-10-15T21:52:24Z","published":"2024-10-15T21:52:24Z","title":"SplatPose+: Real-time Image-Based Pose-Agnostic 3D Anomaly Detection","summary":"  Image-based Pose-Agnostic 3D Anomaly Detection is an important task that has\nemerged in industrial quality control. This task seeks to find anomalies from\nquery images of a tested object given a set of reference images of an\nanomaly-free object. The challenge is that the query views (a.k.a poses) are\nunknown and can be different from the reference views. Currently, new methods\nsuch as OmniposeAD and SplatPose have emerged to bridge the gap by synthesizing\npseudo reference images at the query views for pixel-to-pixel comparison.\nHowever, none of these methods can infer in real-time, which is critical in\nindustrial quality control for massive production. For this reason, we propose\nSplatPose+, which employs a hybrid representation consisting of a Structure\nfrom Motion (SfM) model for localization and a 3D Gaussian Splatting (3DGS)\nmodel for Novel View Synthesis. Although our proposed pipeline requires the\ncomputation of an additional SfM model, it offers real-time inference speeds\nand faster training compared to SplatPose. Quality-wise, we achieved a new SOTA\non the Pose-agnostic Anomaly Detection benchmark with the Multi-Pose Anomaly\nDetection (MAD-SIM) dataset.\n","authors":["Yizhe Liu","Yan Song Hu","Yuhao Chen","John Zelek"],"pdf_url":"https://arxiv.org/pdf/2410.12080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12075v1","updated":"2024-10-15T21:29:26Z","published":"2024-10-15T21:29:26Z","title":"WeatherDG: LLM-assisted Procedural Weather Generation for\n  Domain-Generalized Semantic Segmentation","summary":"  In this work, we propose a novel approach, namely WeatherDG, that can\ngenerate realistic, weather-diverse, and driving-screen images based on the\ncooperation of two foundation models, i.e, Stable Diffusion (SD) and Large\nLanguage Model (LLM). Specifically, we first fine-tune the SD with source data,\naligning the content and layout of generated samples with real-world driving\nscenarios. Then, we propose a procedural prompt generation method based on LLM,\nwhich can enrich scenario descriptions and help SD automatically generate more\ndiverse, detailed images. In addition, we introduce a balanced generation\nstrategy, which encourages the SD to generate high-quality objects of tailed\nclasses under various weather conditions, such as riders and motorcycles. This\nsegmentation-model-agnostic method can improve the generalization ability of\nexisting models by additionally adapting them with the generated synthetic\ndata. Experiments on three challenging datasets show that our method can\nsignificantly improve the segmentation performance of different\nstate-of-the-art models on target domains. Notably, in the setting of\n''Cityscapes to ACDC'', our method improves the baseline HRDA by 13.9% in mIoU.\n","authors":["Chenghao Qian","Yuhu Guo","Yuhong Mo","Wenjing Li"],"pdf_url":"https://arxiv.org/pdf/2410.12075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12074v1","updated":"2024-10-15T21:24:31Z","published":"2024-10-15T21:24:31Z","title":"nvTorchCam: An Open-source Library for Camera-Agnostic Differentiable\n  Geometric Vision","summary":"  We introduce nvTorchCam, an open-source library under the Apache 2.0 license,\ndesigned to make deep learning algorithms camera model-independent. nvTorchCam\nabstracts critical camera operations such as projection and unprojection,\nallowing developers to implement algorithms once and apply them across diverse\ncamera models--including pinhole, fisheye, and 360 equirectangular panoramas,\nwhich are commonly used in automotive and real estate capture applications.\nBuilt on PyTorch, nvTorchCam is fully differentiable and supports GPU\nacceleration and batching for efficient computation. Furthermore, deep learning\nmodels trained for one camera type can be directly transferred to other camera\ntypes without requiring additional modification. In this paper, we provide an\noverview of nvTorchCam, its functionality, and present various code examples\nand diagrams to demonstrate its usage. Source code and installation\ninstructions can be found on the nvTorchCam GitHub page at\nhttps://github.com/NVlabs/nvTorchCam.\n","authors":["Daniel Lichy","Hang Su","Abhishek Badki","Jan Kautz","Orazio Gallo"],"pdf_url":"https://arxiv.org/pdf/2410.12074v1.pdf","comment":"Source code and installation instructions are available at\n  https://github.com/NVlabs/nvTorchCam"},{"id":"http://arxiv.org/abs/2410.12068v1","updated":"2024-10-15T21:08:08Z","published":"2024-10-15T21:08:08Z","title":"V3D-SLAM: Robust RGB-D SLAM in Dynamic Environments with 3D Semantic\n  Geometry Voting","summary":"  Simultaneous localization and mapping (SLAM) in highly dynamic environments\nis challenging due to the correlation complexity between moving objects and the\ncamera pose. Many methods have been proposed to deal with this problem;\nhowever, the moving properties of dynamic objects with a moving camera remain\nunclear. Therefore, to improve SLAM's performance, minimizing disruptive events\nof moving objects with a physical understanding of 3D shapes and dynamics of\nobjects is needed. In this paper, we propose a robust method, V3D-SLAM, to\nremove moving objects via two lightweight re-evaluation stages, including\nidentifying potentially moving and static objects using a spatial-reasoned\nHough voting mechanism and refining static objects by detecting dynamic noise\ncaused by intra-object motions using Chamfer distances as similarity\nmeasurements. Our experiment on the TUM RGB-D benchmark on dynamic sequences\nwith ground-truth camera trajectories showed that our methods outperform the\nmost recent state-of-the-art SLAM methods. Our source code is available at\nhttps://github.com/tuantdang/v3d-slam.\n","authors":["Tuan Dang","Khang Nguyen","Mandfred Huber"],"pdf_url":"https://arxiv.org/pdf/2410.12068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12053v1","updated":"2024-10-15T20:47:48Z","published":"2024-10-15T20:47:48Z","title":"SOE: SO(3)-Equivariant 3D MRI Encoding","summary":"  Representation learning has become increasingly important, especially as\npowerful models have shifted towards learning latent representations before\nfine-tuning for downstream tasks. This approach is particularly valuable in\nleveraging the structural information within brain anatomy. However, a common\nlimitation of recent models developed for MRIs is their tendency to ignore or\nremove geometric information, such as translation and rotation, thereby\ncreating invariance with respect to geometric operations. We contend that\nincorporating knowledge about these geometric transformations into the model\ncan significantly enhance its ability to learn more detailed anatomical\ninformation within brain structures. As a result, we propose a novel method for\nencoding 3D MRIs that enforces equivariance with respect to all rotations in 3D\nspace, in other words, SO(3)-equivariance (SOE). By explicitly modeling this\ngeometric equivariance in the representation space, we ensure that any\nrotational operation applied to the input image space is also reflected in the\nembedding representation space. This approach requires moving beyond\ntraditional representation learning methods, as we need a representation vector\nspace that allows for the application of the same SO(3) operation in that\nspace. To facilitate this, we leverage the concept of vector neurons. The\nrepresentation space formed by our method captures the brain's structural and\nanatomical information more effectively. We evaluate SOE pretrained on the\nstructural MRIs of two public data sets with respect to the downstream task of\npredicting age and diagnosing Alzheimer's Disease from T1-weighted brain scans\nof the ADNI data set. We demonstrate that our approach not only outperforms\nother methods but is also robust against various degrees of rotation along\ndifferent axes. The code is available at\nhttps://github.com/shizhehe/SOE-representation-learning.\n","authors":["Shizhe He","Magdalini Paschali","Jiahong Ouyang","Adnan Masood","Akshay Chaudhari","Ehsan Adeli"],"pdf_url":"https://arxiv.org/pdf/2410.12053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00252v4","updated":"2024-10-15T20:11:42Z","published":"2024-06-01T01:17:25Z","title":"Towards Rationality in Language and Multimodal Agents: A Survey","summary":"  Rationality is the quality of being guided by reason, characterized by\ndecision-making that aligns with evidence and logical principles. It plays a\ncrucial role in reliable problem-solving by ensuring well-grounded and\nconsistent solutions. While large language models (LLMs) have made significant\nprogress in generating human-like text, they still exhibit limitations such as\nbounded knowledge space and inconsistent outputs. In response, recent efforts\nhave shifted toward developing multimodal and multi-agent systems, as well as\nintegrating modules like external tools, programming codes, symbolic reasoners,\nutility function, and conformal risk controls rather than relying solely on a\nsingle LLM for decision-making. This paper surveys the state-of-the-art\nadvancements in language and multimodal agents, evaluates how they contribute\nto make intelligent agents more rational, and identifies open challenges and\nfuture research directions. We maintain an open repository at\nhttps://github.com/bowen-upenn/Agent_Rationality.\n","authors":["Bowen Jiang","Yangxinyu Xie","Xiaomeng Wang","Yuan Yuan","Zhuoqun Hao","Xinyi Bai","Weijie J. Su","Camillo J. Taylor","Tanwi Mallick"],"pdf_url":"https://arxiv.org/pdf/2406.00252v4.pdf","comment":"We maintain an open repository at\n  https://github.com/bowen-upenn/Agent_Rationality"},{"id":"http://arxiv.org/abs/2407.05357v3","updated":"2024-10-15T20:11:07Z","published":"2024-07-07T13:19:51Z","title":"On the power of data augmentation for head pose estimation","summary":"  Deep learning has been impressively successful in the last decade in\npredicting human head poses from monocular images. However, for in-the-wild\ninputs the research community relies predominantly on a single training set,\n300W-LP, of semisynthetic nature without many alternatives. This paper focuses\non gradual extension and improvement of the data to explore the performance\nachievable with augmentation and synthesis strategies further. Modeling-wise a\nnovel multitask head/loss design which includes uncertainty estimation is\nproposed. Overall, the thus obtained models are small, efficient, suitable for\nfull 6 DoF pose estimation, and exhibit very competitive accuracy.\n","authors":["Michael Welter"],"pdf_url":"https://arxiv.org/pdf/2407.05357v3.pdf","comment":"CVPR version. Added evaluation on BIWI. Plenty of writing changes"},{"id":"http://arxiv.org/abs/2404.04452v2","updated":"2024-10-15T19:49:31Z","published":"2024-04-05T23:38:57Z","title":"Vision transformers in domain adaptation and domain generalization: a\n  study of robustness","summary":"  Deep learning models are often evaluated in scenarios where the data\ndistribution is different from those used in the training and validation\nphases. The discrepancy presents a challenge for accurately predicting the\nperformance of models once deployed on the target distribution. Domain\nadaptation and generalization are widely recognized as effective strategies for\naddressing such shifts, thereby ensuring reliable performance. The recent\npromising results in applying vision transformers in computer vision tasks,\ncoupled with advancements in self-attention mechanisms, have demonstrated their\nsignificant potential for robustness and generalization in handling\ndistribution shifts. Motivated by the increased interest from the research\ncommunity, our paper investigates the deployment of vision transformers in\ndomain adaptation and domain generalization scenarios. For domain adaptation\nmethods, we categorize research into feature-level, instance-level, model-level\nadaptations, and hybrid approaches, along with other categorizations with\nrespect to diverse strategies for enhancing domain adaptation. Similarly, for\ndomain generalization, we categorize research into multi-domain learning,\nmeta-learning, regularization techniques, and data augmentation strategies. We\nfurther classify diverse strategies in research, underscoring the various\napproaches researchers have taken to address distribution shifts by integrating\nvision transformers. The inclusion of comprehensive tables summarizing these\ncategories is a distinct feature of our work, offering valuable insights for\nresearchers. These findings highlight the versatility of vision transformers in\nmanaging distribution shifts, crucial for real-world applications, especially\nin critical safety and decision-making scenarios.\n","authors":["Shadi Alijani","Jamil Fayyad","Homayoun Najjaran"],"pdf_url":"https://arxiv.org/pdf/2404.04452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12023v1","updated":"2024-10-15T19:42:45Z","published":"2024-10-15T19:42:45Z","title":"Learned Neural Physics Simulation for Articulated 3D Human Pose\n  Reconstruction","summary":"  We propose a novel neural network approach, LARP (Learned Articulated Rigid\nbody Physics), to model the dynamics of articulated human motion with contact.\nOur goal is to develop a faster and more convenient methodological alternative\nto traditional physics simulators for use in computer vision tasks such as\nhuman motion reconstruction from video. To that end we introduce a training\nprocedure and model components that support the construction of a recurrent\nneural architecture to accurately simulate articulated rigid body dynamics. Our\nneural architecture supports features typically found in traditional physics\nsimulators, such as modeling of joint motors, variable dimensions of body\nparts, contact between body parts and objects, and is an order of magnitude\nfaster than traditional systems when multiple simulations are run in parallel.\nTo demonstrate the value of LARP we use it as a drop-in replacement for a state\nof the art classical non-differentiable simulator in an existing video-based\nreconstruction framework and show comparative or better 3D human pose\nreconstruction accuracy.\n","authors":["Mykhaylo Andriluka","Baruch Tabanpour","C. Daniel Freeman","Cristian Sminchisescu"],"pdf_url":"https://arxiv.org/pdf/2410.12023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12018v1","updated":"2024-10-15T19:33:57Z","published":"2024-10-15T19:33:57Z","title":"LocoMotion: Learning Motion-Focused Video-Language Representations","summary":"  This paper strives for motion-focused video-language representations.\nExisting methods to learn video-language representations use spatial-focused\ndata, where identifying the objects and scene is often enough to distinguish\nthe relevant caption. We instead propose LocoMotion to learn from\nmotion-focused captions that describe the movement and temporal progression of\nlocal object motions. We achieve this by adding synthetic motions to videos and\nusing the parameters of these motions to generate corresponding captions.\nFurthermore, we propose verb-variation paraphrasing to increase the caption\nvariety and learn the link between primitive motions and high-level verbs. With\nthis, we are able to learn a motion-focused video-language representation.\nExperiments demonstrate our approach is effective for a variety of downstream\ntasks, particularly when limited data is available for fine-tuning. Code is\navailable: https://hazeldoughty.github.io/Papers/LocoMotion/\n","authors":["Hazel Doughty","Fida Mohammad Thoker","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12018v1.pdf","comment":"ACCV 2024"},{"id":"http://arxiv.org/abs/2410.12006v1","updated":"2024-10-15T19:13:05Z","published":"2024-10-15T19:13:05Z","title":"Beyond Labels: A Self-Supervised Framework with Masked Autoencoders and\n  Random Cropping for Breast Cancer Subtype Classification","summary":"  This work contributes to breast cancer sub-type classification using\nhistopathological images. We utilize masked autoencoders (MAEs) to learn a\nself-supervised embedding tailored for computer vision tasks in this domain.\nThis embedding captures informative representations of histopathological data,\nfacilitating feature learning without extensive labeled datasets. During\npre-training, we investigate employing a random crop technique to generate a\nlarge dataset from WSIs automatically. Additionally, we assess the performance\nof linear probes for multi-class classification tasks of cancer sub-types using\nthe representations learnt by the MAE. Our approach aims to achieve strong\nperformance on downstream tasks by leveraging the complementary strengths of\nViTs and autoencoders. We evaluate our model's performance on the BRACS dataset\nand compare it with existing benchmarks.\n","authors":["Annalisa Chiocchetti","Marco Dossena","Christopher Irwin","Luigi Portinale"],"pdf_url":"https://arxiv.org/pdf/2410.12006v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.12123v1","updated":"2024-10-15T23:51:04Z","published":"2024-10-15T23:51:04Z","title":"The Moral Case for Using Language Model Agents for Recommendation","summary":"  Our information and communication environment has fallen short of the ideals\nthat networked global communication might have served. Identifying all the\ncauses of its pathologies is difficult, but existing recommender systems very\nlikely play a contributing role. In this paper, which draws on the normative\ntools of philosophy of computing, informed by empirical and technical insights\nfrom natural language processing and recommender systems, we make the moral\ncase for an alternative approach. We argue that existing recommenders\nincentivise mass surveillance, concentrate power, fall prey to narrow\nbehaviourism, and compromise user agency. Rather than just trying to avoid\nalgorithms entirely, or to make incremental improvements to the current\nparadigm, researchers and engineers should explore an alternative paradigm: the\nuse of language model (LM) agents to source and curate content that matches\nusers' preferences and values, expressed in natural language. The use of LM\nagents for recommendation poses its own challenges, including those related to\ncandidate generation, computational efficiency, preference modelling, and\nprompt injection. Nonetheless, if implemented successfully LM agents could:\nguide us through the digital public sphere without relying on mass\nsurveillance; shift power away from platforms towards users; optimise for what\nmatters instead of just for behavioural proxies; and scaffold our agency\ninstead of undermining it.\n","authors":["Seth Lazar","Luke Thorburn","Tian Jin","Luca Belli"],"pdf_url":"https://arxiv.org/pdf/2410.12123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07722v2","updated":"2024-10-15T19:58:37Z","published":"2024-10-10T08:41:34Z","title":"DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities","summary":"  Learned Sparse Retrieval (LSR) models use vocabularies from pre-trained\ntransformers, which often split entities into nonsensical fragments. Splitting\nentities can reduce retrieval accuracy and limits the model's ability to\nincorporate up-to-date world knowledge not included in the training data. In\nthis work, we enhance the LSR vocabulary with Wikipedia concepts and entities,\nenabling the model to resolve ambiguities more effectively and stay current\nwith evolving knowledge. Central to our approach is a Dynamic Vocabulary (DyVo)\nhead, which leverages existing entity embeddings and an entity retrieval\ncomponent that identifies entities relevant to a query or document. We use the\nDyVo head to generate entity weights, which are then merged with word piece\nweights to create joint representations for efficient indexing and retrieval\nusing an inverted index. In experiments across three entity-rich document\nranking datasets, the resulting DyVo model substantially outperforms\nstate-of-the-art baselines.\n","authors":["Thong Nguyen","Shubham Chatterjee","Sean MacAvaney","Iain Mackie","Jeff Dalton","Andrew Yates"],"pdf_url":"https://arxiv.org/pdf/2410.07722v2.pdf","comment":"https://github.com/thongnt99/DyVo"},{"id":"http://arxiv.org/abs/2410.11841v1","updated":"2024-10-15T17:59:30Z","published":"2024-10-15T17:59:30Z","title":"GaVaMoE: Gaussian-Variational Gated Mixture of Experts for Explainable\n  Recommendation","summary":"  Large language model-based explainable recommendation (LLM-based ER) systems\nshow promise in generating human-like explanations for recommendations.\nHowever, they face challenges in modeling user-item collaborative preferences,\npersonalizing explanations, and handling sparse user-item interactions. To\naddress these issues, we propose GaVaMoE, a novel Gaussian-Variational Gated\nMixture of Experts framework for explainable recommendation. GaVaMoE introduces\ntwo key components: (1) a rating reconstruction module that employs Variational\nAutoencoder (VAE) with a Gaussian Mixture Model (GMM) to capture complex\nuser-item collaborative preferences, serving as a pre-trained multi-gating\nmechanism; and (2) a set of fine-grained expert models coupled with the\nmulti-gating mechanism for generating highly personalized explanations. The VAE\ncomponent models latent factors in user-item interactions, while the GMM\nclusters users with similar behaviors. Each cluster corresponds to a gate in\nthe multi-gating mechanism, routing user-item pairs to appropriate expert\nmodels. This architecture enables GaVaMoE to generate tailored explanations for\nspecific user types and preferences, mitigating data sparsity by leveraging\nuser similarities. Extensive experiments on three real-world datasets\ndemonstrate that GaVaMoE significantly outperforms existing methods in\nexplanation quality, personalization, and consistency. Notably, GaVaMoE\nexhibits robust performance in scenarios with sparse user-item interactions,\nmaintaining high-quality explanations even for users with limited historical\ndata.\n","authors":["Fei Tang","Yongliang Shen","Hang Zhang","Zeqi Tan","Wenqi Zhang","Guiyang Hou","Kaitao Song","Weiming Lu","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2410.11841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03764v2","updated":"2024-10-15T16:01:11Z","published":"2024-05-06T18:02:00Z","title":"GOVERN: Gradient Orientation Vote Ensemble for Multi-Teacher Reinforced\n  Distillation","summary":"  Pre-trained language models have become an integral component of\nquestion-answering systems, achieving remarkable performance. However, for\npractical deployment, it is crucial to perform knowledge distillation to\nmaintain high performance while operating under computational constraints. In\nthis paper, we address a key question: given the importance of unsupervised\ndistillation for student model performance, how can knowledge from multiple\nteacher models be effectively ensemble during this stage without the guidance\nof labels? We propose a novel algorithm, GOVERN, to tackle this issue. GOVERN\nhas demonstrated significant improvements in both offline and online\nexperiments, enabling the student model to achieve results comparable to that\nof teacher ensembles. Our experiments show that GOVERN remarkably requires a\nmere 1\\% of the ensemble method's inference budget to achieve 99.5\\% of\nperformance. The proposed algorithm has been successfully deployed in a\nreal-world commercial question-answering system, demonstrating its real-world\napplicability.\n","authors":["Wenjie Zhou","Zhenxin Ding","Xiaodong Zhang","Haibo Shi","Junfeng Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2405.03764v2.pdf","comment":"Accepted by EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.11719v1","updated":"2024-10-15T15:50:53Z","published":"2024-10-15T15:50:53Z","title":"Adaptive Coordinators and Prompts on Heterogeneous Graphs for\n  Cross-Domain Recommendations","summary":"  In the online digital world, users frequently engage with diverse items\nacross multiple domains (e.g., e-commerce platforms, streaming services, and\nsocial media networks), forming complex heterogeneous interaction graphs.\nLeveraging this multi-domain information can undoubtedly enhance the\nperformance of recommendation systems by providing more comprehensive user\ninsights and alleviating data sparsity in individual domains. However,\nintegrating multi-domain knowledge for the cross-domain recommendation is very\nhard due to inherent disparities in user behavior and item characteristics and\nthe risk of negative transfer, where irrelevant or conflicting information from\nthe source domains adversely impacts the target domain's performance. To\naddress these challenges, we offer HAGO, a novel framework with\n$\\textbf{H}$eterogeneous $\\textbf{A}$daptive $\\textbf{G}$raph\nco$\\textbf{O}$rdinators, which dynamically integrate multi-domain graphs into a\ncohesive structure by adaptively adjusting the connections between coordinators\nand multi-domain graph nodes, thereby enhancing beneficial inter-domain\ninteractions while mitigating negative transfer effects. Additionally, we\ndevelop a universal multi-domain graph pre-training strategy alongside HAGO to\ncollaboratively learn high-quality node representations across domains. To\neffectively transfer the learned multi-domain knowledge to the target domain,\nwe design an effective graph prompting method, which incorporates pre-trained\nembeddings with learnable prompts for the recommendation task. Our framework is\ncompatible with various graph-based models and pre-training techniques,\ndemonstrating broad applicability and effectiveness. Further experimental\nresults show that our solutions outperform state-of-the-art methods in\nmulti-domain recommendation scenarios and highlight their potential for\nreal-world applications.\n","authors":["Hengyu Zhang","Chunxu Shen","Xiangguo Sun","Jie Tan","Yu Rong","Chengzhi Piao","Hong Cheng","Lingling Yi"],"pdf_url":"https://arxiv.org/pdf/2410.11719v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.07671v2","updated":"2024-10-15T15:29:51Z","published":"2024-10-10T07:29:31Z","title":"DISCO: A Hierarchical Disentangled Cognitive Diagnosis Framework for\n  Interpretable Job Recommendation","summary":"  The rapid development of online recruitment platforms has created\nunprecedented opportunities for job seekers while concurrently posing the\nsignificant challenge of quickly and accurately pinpointing positions that\nalign with their skills and preferences. Job recommendation systems have\nsignificantly alleviated the extensive search burden for job seekers by\noptimizing user engagement metrics, such as clicks and applications, thus\nachieving notable success. In recent years, a substantial amount of research\nhas been devoted to developing effective job recommendation models, primarily\nfocusing on text-matching based and behavior modeling based methods. While\nthese approaches have realized impressive outcomes, it is imperative to note\nthat research on the explainability of recruitment recommendations remains\nprofoundly unexplored. To this end, in this paper, we propose DISCO, a\nhierarchical Disentanglement based Cognitive diagnosis framework, aimed at\nflexibly accommodating the underlying representation learning model for\neffective and interpretable job recommendations. Specifically, we first design\na hierarchical representation disentangling module to explicitly mine the\nhierarchical skill-related factors implied in hidden representations of job\nseekers and jobs. Subsequently, we propose level-aware association modeling to\nenhance information communication and robust representation learning both\ninter- and intra-level, which consists of the interlevel knowledge influence\nmodule and the level-wise contrastive learning. Finally, we devise an\ninteraction diagnosis module incorporating a neural diagnosis function for\neffectively modeling the multi-level recruitment interaction process between\njob seekers and jobs, which introduces the cognitive measurement theory.\n","authors":["Xiaoshan Yu","Chuan Qin","Qi Zhang","Chen Zhu","Haiping Ma","Xingyi Zhang","Hengshu Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.07671v2.pdf","comment":"Accepted by ICDM 2024. 10 pages"},{"id":"http://arxiv.org/abs/2401.00797v2","updated":"2024-10-15T12:37:40Z","published":"2024-01-01T15:57:15Z","title":"Curriculum-scheduled Knowledge Distillation from Multiple Pre-trained\n  Teachers for Multi-domain Sequential Recommendation","summary":"  Pre-trained recommendation models (PRMs) have received increasing interest\nrecently. However, their intrinsically heterogeneous model structure, huge\nmodel size and computation cost hinder their adoptions in practical recommender\nsystems. Hence, it is highly essential to explore how to use different\npre-trained recommendation models efficiently in real-world systems. In this\npaper, we propose a novel curriculum-scheduled knowledge distillation from\nmultiple pre-trained teachers for multi-domain sequential recommendation,\ncalled CKD-MDSR, which takes full advantages of different PRMs as multiple\nteacher models to boost a small student recommendation model, integrating the\nknowledge across multiple domains from PRMs. Specifically, CKD-MDSR first\nadopts curriculum-scheduled user behavior sequence sampling and distills\ninformative knowledge jointly from the representative PRMs such as UniSRec and\nRecformer. Then, the knowledge from the above PRMs are selectively integrated\ninto the student model in consideration of their confidence and consistency.\nFinally, we verify the proposed method on multi-domain sequential\nrecommendation and further demonstrate its universality with multiple types of\nstudent models, including feature interaction and graph based recommendation\nmodels. Extensive experiments on five real-world datasets demonstrate the\neffectiveness and efficiency of CKD-MDSR, which can be viewed as an efficient\nshortcut using PRMs in real-world systems.\n","authors":["Wenqi Sun","Ruobing Xie","Junjie Zhang","Wayne Xin Zhao","Leyu Lin","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2401.00797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10645v2","updated":"2024-10-15T11:01:33Z","published":"2024-08-20T08:36:59Z","title":"CoRA: Collaborative Information Perception by Large Language Model's\n  Weights for Recommendation","summary":"  Involving collaborative information in Large Language Models (LLMs) is a\npromising technique for adapting LLMs for recommendation. Existing methods\nachieve this by concatenating collaborative features with text tokens into a\nunified sequence input and then fine-tuning to align these features with LLM's\ninput space. Although effective, in this work, we identify two limitations when\nadapting LLMs to recommendation tasks, which hinder the integration of general\nknowledge and collaborative information, resulting in sub-optimal\nrecommendation performance. (1) Fine-tuning LLM with recommendation data can\nundermine its inherent world knowledge and fundamental competencies, which are\ncrucial for interpreting and inferring recommendation text. (2) Incorporating\ncollaborative features into textual prompts disrupts the semantics of the\noriginal prompts, preventing LLM from generating appropriate outputs. In this\npaper, we propose a new paradigm, CoRA (an acronym for Collaborative LoRA),\nwith a collaborative weights generator. Rather than input space alignment, this\nmethod aligns collaborative information with LLM's parameter space,\nrepresenting them as incremental weights to update LLM's output. This way, LLM\nperceives collaborative information without altering its general knowledge and\ntext inference capabilities. Specifically, we employ a collaborative filtering\nmodel to extract user and item embeddings, converting them into collaborative\nweights with low-rank properties through the collaborative weights generator.\nWe then merge the collaborative weights into LLM's weights, enabling LLM to\nperceive the collaborative signals and generate personalized recommendations\nwithout fine-tuning or extra collaborative tokens in prompts. Extensive\nexperiments confirm that CoRA effectively integrates collaborative information\ninto LLM, enhancing recommendation performance.\n","authors":["Yuting Liu","Jinghao Zhang","Yizhou Dang","Yuliang Liang","Qiang Liu","Guibing Guo","Jianzhe Zhao","Xingwei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.10645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11464v1","updated":"2024-10-15T10:11:18Z","published":"2024-10-15T10:11:18Z","title":"CoActionGraphRec: Sequential Multi-Interest Recommendations Using\n  Co-Action Graphs","summary":"  There are unique challenges to developing item recommender systems for\ne-commerce platforms like eBay due to sparse data and diverse user interests.\nWhile rich user-item interactions are important, eBay's data sparsity exceeds\nother e-commerce sites by an order of magnitude. To address this challenge, we\npropose CoActionGraphRec (CAGR), a text based two-tower deep learning model\n(Item Tower and User Tower) utilizing co-action graph layers. In order to\nenhance user and item representations, a graph-based solution tailored to\neBay's environment is utilized. For the Item Tower, we represent each item\nusing its co-action items to capture collaborative signals in a co-action graph\nthat is fully leveraged by the graph neural network component. For the User\nTower, we build a fully connected graph of each user's behavior sequence, with\nedges encoding pairwise relationships. Furthermore, an explicit interaction\nmodule learns representations capturing behavior interactions. Extensive\noffline and online A/B test experiments demonstrate the effectiveness of our\nproposed approach and results show improved performance over state-of-the-art\nmethods on key metrics.\n","authors":["Yi Sun","Yuri M. Brovman"],"pdf_url":"https://arxiv.org/pdf/2410.11464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11457v1","updated":"2024-10-15T10:02:55Z","published":"2024-10-15T10:02:55Z","title":"LR-SQL: A Supervised Fine-Tuning Method for Text2SQL Tasks under\n  Low-Resource Scenarios","summary":"  Large language models revolutionize Text2SQL through supervised fine-tuning,\nyet a crucial limitation is overlooked: the complexity of databases leads to an\nincreased context length, consequently resulting in higher GPU memory demands\nfor model fine-tuning. To address this issue, we propose LR-SQL. LR-SQL\ncomprises two supervised fine-tuning models: the schema\\_link model and the\nSQL\\_generation model, with the schema\\_link model serving as the focal point\nfor streamlining the overall process. During the fine-tuning of the\nschema\\_link model, LR-SQL breaks down the complete database into flexible\ncombinations of tables with adjustable quantities, enabling the model to learn\nthe relationships within the entire database from these dispersed slices.\nFurthermore, to enhance the model's ability to perceive the relationships among\nvarious discrete slices during inference, LR-SQL trains the model's\nChain-of-Thought capability for this task. Experimental results demonstrate\nthat LR-SQL can reduce the total GPU memory usage by 40\\% compared to existing\nfine-tuning methods, while only losing 2\\% of table prediction accuracy in\nschema\\_link task. For the overall Text2SQL task, the Execution Accuracy\ndecrease by 0.6\\%.Our project is now available on\nhttps://github.com/hongWin/LR-SQL\n","authors":["Wen Wuzhenghong","Zhang Yongpan","Pan Su","Sun Yuwei","Lu Pengwei","Ding Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.11457v1.pdf","comment":"12pages, 4 figures,submitting to a journal"},{"id":"http://arxiv.org/abs/2410.11370v1","updated":"2024-10-15T07:50:34Z","published":"2024-10-15T07:50:34Z","title":"Enhance Graph Alignment for Large Language Models","summary":"  Graph-structured data is prevalent in the real world. Recently, due to the\npowerful emergent capabilities, Large Language Models (LLMs) have shown\npromising performance in modeling graphs. The key to effectively applying LLMs\non graphs is converting graph data into a format LLMs can comprehend.\nGraph-to-token approaches are popular in enabling LLMs to process graph\ninformation. They transform graphs into sequences of tokens and align them with\ntext tokens through instruction tuning, where self-supervised instruction\ntuning helps LLMs acquire general knowledge about graphs, and supervised\nfine-tuning specializes LLMs for the downstream tasks on graphs. Despite their\ninitial success, we find that existing methods have a misalignment between\nself-supervised tasks and supervised downstream tasks, resulting in negative\ntransfer from self-supervised fine-tuning to downstream tasks. To address these\nissues, we propose Graph Alignment Large Language Models (GALLM) to benefit\nfrom aligned task templates. In the self-supervised tuning stage, we introduce\na novel text matching task using templates aligned with downstream tasks. In\nthe task-specific tuning stage, we propose two category prompt methods that\nlearn supervision information from additional explanation with further aligned\ntemplates. Experimental evaluations on four datasets demonstrate substantial\nimprovements in supervised learning, multi-dataset generalizability, and\nparticularly in zero-shot capability, highlighting the model's potential as a\ngraph foundation model.\n","authors":["Haitong Luo","Xuying Meng","Suhang Wang","Tianxiang Zhao","Fali Wang","Hanyun Cao","Yujun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.11370v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.11355v1","updated":"2024-10-15T07:25:33Z","published":"2024-10-15T07:25:33Z","title":"Reducing Labeling Costs in Sentiment Analysis via Semi-Supervised\n  Learning","summary":"  Labeling datasets is a noteworthy challenge in machine learning, both in\nterms of cost and time. This research, however, leverages an efficient answer.\nBy exploring label propagation in semi-supervised learning, we can\nsignificantly reduce the number of labels required compared to traditional\nmethods. We employ a transductive label propagation method based on the\nmanifold assumption for text classification. Our approach utilizes a\ngraph-based method to generate pseudo-labels for unlabeled data for the text\nclassification task, which are then used to train deep neural networks. By\nextending labels based on cosine proximity within a nearest neighbor graph from\nnetwork embeddings, we combine unlabeled data into supervised learning, thereby\nreducing labeling costs. Based on previous successes in other domains, this\nstudy builds and evaluates this approach's effectiveness in sentiment analysis,\npresenting insights into semi-supervised learning.\n","authors":["Minoo Jafarlou","Mario M. Kubek"],"pdf_url":"https://arxiv.org/pdf/2410.11355v1.pdf","comment":"12 pages, 7 figures, accepted at the 2024 8th International\n  Conference on Natural Language Processing and Information Retrieval (NLPIR\n  2024), Okayama, Japan, 2024"},{"id":"http://arxiv.org/abs/2410.11327v1","updated":"2024-10-15T06:54:27Z","published":"2024-10-15T06:54:27Z","title":"Sequential LLM Framework for Fashion Recommendation","summary":"  The fashion industry is one of the leading domains in the global e-commerce\nsector, prompting major online retailers to employ recommendation systems for\nproduct suggestions and customer convenience. While recommendation systems have\nbeen widely studied, most are designed for general e-commerce problems and\nstruggle with the unique challenges of the fashion domain. To address these\nissues, we propose a sequential fashion recommendation framework that leverages\na pre-trained large language model (LLM) enhanced with recommendation-specific\nprompts. Our framework employs parameter-efficient fine-tuning with extensive\nfashion data and introduces a novel mix-up-based retrieval technique for\ntranslating text into relevant product suggestions. Extensive experiments show\nour proposed framework significantly enhances fashion recommendation\nperformance.\n","authors":["Han Liu","Xianfeng Tang","Tianlang Chen","Jiapeng Liu","Indu Indu","Henry Peng Zou","Peng Dai","Roberto Fernandez Galan","Michael D Porter","Dongmei Jia","Ning Zhang","Lian Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.11327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00080v3","updated":"2024-10-15T05:34:07Z","published":"2024-04-30T16:35:08Z","title":"Recommenadation aided Caching using Combinatorial Multi-armed Bandits","summary":"  We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.\n","authors":["Pavamana K J","Chandramani Kishore Singh"],"pdf_url":"https://arxiv.org/pdf/2405.00080v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02130v4","updated":"2024-10-15T04:06:44Z","published":"2024-01-04T08:31:47Z","title":"Spectral-Based Graph Neural Networks for Complementary Item\n  Recommendation","summary":"  Modeling complementary relationships greatly helps recommender systems to\naccurately and promptly recommend the subsequent items when one item is\npurchased. Unlike traditional similar relationships, items with complementary\nrelationships may be purchased successively (such as iPhone and Airpods Pro),\nand they not only share relevance but also exhibit dissimilarity. Since the two\nattributes are opposites, modeling complementary relationships is challenging.\nPrevious attempts to exploit these relationships have either ignored or\noversimplified the dissimilarity attribute, resulting in ineffective modeling\nand an inability to balance the two attributes. Since Graph Neural Networks\n(GNNs) can capture the relevance and dissimilarity between nodes in the\nspectral domain, we can leverage spectral-based GNNs to effectively understand\nand model complementary relationships. In this study, we present a novel\napproach called Spectral-based Complementary Graph Neural Networks (SComGNN)\nthat utilizes the spectral properties of complementary item graphs. We make the\nfirst observation that complementary relationships consist of low-frequency and\nmid-frequency components, corresponding to the relevance and dissimilarity\nattributes, respectively. Based on this spectral observation, we design\nspectral graph convolutional networks with low-pass and mid-pass filters to\ncapture the low-frequency and mid-frequency components. Additionally, we\npropose a two-stage attention mechanism to adaptively integrate and balance the\ntwo attributes. Experimental results on four e-commerce datasets demonstrate\nthe effectiveness of our model, with SComGNN significantly outperforming\nexisting baseline models.\n","authors":["Haitong Luo","Xuying Meng","Suhang Wang","Hanyun Cao","Weiyao Zhang","Yequan Wang","Yujun Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.02130v4.pdf","comment":"Accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2410.11217v1","updated":"2024-10-15T03:04:26Z","published":"2024-10-15T03:04:26Z","title":"On the Capacity of Citation Generation by Large Language Models","summary":"  Retrieval-augmented generation (RAG) appears as a promising method to\nalleviate the \"hallucination\" problem in large language models (LLMs), since it\ncan incorporate external traceable resources for response generation. The\nessence of RAG in combating the hallucination issue lies in accurately\nattributing claims in responses to the corresponding retrieved documents.\nHowever, most of existing works focus on improving the quality of generated\nresponses from the LLM, while largely overlooked its ability to attribute\nsources accurately. In this study, we conduct a systematic analysis about the\ncapabilities of LLMs in generating citations within response generation, and\nfurther introduce a novel method to enhance their citation generation\nabilities. Specifically, we evaluate both the correctness and citation quality\nfor seven widely-used LLMs on two benchmark datasets. Meanwhile, we introduce\nnew citation evaluation metrics to eliminate the over-penalization of\nunnecessary and excessive citations in existing metrics. Furthermore, we\npropose a Generate-then-Refine method that completes relevant citations and\nremoves irrelevant ones without altering the response text. The results on\nWebGLM-QA, ASQA and ELI5 datasets show that our method substantially improves\nthe quality of citations in responses generated by LLMs.\n","authors":["Haosheng Qian","Yixing Fan","Ruqing Zhang","Jiafeng Guo"],"pdf_url":"https://arxiv.org/pdf/2410.11217v1.pdf","comment":"Accepted by CCIR 2024"},{"id":"http://arxiv.org/abs/2410.11150v1","updated":"2024-10-15T00:23:18Z","published":"2024-10-15T00:23:18Z","title":"Optimizing Encoder-Only Transformers for Session-Based Recommendation\n  Systems","summary":"  Session-based recommendation is the task of predicting the next item a user\nwill interact with, often without access to historical user data. In this work,\nwe introduce Sequential Masked Modeling, a novel approach for encoder-only\ntransformer architectures to tackle the challenges of single-session\nrecommendation. Our method combines data augmentation through window sliding\nwith a unique penultimate token masking strategy to capture sequential\ndependencies more effectively. By enhancing how transformers handle session\ndata, Sequential Masked Modeling significantly improves next-item prediction\nperformance.\n  We evaluate our approach on three widely-used datasets, Yoochoose 1/64,\nDiginetica, and Tmall, comparing it to state-of-the-art single-session,\ncross-session, and multi-relation approaches. The results demonstrate that our\nTransformer-SMM models consistently outperform all models that rely on the same\namount of information, while even rivaling methods that have access to more\nextensive user history. This study highlights the potential of encoder-only\ntransformers in session-based recommendation and opens the door for further\nimprovements.\n","authors":["Anis Redjdal","Luis Pinto","Michel Desmarais"],"pdf_url":"https://arxiv.org/pdf/2410.11150v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.12051v1","updated":"2024-10-15T20:41:10Z","published":"2024-10-15T20:41:10Z","title":"Enabling Data-Driven and Empathetic Interactions: A Context-Aware 3D\n  Virtual Agent in Mixed Reality for Enhanced Financial Customer Experience","summary":"  In this paper, we introduce a novel system designed to enhance customer\nservice in the financial and retail sectors through a context-aware 3D virtual\nagent, utilizing Mixed Reality (MR) and Vision Language Models (VLMs). Our\napproach focuses on enabling data-driven and empathetic interactions that\nensure customer satisfaction by introducing situational awareness of the\nphysical location, personalized interactions based on customer profiles, and\nrigorous privacy and security standards. We discuss our design considerations\ncritical for deployment in real-world customer service environments, addressing\nchallenges in user data management and sensitive information handling. We also\noutline the system architecture and key features unique to banking and retail\nenvironments. Our work demonstrates the potential of integrating MR and VLMs in\nservice industries, offering practical insights in customer service delivery\nwhile maintaining high standards of security and personalization.\n","authors":["Cindy Xu","Mengyu Chen","Pranav Deshpande","Elvir Azanli","Runqing Yang","Joseph Ligman"],"pdf_url":"https://arxiv.org/pdf/2410.12051v1.pdf","comment":"to appear at 1st Workshop on Intelligent XR: Harnessing AI for\n  Next-Generation XR User Experiences at International Symposium on Mixed and\n  Augmented Reality (ISMAR) 2024"},{"id":"http://arxiv.org/abs/2410.12018v1","updated":"2024-10-15T19:33:57Z","published":"2024-10-15T19:33:57Z","title":"LocoMotion: Learning Motion-Focused Video-Language Representations","summary":"  This paper strives for motion-focused video-language representations.\nExisting methods to learn video-language representations use spatial-focused\ndata, where identifying the objects and scene is often enough to distinguish\nthe relevant caption. We instead propose LocoMotion to learn from\nmotion-focused captions that describe the movement and temporal progression of\nlocal object motions. We achieve this by adding synthetic motions to videos and\nusing the parameters of these motions to generate corresponding captions.\nFurthermore, we propose verb-variation paraphrasing to increase the caption\nvariety and learn the link between primitive motions and high-level verbs. With\nthis, we are able to learn a motion-focused video-language representation.\nExperiments demonstrate our approach is effective for a variety of downstream\ntasks, particularly when limited data is available for fine-tuning. Code is\navailable: https://hazeldoughty.github.io/Papers/LocoMotion/\n","authors":["Hazel Doughty","Fida Mohammad Thoker","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12018v1.pdf","comment":"ACCV 2024"},{"id":"http://arxiv.org/abs/2410.11817v1","updated":"2024-10-15T17:46:31Z","published":"2024-10-15T17:46:31Z","title":"Improving Long-Text Alignment for Text-to-Image Diffusion Models","summary":"  The rapid advancement of text-to-image (T2I) diffusion models has enabled\nthem to generate unprecedented results from given texts. However, as text\ninputs become longer, existing encoding methods like CLIP face limitations, and\naligning the generated images with long texts becomes challenging. To tackle\nthese issues, we propose LongAlign, which includes a segment-level encoding\nmethod for processing long texts and a decomposed preference optimization\nmethod for effective alignment training. For segment-level encoding, long texts\nare divided into multiple segments and processed separately. This method\novercomes the maximum input length limits of pretrained encoding models. For\npreference optimization, we provide decomposed CLIP-based preference models to\nfine-tune diffusion models. Specifically, to utilize CLIP-based preference\nmodels for T2I alignment, we delve into their scoring mechanisms and find that\nthe preference scores can be decomposed into two components: a text-relevant\npart that measures T2I alignment and a text-irrelevant part that assesses other\nvisual aspects of human preference. Additionally, we find that the\ntext-irrelevant part contributes to a common overfitting problem during\nfine-tuning. To address this, we propose a reweighting strategy that assigns\ndifferent weights to these two components, thereby reducing overfitting and\nenhancing alignment. After fine-tuning $512 \\times 512$ Stable Diffusion (SD)\nv1.5 for about 20 hours using our method, the fine-tuned SD outperforms\nstronger foundation models in T2I alignment, such as PixArt-$\\alpha$ and\nKandinsky v2.2. The code is available at\nhttps://github.com/luping-liu/LongAlign.\n","authors":["Luping Liu","Chao Du","Tianyu Pang","Zehan Wang","Chongxuan Li","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2410.11817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12831v2","updated":"2024-10-15T17:31:56Z","published":"2024-06-18T17:51:37Z","title":"VIA: Unified Spatiotemporal Video Adaptation Framework for Global and\n  Local Video Editing","summary":"  Video editing is a cornerstone of digital media, from entertainment and\neducation to professional communication. However, previous methods often\noverlook the necessity of comprehensively understanding both global and local\ncontexts, leading to inaccurate and inconsistent edits in the spatiotemporal\ndimension, especially for long videos. In this paper, we introduce VIA, a\nunified spatiotemporal Video Adaptation framework for global and local video\nediting, pushing the limits of consistently editing minute-long videos. First,\nto ensure local consistency within individual frames, we designed test-time\nediting adaptation to adapt a pre-trained image editing model for improving\nconsistency between potential editing directions and the text instruction, and\nadapt masked latent variables for precise local control. Furthermore, to\nmaintain global consistency over the video sequence, we introduce\nspatiotemporal adaptation that recursively gather consistent attention\nvariables in key frames and strategically applies them across the whole\nsequence to realize the editing effects. Extensive experiments demonstrate\nthat, compared to baseline methods, our VIA approach produces edits that are\nmore faithful to the source videos, more coherent in the spatiotemporal\ncontext, and more precise in local control. More importantly, we show that VIA\ncan achieve consistent long video editing in minutes, unlocking the potential\nfor advanced video editing tasks over long video sequences.\n","authors":["Jing Gu","Yuwei Fang","Ivan Skorokhodov","Peter Wonka","Xinya Du","Sergey Tulyakov","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2406.12831v2.pdf","comment":"19 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.11779v1","updated":"2024-10-15T16:57:44Z","published":"2024-10-15T16:57:44Z","title":"MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation","summary":"  Multimodal Large Language Models (MLLMs) frequently exhibit hallucination\nphenomena, but the underlying reasons remain poorly understood. In this paper,\nwe present an empirical analysis and find that, although MLLMs incorrectly\ngenerate the objects in the final output, they are actually able to recognize\nvisual objects in the preceding layers. We speculate that this may be due to\nthe strong knowledge priors of the language model suppressing the visual\ninformation, leading to hallucinations. Motivated by this, we propose a novel\ndynamic correction decoding method for MLLMs (DeCo), which adaptively selects\nthe appropriate preceding layers and proportionally integrates knowledge into\nthe final layer to adjust the output logits. Note that DeCo is model agnostic\nand can be seamlessly incorporated with various classic decoding strategies and\napplied to different MLLMs. We evaluate DeCo on widely-used benchmarks,\ndemonstrating that it can reduce hallucination rates by a large margin compared\nto baselines, highlighting its potential to mitigate hallucinations. Code is\navailable at https://github.com/zjunlp/DeCo.\n","authors":["Chenxi Wang","Xiang Chen","Ningyu Zhang","Bozhong Tian","Haoming Xu","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2410.11779v1.pdf","comment":"Ongoing work"},{"id":"http://arxiv.org/abs/2410.11701v1","updated":"2024-10-15T15:39:37Z","published":"2024-10-15T15:39:37Z","title":"Magnifier Prompt: Tackling Multimodal Hallucination via Extremely Simple\n  Instructions","summary":"  Hallucinations in multimodal large language models (MLLMs) hinder their\npractical applications. To address this, we propose a Magnifier Prompt\n(MagPrompt), a simple yet effective method to tackle hallucinations in MLLMs\nvia extremely simple instructions. MagPrompt is based on the following two key\nprinciples, which guide the design of various effective prompts, demonstrating\nrobustness: (1) MLLMs should focus more on the image. (2) When there are\nconflicts between the image and the model's inner knowledge, MLLMs should\nprioritize the image. MagPrompt is training-free and can be applied to\nopen-source and closed-source models, such as GPT-4o and Gemini-pro. It\nperforms well across many datasets and its effectiveness is comparable or even\nbetter than more complex methods like VCD. Furthermore, our prompt design\nprinciples and experimental analyses provide valuable insights into multimodal\nhallucination.\n","authors":["Yuhan Fu","Ruobing Xie","Jiazhen Liu","Bangxiang Lan","Xingwu Sun","Zhanhui Kang","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2410.11701v1.pdf","comment":"9 pages, 13 tables, 4 figures"},{"id":"http://arxiv.org/abs/2410.11582v1","updated":"2024-10-15T13:15:50Z","published":"2024-10-15T13:15:50Z","title":"On-the-fly Modulation for Balanced Multimodal Learning","summary":"  Multimodal learning is expected to boost model performance by integrating\ninformation from different modalities. However, its potential is not fully\nexploited because the widely-used joint training strategy, which has a uniform\nobjective for all modalities, leads to imbalanced and under-optimized uni-modal\nrepresentations. Specifically, we point out that there often exists modality\nwith more discriminative information, e.g., vision of playing football and\nsound of blowing wind. They could dominate the joint training process,\nresulting in other modalities being significantly under-optimized. To alleviate\nthis problem, we first analyze the under-optimized phenomenon from both the\nfeed-forward and the back-propagation stages during optimization. Then,\nOn-the-fly Prediction Modulation (OPM) and On-the-fly Gradient Modulation (OGM)\nstrategies are proposed to modulate the optimization of each modality, by\nmonitoring the discriminative discrepancy between modalities during training.\nConcretely, OPM weakens the influence of the dominant modality by dropping its\nfeature with dynamical probability in the feed-forward stage, while OGM\nmitigates its gradient in the back-propagation stage. In experiments, our\nmethods demonstrate considerable improvement across a variety of multimodal\ntasks. These simple yet effective strategies not only enhance performance in\nvanilla and task-oriented multimodal models, but also in more complex\nmultimodal tasks, showcasing their effectiveness and flexibility. The source\ncode is available at \\url{https://github.com/GeWu-Lab/BML_TPAMI2024}.\n","authors":["Yake Wei","Di Hu","Henghui Du","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2410.11582v1.pdf","comment":"Accepted by T-PAMI 2024"},{"id":"http://arxiv.org/abs/2404.01336v3","updated":"2024-10-15T12:40:39Z","published":"2024-03-30T14:39:09Z","title":"FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain\n  Fake News Detection","summary":"  Existing benchmarks for fake news detection have significantly contributed to\nthe advancement of models in assessing the authenticity of news content.\nHowever, these benchmarks typically focus solely on news pertaining to a single\nsemantic topic or originating from a single platform, thereby failing to\ncapture the diversity of multi-domain news in real scenarios. In order to\nunderstand fake news across various domains, the external knowledge and\nfine-grained annotations are indispensable to provide precise evidence and\nuncover the diverse underlying strategies for fabrication, which are also\nignored by existing benchmarks. To address this gap, we introduce a novel\nmulti-domain knowledge-enhanced benchmark with fine-grained annotations, named\n\\textbf{FineFake}. FineFake encompasses 16,909 data samples spanning six\nsemantic topics and eight platforms. Each news item is enriched with\nmulti-modal content, potential social context, semi-manually verified common\nknowledge, and fine-grained annotations that surpass conventional binary\nlabels. Furthermore, we formulate three challenging tasks based on FineFake and\npropose a knowledge-enhanced domain adaptation network. Extensive experiments\nare conducted on FineFake under various scenarios, providing accurate and\nreliable benchmarks for future endeavors. The entire FineFake project is\npublicly accessible as an open-source repository at\n\\url{https://github.com/Accuser907/FineFake}.\n","authors":["Ziyi Zhou","Xiaoming Zhang","Litian Zhang","Jiacheng Liu","Senzhang Wang","Zheng Liu","Xi Zhang","Chaozhuo Li","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2404.01336v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11522v1","updated":"2024-10-15T11:48:31Z","published":"2024-10-15T11:48:31Z","title":"Leveraging LLM Embeddings for Cross Dataset Label Alignment and Zero\n  Shot Music Emotion Prediction","summary":"  In this work, we present a novel method for music emotion recognition that\nleverages Large Language Model (LLM) embeddings for label alignment across\nmultiple datasets and zero-shot prediction on novel categories. First, we\ncompute LLM embeddings for emotion labels and apply non-parametric clustering\nto group similar labels, across multiple datasets containing disjoint labels.\nWe use these cluster centers to map music features (MERT) to the LLM embedding\nspace. To further enhance the model, we introduce an alignment regularization\nthat enables dissociation of MERT embeddings from different clusters. This\nfurther enhances the model's ability to better adaptation to unseen datasets.\nWe demonstrate the effectiveness of our approach by performing zero-shot\ninference on a new dataset, showcasing its ability to generalize to unseen\nlabels without additional training.\n","authors":["Renhang Liu","Abhinaba Roy","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2410.11522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11417v1","updated":"2024-10-15T09:07:25Z","published":"2024-10-15T09:07:25Z","title":"VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models","summary":"  Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs.\n","authors":["Xiaohan Lan","Yitian Yuan","Zequn Jie","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2410.11417v1.pdf","comment":"9 pages, 4 figures"}]},"2024-10-14T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.05536v2","updated":"2024-10-14T23:20:26Z","published":"2024-10-07T22:25:37Z","title":"On Feature Decorrelation in Cloth-Changing Person Re-identification","summary":"  Cloth-changing person re-identification (CC-ReID) poses a significant\nchallenge in computer vision. A prevailing approach is to prompt models to\nconcentrate on causal attributes, like facial features and hairstyles, rather\nthan confounding elements such as clothing appearance. Traditional methods to\nachieve this involve integrating multi-modality data or employing manually\nannotated clothing labels, which tend to complicate the model and require\nextensive human effort. In our study, we demonstrate that simply reducing\nfeature correlations during training can significantly enhance the baseline\nmodel's performance. We theoretically elucidate this effect and introduce a\nnovel regularization technique based on density ratio estimation. This\ntechnique aims to minimize feature correlation in the training process of\ncloth-changing ReID baselines. Our approach is model-independent, offering\nbroad enhancements without needing additional data or labels. We validate our\nmethod through comprehensive experiments on prevalent CC-ReID datasets, showing\nits effectiveness in improving baseline models' generalization capabilities.\n","authors":["Hongjun Wang","Jiyuan Chen","Renhe Jiang","Xuan Song","Yinqiang Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.05536v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11046v1","updated":"2024-10-14T19:51:32Z","published":"2024-10-14T19:51:32Z","title":"SGUQ: Staged Graph Convolution Neural Network for Alzheimer's Disease\n  Diagnosis using Multi-Omics Data","summary":"  Alzheimer's disease (AD) is a chronic neurodegenerative disorder and the\nleading cause of dementia, significantly impacting cost, mortality, and burden\nworldwide. The advent of high-throughput omics technologies, such as genomics,\ntranscriptomics, proteomics, and epigenomics, has revolutionized the molecular\nunderstanding of AD. Conventional AI approaches typically require the\ncompletion of all omics data at the outset to achieve optimal AD diagnosis,\nwhich are inefficient and may be unnecessary. To reduce the clinical cost and\nimprove the accuracy of AD diagnosis using multi-omics data, we propose a novel\nstaged graph convolutional network with uncertainty quantification (SGUQ). SGUQ\nbegins with mRNA and progressively incorporates DNA methylation and miRNA data\nonly when necessary, reducing overall costs and exposure to harmful tests.\nExperimental results indicate that 46.23% of the samples can be reliably\npredicted using only single-modal omics data (mRNA), while an additional 16.04%\nof the samples can achieve reliable predictions when combining two omics data\ntypes (mRNA + DNA methylation). In addition, the proposed staged SGUQ achieved\nan accuracy of 0.858 on ROSMAP dataset, which outperformed existing methods\nsignificantly. The proposed SGUQ can not only be applied to AD diagnosis using\nmulti-omics data but also has the potential for clinical decision-making using\nmulti-viewed data. Our implementation is publicly available at\nhttps://github.com/chenzhao2023/multiomicsuncertainty.\n","authors":["Liang Tao","Yixin Xie","Jeffrey D Deng","Hui Shen","Hong-Wen Deng","Weihua Zhou","Chen Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.11046v1.pdf","comment":"20 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.10994v1","updated":"2024-10-14T18:20:09Z","published":"2024-10-14T18:20:09Z","title":"GraFPrint: A GNN-Based Approach for Audio Identification","summary":"  This paper introduces GraFPrint, an audio identification framework that\nleverages the structural learning capabilities of Graph Neural Networks (GNNs)\nto create robust audio fingerprints. Our method constructs a k-nearest neighbor\n(k-NN) graph from time-frequency representations and applies max-relative graph\nconvolutions to encode local and global information. The network is trained\nusing a self-supervised contrastive approach, which enhances resilience to\nambient distortions by optimizing feature representation. GraFPrint\ndemonstrates superior performance on large-scale datasets at various levels of\ngranularity, proving to be both lightweight and scalable, making it suitable\nfor real-world applications with extensive reference databases.\n","authors":["Aditya Bhattacharjee","Shubhr Singh","Emmanouil Benetos"],"pdf_url":"https://arxiv.org/pdf/2410.10994v1.pdf","comment":"Submitted to IEEE International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP 2025)"},{"id":"http://arxiv.org/abs/2410.10639v1","updated":"2024-10-14T15:50:35Z","published":"2024-10-14T15:50:35Z","title":"Generating Model Parameters for Controlling: Parameter Diffusion for\n  Controllable Multi-Task Recommendation","summary":"  Commercial recommender systems face the challenge that task requirements from\nplatforms or users often change dynamically (e.g., varying preferences for\naccuracy or diversity). Ideally, the model should be re-trained after resetting\na new objective function, adapting to these changes in task requirements.\nHowever, in practice, the high computational costs associated with retraining\nmake this process impractical for models already deployed to online\nenvironments. This raises a new challenging problem: how to efficiently adapt\nthe learning model to different task requirements by controlling model\nparameters after deployment, without the need for retraining. To address this\nissue, we propose a novel controllable learning approach via Parameter\nDiffusion for controllable multi-task Recommendation (PaDiRec), which allows\nthe customization and adaptation of recommendation model parameters to new task\nrequirements without retraining. Specifically, we first obtain the optimized\nmodel parameters through adapter tunning based on the feasible task\nrequirements. Then, we utilize the diffusion model as a parameter generator,\nemploying classifier-free guidance in conditional training to learn the\ndistribution of optimized model parameters under various task requirements.\nFinally, the diffusion model is applied to effectively generate model\nparameters in a test-time adaptation manner given task requirements. As a\nmodel-agnostic approach, PaDiRec can leverage existing recommendation models as\nbackbones to enhance their controllability. Extensive experiments on public\ndatasets and a dataset from a commercial app, indicate that PaDiRec can\neffectively enhance controllability through efficient model parameter\ngeneration. The code is released at\nhttps://anonymous.4open.science/r/PaDiRec-DD13.\n","authors":["Chenglei Shen","Jiahao Zhao","Xiao Zhang","Weijie Yu","Ming He","Jianping Fan"],"pdf_url":"https://arxiv.org/pdf/2410.10639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10594v1","updated":"2024-10-14T15:04:18Z","published":"2024-10-14T15:04:18Z","title":"VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality\n  Documents","summary":"  Retrieval-augmented generation (RAG) is an effective technique that enables\nlarge language models (LLMs) to utilize external knowledge sources for\ngeneration. However, current RAG systems are solely based on text, rendering it\nimpossible to utilize vision information like layout and images that play\ncrucial roles in real-world multi-modality documents. In this paper, we\nintroduce VisRAG, which tackles this issue by establishing a vision-language\nmodel (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the\ndocument to obtain text, the document is directly embedded using a VLM as an\nimage and then retrieved to enhance the generation of a VLM. Compared to\ntraditional text-based RAG, VisRAG maximizes the retention and utilization of\nthe data information in the original documents, eliminating the information\nloss introduced during the parsing process. We collect both open-source and\nsynthetic data to train the retriever in VisRAG and explore a variety of\ngeneration methods. Experiments demonstrate that VisRAG outperforms traditional\nRAG in both the retrieval and generation stages, achieving a 25--39\\%\nend-to-end performance gain over traditional text-based RAG pipeline. Further\nanalysis reveals that VisRAG is effective in utilizing training data and\ndemonstrates strong generalization capability, positioning it as a promising\nsolution for RAG on multi-modality documents. Our code and data are available\nat https://github.com/openbmb/visrag .\n","authors":["Shi Yu","Chaoyue Tang","Bokai Xu","Junbo Cui","Junhao Ran","Yukun Yan","Zhenghao Liu","Shuo Wang","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.10594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10542v1","updated":"2024-10-14T14:22:12Z","published":"2024-10-14T14:22:12Z","title":"Rethinking Legal Judgement Prediction in a Realistic Scenario in the Era\n  of Large Language Models","summary":"  This study investigates judgment prediction in a realistic scenario within\nthe context of Indian judgments, utilizing a range of transformer-based models,\nincluding InLegalBERT, BERT, and XLNet, alongside LLMs such as Llama-2 and\nGPT-3.5 Turbo. In this realistic scenario, we simulate how judgments are\npredicted at the point when a case is presented for a decision in court, using\nonly the information available at that time, such as the facts of the case,\nstatutes, precedents, and arguments. This approach mimics real-world\nconditions, where decisions must be made without the benefit of hindsight,\nunlike retrospective analyses often found in previous studies. For transformer\nmodels, we experiment with hierarchical transformers and the summarization of\njudgment facts to optimize input for these models. Our experiments with LLMs\nreveal that GPT-3.5 Turbo excels in realistic scenarios, demonstrating robust\nperformance in judgment prediction. Furthermore, incorporating additional legal\ninformation, such as statutes and precedents, significantly improves the\noutcome of the prediction task. The LLMs also provide explanations for their\npredictions. To evaluate the quality of these predictions and explanations, we\nintroduce two human evaluation metrics: Clarity and Linking. Our findings from\nboth automatic and human evaluations indicate that, despite advancements in\nLLMs, they are yet to achieve expert-level performance in judgment prediction\nand explanation tasks.\n","authors":["Shubham Kumar Nigam","Aniket Deroy","Subhankar Maity","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2410.10542v1.pdf","comment":"Accepted on NLLP at EMNLP 2024"},{"id":"http://arxiv.org/abs/2309.00976v4","updated":"2024-10-14T14:11:22Z","published":"2023-09-02T16:20:41Z","title":"Pure Message Passing Can Estimate Common Neighbor for Link Prediction","summary":"  Message Passing Neural Networks (MPNNs) have emerged as the {\\em de facto}\nstandard in graph representation learning. However, when it comes to link\nprediction, they often struggle, surpassed by simple heuristics such as Common\nNeighbor (CN). This discrepancy stems from a fundamental limitation: while\nMPNNs excel in node-level representation, they stumble with encoding the joint\nstructural features essential to link prediction, like CN. To bridge this gap,\nwe posit that, by harnessing the orthogonality of input vectors, pure\nmessage-passing can indeed capture joint structural features. Specifically, we\nstudy the proficiency of MPNNs in approximating CN heuristics. Based on our\nfindings, we introduce the Message Passing Link Predictor (MPLP), a novel link\nprediction model. MPLP taps into quasi-orthogonal vectors to estimate\nlink-level structural features, all while preserving the node-level\ncomplexities. Moreover, our approach demonstrates that leveraging\nmessage-passing to capture structural features could offset MPNNs'\nexpressiveness limitations at the expense of estimation variance. We conduct\nexperiments on benchmark datasets from various domains, where our method\nconsistently outperforms the baseline methods.\n","authors":["Kaiwen Dong","Zhichun Guo","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2309.00976v4.pdf","comment":"Accepted to Neurips'24"},{"id":"http://arxiv.org/abs/2410.10455v1","updated":"2024-10-14T12:49:13Z","published":"2024-10-14T12:49:13Z","title":"Advancing Academic Knowledge Retrieval via LLM-enhanced Representation\n  Similarity Fusion","summary":"  In an era marked by robust technological growth and swift information\nrenewal, furnishing researchers and the populace with top-tier, avant-garde\nacademic insights spanning various domains has become an urgent necessity. The\nKDD Cup 2024 AQA Challenge is geared towards advancing retrieval models to\nidentify pertinent academic terminologies from suitable papers for scientific\ninquiries. This paper introduces the LLM-KnowSimFuser proposed by Robo Space,\nwhich wins the 2nd place in the competition. With inspirations drawed from the\nsuperior performance of LLMs on multiple tasks, after careful analysis of the\nprovided datasets, we firstly perform fine-tuning and inference using\nLLM-enhanced pre-trained retrieval models to introduce the tremendous language\nunderstanding and open-domain knowledge of LLMs into this task, followed by a\nweighted fusion based on the similarity matrix derived from the inference\nresults. Finally, experiments conducted on the competition datasets show the\nsuperiority of our proposal, which achieved a score of 0.20726 on the final\nleaderboard.\n","authors":["Wei Dai","Peng Fu","Chunjing Gan"],"pdf_url":"https://arxiv.org/pdf/2410.10455v1.pdf","comment":"The 2nd Place of KDD Cup 2024 OAG-Challenge AQA"},{"id":"http://arxiv.org/abs/2407.19669v2","updated":"2024-10-14T12:19:44Z","published":"2024-07-29T03:12:28Z","title":"mGTE: Generalized Long-Context Text Representation and Reranking Models\n  for Multilingual Text Retrieval","summary":"  We present systematic efforts in building long-context multilingual text\nrepresentation model (TRM) and reranker from scratch for text retrieval. We\nfirst introduce a text encoder (base size) enhanced with RoPE and unpadding,\npre-trained in a native 8192-token context (longer than 512 of previous\nmultilingual encoders). Then we construct a hybrid TRM and a cross-encoder\nreranker by contrastive learning. Evaluations show that our text encoder\noutperforms the same-sized previous state-of-the-art XLM-R. Meanwhile, our TRM\nand reranker match the performance of large-sized state-of-the-art BGE-M3\nmodels and achieve better results on long-context retrieval benchmarks. Further\nanalysis demonstrate that our proposed models exhibit higher efficiency during\nboth training and inference. We believe their efficiency and effectiveness\ncould benefit various researches and industrial applications.\n","authors":["Xin Zhang","Yanzhao Zhang","Dingkun Long","Wen Xie","Ziqi Dai","Jialong Tang","Huan Lin","Baosong Yang","Pengjun Xie","Fei Huang","Meishan Zhang","Wenjie Li","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.19669v2.pdf","comment":"Camera-ready version of EMNLP 2024: Industry Track"},{"id":"http://arxiv.org/abs/2410.10408v1","updated":"2024-10-14T12:00:58Z","published":"2024-10-14T12:00:58Z","title":"Medico: Towards Hallucination Detection and Correction with Multi-source\n  Evidence Fusion","summary":"  As we all know, hallucinations prevail in Large Language Models (LLMs), where\nthe generated content is coherent but factually incorrect, which inflicts a\nheavy blow on the widespread application of LLMs. Previous studies have shown\nthat LLMs could confidently state non-existent facts rather than answering ``I\ndon't know''. Therefore, it is necessary to resort to external knowledge to\ndetect and correct the hallucinated content. Since manual detection and\ncorrection of factual errors is labor-intensive, developing an automatic\nend-to-end hallucination-checking approach is indeed a needful thing. To this\nend, we present Medico, a Multi-source evidence fusion enhanced hallucination\ndetection and correction framework. It fuses diverse evidence from multiple\nsources, detects whether the generated content contains factual errors,\nprovides the rationale behind the judgment, and iteratively revises the\nhallucinated content. Experimental results on evidence retrieval (0.964 HR@5,\n0.908 MRR@5), hallucination detection (0.927-0.951 F1), and hallucination\ncorrection (0.973-0.979 approval rate) manifest the great potential of Medico.\nA video demo of Medico can be found at https://youtu.be/RtsO6CSesBI.\n","authors":["Xinping Zhao","Jindi Yu","Zhenyu Liu","Jifang Wang","Dongfang Li","Yibin Chen","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.10408v1.pdf","comment":"12 pages, 3 figures, 6 tables. Accepted by EMNLP 2024's demo track"},{"id":"http://arxiv.org/abs/2410.10381v1","updated":"2024-10-14T11:10:15Z","published":"2024-10-14T11:10:15Z","title":"Collaborative filtering based on nonnegative/binary matrix factorization","summary":"  Collaborative filtering generates recommendations based on user-item\nsimilarities through rating data, which may involve numerous unrated items. To\npredict scores for unrated items, matrix factorization techniques, such as\nnonnegative matrix factorization (NMF), are often employed to predict scores\nfor unrated items. Nonnegative/binary matrix factorization (NBMF), which is an\nextension of NMF, approximates a nonnegative matrix as the product of\nnonnegative and binary matrices. Previous studies have employed NBMF for image\nanalysis where the data were dense. In this paper, we propose a modified NBMF\nalgorithm that can be applied to collaborative filtering where data are sparse.\nIn the modified method, unrated elements in a rating matrix are masked, which\nimproves the collaborative filtering performance. Utilizing a low-latency Ising\nmachine in NBMF is advantageous in terms of the computation time, making the\nproposed method beneficial.\n","authors":["Yukino Terui","Yuka Inoue","Yohei Hamakawa","Kosuke Tatsumura","Kazue Kudo"],"pdf_url":"https://arxiv.org/pdf/2410.10381v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.10372v1","updated":"2024-10-14T10:55:58Z","published":"2024-10-14T10:55:58Z","title":"BookWorm: A Dataset for Character Description and Analysis","summary":"  Characters are at the heart of every story, driving the plot and engaging\nreaders. In this study, we explore the understanding of characters in\nfull-length books, which contain complex narratives and numerous interacting\ncharacters. We define two tasks: character description, which generates a brief\nfactual profile, and character analysis, which offers an in-depth\ninterpretation, including character development, personality, and social\ncontext. We introduce the BookWorm dataset, pairing books from the Gutenberg\nProject with human-written descriptions and analyses. Using this dataset, we\nevaluate state-of-the-art long-context models in zero-shot and fine-tuning\nsettings, utilizing both retrieval-based and hierarchical processing for\nbook-length inputs. Our findings show that retrieval-based approaches\noutperform hierarchical ones in both tasks. Additionally, fine-tuned models\nusing coreference-based retrieval produce the most factual descriptions, as\nmeasured by fact- and entailment-based metrics. We hope our dataset,\nexperiments, and analysis will inspire further research in character-based\nnarrative understanding.\n","authors":["Argyrios Papoudakis","Mirella Lapata","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2410.10372v1.pdf","comment":"30 pages, 2 figures, EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.10367v1","updated":"2024-10-14T10:46:32Z","published":"2024-10-14T10:46:32Z","title":"A Hybrid Filtering for Micro-video Hashtag Recommendation using\n  Graph-based Deep Neural Network","summary":"  Due to the growing volume of user generated content, hashtags are employed as\ntopic indicators to manage content efficiently on social media platforms.\nHowever, finding these vital topics is challenging in microvideos since they\ncontain substantial information in a short duration. Existing methods that\nrecommend hashtags for microvideos primarily focus on content and\npersonalization while disregarding relatedness among users. Moreover, the cold\nstart user issue prevails in hashtag recommendation systems. Considering the\nabove, we propose a hybrid filtering based MIcro-video haSHtag recommendatiON\nMISHON technique to recommend hashtags for micro-videos. Besides content based\nfiltering, we employ user-based collaborative filtering to enhance\nrecommendations. Since hashtags reflect users topical interests, we find\nsimilar users based on historical tagging behavior to model user relatedness.\nWe employ a graph-based deep neural network to model user to user, modality to\nmodality, and user to modality interactions. We then use refined modality\nspecific and user representations to recommend pertinent hashtags for\nmicrovideos. The empirical results on three real world datasets demonstrate\nthat MISHON attains a comparative enhancement of 3.6, 2.8, and 6.5 reported in\npercentage concerning the F1 score, respectively. Since cold start users exist\nwhose historical tagging information is unavailable, we also propose a content\nand social influence based technique to model the relatedness of cold start\nusers with influential users. The proposed solution shows a relative\nimprovement of 15.8 percent in the F1 score over its content only counterpart.\nThese results show that the proposed framework mitigates the cold start user\nproblem.\n","authors":["Shubhi Bansal","Kushaan Gowda","Mohammad Zia Ur Rehman","Chandravardhan Singh Raghaw","Nagendra Kumar"],"pdf_url":"https://arxiv.org/pdf/2410.10367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10360v1","updated":"2024-10-14T10:26:57Z","published":"2024-10-14T10:26:57Z","title":"Parenting: Optimizing Knowledge Selection of Retrieval-Augmented\n  Language Models with Parameter Decoupling and Tailored Tuning","summary":"  Retrieval-Augmented Generation (RAG) offers an effective solution to the\nissues faced by Large Language Models (LLMs) in hallucination generation and\nknowledge obsolescence by incorporating externally retrieved knowledge.\nHowever, due to potential conflicts between internal and external knowledge, as\nwell as retrieval noise, LLMs often struggle to effectively integrate external\nevidence, leading to a decline in performance. Although existing methods\nattempt to tackle these challenges, they often struggle to strike a balance\nbetween model adherence and robustness, resulting in significant learning\nvariance. Inspired by human cognitive processes, we propose Parenting, a novel\nframework that decouples adherence and robustness within the parameter space of\nLLMs. Specifically, Parenting utilizes a key parameter mining method based on\nforward activation gain to identify and isolate the crucial parameter units\nthat are strongly linked to adherence and robustness. Then, Parenting employs a\ntype-guided tailored tuning strategy, applying specific and appropriate\nfine-tuning methods to parameter units representing different capabilities,\naiming to achieve a balanced enhancement of adherence and robustness. Extensive\nexperiments on various datasets and models validate the effectiveness and\ngeneralizability of our methods.\n","authors":["Yongxin Xu","Ruizhe Zhang","Xinke Jiang","Yujie Feng","Yuzhen Xiao","Xinyu Ma","Runchuan Zhu","Xu Chu","Junfeng Zhao","Yasha Wang"],"pdf_url":"https://arxiv.org/pdf/2410.10360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10296v1","updated":"2024-10-14T08:49:11Z","published":"2024-10-14T08:49:11Z","title":"Enhancing Attributed Graph Networks with Alignment and Uniformity\n  Constraints for Session-based Recommendation","summary":"  Session-based Recommendation (SBR), seeking to predict a user's next action\nbased on an anonymous session, has drawn increasing attention for its\npracticability. Most SBR models only rely on the contextual transitions within\na short session to learn item representations while neglecting additional\nvaluable knowledge. As such, their model capacity is largely limited by the\ndata sparsity issue caused by short sessions. A few studies have exploited the\nModeling of Item Attributes (MIA) to enrich item representations. However, they\nusually involve specific model designs that can hardly transfer to existing\nattribute-agnostic SBR models and thus lack universality. In this paper, we\npropose a model-agnostic framework, named AttrGAU (Attributed Graph Networks\nwith Alignment and Uniformity Constraints), to bring the MIA's superiority into\nexisting attribute-agnostic models, to improve their accuracy and robustness\nfor recommendation. Specifically, we first build a bipartite attributed graph\nand design an attribute-aware graph convolution to exploit the rich attribute\nsemantics hidden in the heterogeneous item-attribute relationship. We then\ndecouple existing attribute-agnostic SBR models into the graph neural network\nand attention readout sub-modules to satisfy the non-intrusive requirement.\nLastly, we design two representation constraints, i.e., alignment and\nuniformity, to optimize distribution discrepancy in representation between the\nattribute semantics and collaborative semantics. Extensive experiments on three\npublic benchmark datasets demonstrate that the proposed AttrGAU framework can\nsignificantly enhance backbone models' recommendation performance and\nrobustness against data sparsity and data noise issues. Our implementation\ncodes will be available at https://github.com/ItsukiFujii/AttrGAU.\n","authors":["Xinping Zhao","Chaochao Chen","Jiajie Su","Yizhao Zhang","Baotian Hu"],"pdf_url":"https://arxiv.org/pdf/2410.10296v1.pdf","comment":"11 pages, 4 figures, 5 tables. Accepted by ICWS 2024"},{"id":"http://arxiv.org/abs/2410.10293v1","updated":"2024-10-14T08:47:21Z","published":"2024-10-14T08:47:21Z","title":"FunnelRAG: A Coarse-to-Fine Progressive Retrieval Paradigm for RAG","summary":"  Retrieval-Augmented Generation (RAG) prevails in Large Language Models. It\nmainly consists of retrieval and generation. The retrieval modules (a.k.a.\nretrievers) aim to find useful information used to facilitate generation\nmodules (a.k.a. generators). As such, generators' performance largely depends\non the effectiveness and efficiency of retrievers. However, the retrieval\nparadigm that we design and use remains flat, which treats the retrieval\nprocedures as a one-off deal with constant granularity. Despite effectiveness,\nwe argue that they suffer from two limitations: (1) flat retrieval exerts a\nsignificant burden on one retriever; (2) constant granularity limits the\nceiling of retrieval performance. In this work, we propose a progressive\nretrieval paradigm with coarse-to-fine granularity for RAG, termed FunnelRAG,\nso as to balance effectiveness and efficiency. Specifically, FunnelRAG\nestablishes a progressive retrieval pipeline by collaborating coarse-to-fine\ngranularity, large-to-small quantity, and low-to-high capacity, which can\nrelieve the burden on one retriever and also promote the ceiling of retrieval\nperformance. Extensive experiments manifest that FunnelRAG achieves comparable\nretrieval performance while the time overhead is reduced by nearly 40 percent.\n","authors":["Xinping Zhao","Yan Zhong","Zetian Sun","Xinshuo Hu","Zhenyu Liu","Dongfang Li","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.10293v1.pdf","comment":"18 pages, 6 figures, 13 tables"},{"id":"http://arxiv.org/abs/2410.10286v1","updated":"2024-10-14T08:38:29Z","published":"2024-10-14T08:38:29Z","title":"Back-of-the-Book Index Automation for Arabic Documents","summary":"  Back-of-the-book indexes are crucial for book readability. Their manual\ncreation is laborious and error prone. In this paper, we consider automating\nback-of-the-book index extraction for Arabic books to help simplify both the\ncreation and review tasks. Given a back-of-the-book index, we aim to check and\nidentify the accurate occurrences of index terms relative to the associated\npages. To achieve this, we first define a pool of candidates for each term by\nextracting all possible noun phrases from paragraphs appearing on the relevant\nindex pages. These noun phrases, identified through part-of-speech analysis,\nare stored in a vector database for efficient retrieval. We use several\nmetrics, including exact matches, lexical similarity, and semantic similarity,\nto determine the most appropriate occurrence. The candidate with the highest\nscore based on these metrics is chosen as the occurrence of the term. We\nfine-tuned a heuristic method, that considers the above metrics and that\nachieves an F1-score of .966 (precision=.966, recall=.966). These excellent\nresults open the door for future work related to automation of back-of-the-book\nindex generation and checking.\n","authors":["Nawal Haidar","Fadi A. Zaraket"],"pdf_url":"https://arxiv.org/pdf/2410.10286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08478v2","updated":"2024-10-14T07:55:16Z","published":"2024-10-11T03:10:09Z","title":"Personalized Item Representations in Federated Multimodal Recommendation","summary":"  Federated recommendation systems are essential for providing personalized\nrecommendations while protecting user privacy. However, current methods mainly\nrely on ID-based item embeddings, neglecting the rich multimodal information of\nitems. To address this, we propose a Federated Multimodal Recommendation\nSystem, called FedMR. FedMR uses a foundation model on the server to encode\nmultimodal item data, such as images and text. To handle data heterogeneity\ncaused by user preference differences, FedMR introduces a Mixing Feature Fusion\nModule on each client, which adjusts fusion strategy weights based on user\ninteraction history to generate personalized item representations that capture\nusers' fine-grained preferences. FedMR is compatible with existing ID-based\nfederated recommendation systems, improving performance without modifying the\noriginal framework. Experiments on four real-world multimodal datasets\ndemonstrate FedMR's effectiveness. The code is available at\nhttps://anonymous.4open.science/r/FedMR.\n","authors":["Zhiwei Li","Guodong Long","Jing Jiang","Chengqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.08478v2.pdf","comment":"12 pages, 4 figures, 5 tables, conference"},{"id":"http://arxiv.org/abs/2410.10130v1","updated":"2024-10-14T03:37:47Z","published":"2024-10-14T03:37:47Z","title":"DecKG: Decentralized Collaborative Learning with Knowledge Graph\n  Enhancement for POI Recommendation","summary":"  Decentralized collaborative learning for Point-of-Interest (POI)\nrecommendation has gained research interest due to its advantages in privacy\npreservation and efficiency, as it keeps data locally and leverages\ncollaborative learning among clients to train models in a decentralized manner.\nHowever, since local data is often limited and insufficient for training\naccurate models, a common solution is integrating external knowledge as\nauxiliary information to enhance model performance. Nevertheless, this solution\nposes challenges for decentralized collaborative learning. Due to private\nnature of local data, identifying relevant auxiliary information specific to\neach user is non-trivial. Furthermore, resource-constrained local devices\nstruggle to accommodate all auxiliary information, which places heavy burden on\nlocal storage. To fill the gap, we propose a novel decentralized collaborative\nlearning with knowledge graph enhancement framework for POI recommendation\n(DecKG). Instead of directly uploading interacted items, users generate\ndesensitized check-in data by uploading general categories of interacted items\nand sampling similar items from same category. The server then pretrains KG\nwithout sensitive user-item interactions and deploys relevant partitioned\nsub-KGs to individual users. Entities are further refined on the device,\nallowing client to client communication to exchange knowledge learned from\nlocal data and sub-KGs. Evaluations across two real-world datasets demonstrate\nDecKG's effectiveness recommendation performance.\n","authors":["Ruiqi Zheng","Liang Qu","Guanhua Ye","Tong Chen","Yuhui Shi","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2410.10130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10127v1","updated":"2024-10-14T03:26:51Z","published":"2024-10-14T03:26:51Z","title":"MAIR: A Massive Benchmark for Evaluating Instructed Retrieval","summary":"  Recent information retrieval (IR) models are pre-trained and\ninstruction-tuned on massive datasets and tasks, enabling them to perform well\non a wide range of tasks and potentially generalize to unseen tasks with\ninstructions. However, existing IR benchmarks focus on a limited scope of\ntasks, making them insufficient for evaluating the latest IR models. In this\npaper, we propose MAIR (Massive Instructed Retrieval Benchmark), a\nheterogeneous IR benchmark that includes 126 distinct IR tasks across 6\ndomains, collected from existing datasets. We benchmark state-of-the-art\ninstruction-tuned text embedding models and re-ranking models. Our experiments\nreveal that instruction-tuned models generally achieve superior performance\ncompared to non-instruction-tuned models on MAIR. Additionally, our results\nsuggest that current instruction-tuned text embedding models and re-ranking\nmodels still lack effectiveness in specific long-tail tasks. MAIR is publicly\navailable at https://github.com/sunnweiwei/Mair.\n","authors":["Weiwei Sun","Zhengliang Shi","Jiulong Wu","Lingyong Yan","Xinyu Ma","Yiding Liu","Min Cao","Dawei Yin","Zhaochun Ren"],"pdf_url":"https://arxiv.org/pdf/2410.10127v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2403.16504v3","updated":"2024-10-14T03:26:10Z","published":"2024-03-25T07:38:40Z","title":"LARA: Linguistic-Adaptive Retrieval-Augmentation for Multi-Turn Intent\n  Classification","summary":"  Multi-turn intent classification is notably challenging due to the complexity\nand evolving nature of conversational contexts. This paper introduces LARA, a\nLinguistic-Adaptive Retrieval-Augmentation framework to enhance accuracy in\nmulti-turn classification tasks across six languages, accommodating a large\nnumber of intents in chatbot interactions. LARA combines a fine-tuned smaller\nmodel with a retrieval-augmented mechanism, integrated within the architecture\nof LLMs. The integration allows LARA to dynamically utilize past dialogues and\nrelevant intents, thereby improving the understanding of the context.\nFurthermore, our adaptive retrieval techniques bolster the cross-lingual\ncapabilities of LLMs without extensive retraining and fine-tuning.\nComprehensive experiments demonstrate that LARA achieves state-of-the-art\nperformance on multi-turn intent classification tasks, enhancing the average\naccuracy by 3.67\\% from state-of-the-art single-turn intent classifiers.\n","authors":["Junhua Liu","Yong Keat Tan","Bin Fu","Kwan Hui Lim"],"pdf_url":"https://arxiv.org/pdf/2403.16504v3.pdf","comment":"Accepted to EMNLP'24"},{"id":"http://arxiv.org/abs/2409.02864v2","updated":"2024-10-14T01:07:54Z","published":"2024-09-04T16:43:14Z","title":"Language Model Powered Digital Biology","summary":"  Recent advancements in Large Language Models (LLMs) are transforming biology,\ncomputer science, and many other research fields, as well as impacting everyday\nlife. While transformer-based technologies are currently being deployed in\nbiology, no available agentic system has been developed to tackle\nbioinformatics workflows. We present a prototype Bioinformatics Retrieval\nAugmented Data (BRAD) digital assistant. BRAD is a chatbot and agentic system\nthat integrates a suite of tools to handle bioinformatics tasks, from code\nexecution to online search. We demonstrate its capabilities through (1)\nimproved question-and-answering with retrieval augmented generation (RAG), (2)\nthe ability to run complex software pipelines, and (3) the ability to organize\nand distribute tasks in agentic workflows. We use BRAD for automation,\nperforming tasks ranging from gene enrichment and searching the archive to\nautomatic code generation for running biomarker identification pipelines. BRAD\nis a step toward autonomous, self-driving labs for digital biology.\n","authors":["Joshua Pickard","Marc Andrew Choi","Natalie Oliven","Cooper Stansbury","Jillian Cwycyshyn","Nicholas Galioto","Alex Gorodetsky","Alvaro Velasquez","Indika Rajapakse"],"pdf_url":"https://arxiv.org/pdf/2409.02864v2.pdf","comment":"49 pages, 3 tables, 12 figures"},{"id":"http://arxiv.org/abs/2310.16605v4","updated":"2024-10-14T00:45:35Z","published":"2023-10-25T12:50:34Z","title":"Enhancing Dense Retrievers' Robustness with Group-level Reweighting","summary":"  The anchor-document data derived from web graphs offers a wealth of paired\ninformation for training dense retrieval models in an unsupervised manner.\nHowever, unsupervised data contains diverse patterns across the web graph and\noften exhibits significant imbalance, leading to suboptimal performance in\nunderrepresented or difficult groups. In this paper, we introduce WebDRO, an\nefficient approach for clustering the web graph data and optimizing group\nweights to enhance the robustness of dense retrieval models. Initially, we\nbuild an embedding model for clustering anchor-document pairs. Specifically, we\ncontrastively train the embedding model for link prediction, which guides the\nembedding model in capturing the document features behind the web graph links.\nSubsequently, we employ the group distributional robust optimization to\nrecalibrate the weights across different clusters of anchor-document pairs\nduring training retrieval models. During training, we direct the model to\nassign higher weights to clusters with higher loss and focus more on worst-case\nscenarios. This approach ensures that the model has strong generalization\nability on all data patterns. Our experiments on MS MARCO and BEIR demonstrate\nthat our method can effectively improve retrieval performance in unsupervised\ntraining and finetuning settings. Further analysis confirms the stability and\nvalidity of group weights learned by WebDRO. The code of this paper can be\nobtained from https://github.com/Hanpx20/GroupDRO_Dense_Retrieval.\n","authors":["Peixuan Han","Zhenghao Liu","Zhiyuan Liu","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2310.16605v4.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.06753v3","updated":"2024-10-14T16:06:54Z","published":"2024-08-13T09:19:59Z","title":"Detecting Audio-Visual Deepfakes with Fine-Grained Inconsistencies","summary":"  Existing methods on audio-visual deepfake detection mainly focus on\nhigh-level features for modeling inconsistencies between audio and visual data.\nAs a result, these approaches usually overlook finer audio-visual artifacts,\nwhich are inherent to deepfakes. Herein, we propose the introduction of\nfine-grained mechanisms for detecting subtle artifacts in both spatial and\ntemporal domains. First, we introduce a local audio-visual model capable of\ncapturing small spatial regions that are prone to inconsistencies with audio.\nFor that purpose, a fine-grained mechanism based on a spatially-local distance\ncoupled with an attention module is adopted. Second, we introduce a\ntemporally-local pseudo-fake augmentation to include samples incorporating\nsubtle temporal inconsistencies in our training set. Experiments on the DFDC\nand the FakeAVCeleb datasets demonstrate the superiority of the proposed method\nin terms of generalization as compared to the state-of-the-art under both\nin-dataset and cross-dataset settings.\n","authors":["Marcella Astrid","Enjie Ghorbel","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2408.06753v3.pdf","comment":"Accepted in BMVC 2024"},{"id":"http://arxiv.org/abs/2409.02845v2","updated":"2024-10-14T14:55:13Z","published":"2024-09-04T16:17:41Z","title":"Multi-Track MusicLDM: Towards Versatile Music Generation with Latent\n  Diffusion Model","summary":"  Diffusion models have shown promising results in cross-modal generation tasks\ninvolving audio and music, such as text-to-sound and text-to-music generation.\nThese text-controlled music generation models typically focus on generating\nmusic by capturing global musical attributes like genre and mood. However,\nmusic composition is a complex, multilayered task that often involves musical\narrangement as an integral part of the process. This process involves composing\neach instrument to align with existing ones in terms of beat, dynamics,\nharmony, and melody, requiring greater precision and control over tracks than\ntext prompts usually provide. In this work, we address these challenges by\nextending the MusicLDM, a latent diffusion model for music, into a multi-track\ngenerative model. By learning the joint probability of tracks sharing a\ncontext, our model is capable of generating music across several tracks that\ncorrespond well to each other, either conditionally or unconditionally.\nAdditionally, our model is capable of arrangement generation, where the model\ncan generate any subset of tracks given the others (e.g., generating a piano\ntrack complementing given bass and drum tracks). We compared our model with an\nexisting multi-track generative model and demonstrated that our model achieves\nconsiderable improvements across objective metrics for both total and\narrangement generation tasks.\n","authors":["Tornike Karchkhadze","Mohammad Rasool Izadi","Ke Chen","Gerard Assayag","Shlomo Dubnov"],"pdf_url":"https://arxiv.org/pdf/2409.02845v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14485v7","updated":"2024-10-14T14:26:04Z","published":"2024-06-20T16:48:14Z","title":"Proceedings of The second international workshop on eXplainable AI for\n  the Arts (XAIxArts)","summary":"  This second international workshop on explainable AI for the Arts (XAIxArts)\nbrought together a community of researchers in HCI, Interaction Design, AI,\nexplainable AI (XAI), and digital arts to explore the role of XAI for the Arts.\nWorkshop held at the 16th ACM Conference on Creativity and Cognition (C&C\n2024), Chicago, USA.\n","authors":["Nick Bryan-Kinns","Corey Ford","Shuoyang Zheng","Helen Kennedy","Alan Chamberlain","Makayla Lewis","Drew Hemment","Zijin Li","Qiong Wu","Lanxi Xiao","Gus Xia","Jeba Rezwana","Michael Clemens","Gabriel Vigliensoni"],"pdf_url":"https://arxiv.org/pdf/2406.14485v7.pdf","comment":"Proceedings of The second international workshop on eXplainable AI\n  for the Arts (XAIxArts)"},{"id":"http://arxiv.org/abs/2410.10319v1","updated":"2024-10-14T09:25:09Z","published":"2024-10-14T09:25:09Z","title":"Spatial-Aware Efficient Projector for MLLMs via Multi-Layer Feature\n  Aggregation","summary":"  The projector plays a crucial role in multi-modal language models (MLLMs).\nThe number of visual tokens it outputs affects the efficiency of the MLLM,\nwhile the quality of the visual tokens influences the visual understanding\ncapabilities of the MLLM. Current explorations on the projector focus on\nreducing the number of visual tokens to improve efficiency, often overlooking\nthe inherent spatial discrepancy between the serialized 2-dimensional visual\ntoken sequences and natural language token sequences. A Spatial-Aware Efficient\nProjector (SAEP) is proposed to address this issue. In detail, our SAEP method\nemploys an modified separable depthwise convolution module on multi-layer\nvisual features to enhance the spatial information of visual tokens. As a\nresult, our SAEP method can not only largely reduce the number of visual tokens\nby 75\\%, but also significantly improve the multimodal spatial understanding\ncapability of MLLMs. Moreover, compared to existing projectors, our SAEP gets\nbest performances on massive multimodal evaluation benchmarks, which denotes\nits effectiveness on bridging the modality gap.\n","authors":["Shun Qian","Bingquan Liu","Chengjie Sun","Zhen Xu","Baoxun Wang"],"pdf_url":"https://arxiv.org/pdf/2410.10319v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.10291v1","updated":"2024-10-14T08:45:35Z","published":"2024-10-14T08:45:35Z","title":"Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal\n  Perspective","summary":"  Accurate interpretation and visualization of human instructions are crucial\nfor text-to-image (T2I) synthesis. However, current models struggle to capture\nsemantic variations from word order changes, and existing evaluations, relying\non indirect metrics like text-image similarity, fail to reliably assess these\nchallenges. This often obscures poor performance on complex or uncommon\nlinguistic patterns by the focus on frequent word combinations. To address\nthese deficiencies, we propose a novel metric called SemVarEffect and a\nbenchmark named SemVarBench, designed to evaluate the causality between\nsemantic variations in inputs and outputs in T2I synthesis. Semantic variations\nare achieved through two types of linguistic permutations, while avoiding\neasily predictable literal variations. Experiments reveal that the\nCogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.\nSemantic variations in object relations are less understood than attributes,\nscoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in\nUNet or Transformers plays a crucial role in handling semantic variations, a\nfactor previously overlooked by a focus on textual encoders. Our work\nestablishes an effective evaluation framework that advances the T2I synthesis\ncommunity's exploration of human instruction understanding.\n","authors":["Xiangru Zhu","Penglei Sun","Yaoxian Song","Yanghua Xiao","Zhixu Li","Chengyu Wang","Jun Huang","Bei Yang","Xiaoxiao Xu"],"pdf_url":"https://arxiv.org/pdf/2410.10291v1.pdf","comment":"Our benchmark and code are available at\n  https://github.com/zhuxiangru/SemVarBench"},{"id":"http://arxiv.org/abs/2405.07930v2","updated":"2024-10-14T08:19:13Z","published":"2024-05-13T17:01:28Z","title":"Improving Multimodal Learning with Multi-Loss Gradient Modulation","summary":"  Learning from multiple modalities, such as audio and video, offers\nopportunities for leveraging complementary information, enhancing robustness,\nand improving contextual understanding and performance. However, combining such\nmodalities presents challenges, especially when modalities differ in data\nstructure, predictive contribution, and the complexity of their learning\nprocesses. It has been observed that one modality can potentially dominate the\nlearning process, hindering the effective utilization of information from other\nmodalities and leading to sub-optimal model performance. To address this issue\nthe vast majority of previous works suggest to assess the unimodal\ncontributions and dynamically adjust the training to equalize them. We improve\nupon previous work by introducing a multi-loss objective and further refining\nthe balancing process, allowing it to dynamically adjust the learning pace of\neach modality in both directions, acceleration and deceleration, with the\nability to phase out balancing effects upon convergence. We achieve superior\nresults across three audio-video datasets: on CREMA-D, models with ResNet\nbackbone encoders surpass the previous best by 1.9% to 12.4%, and Conformer\nbackbone models deliver improvements ranging from 2.8% to 14.1% across\ndifferent fusion methods. On AVE, improvements range from 2.7% to 7.7%, while\non UCF101, gains reach up to 6.1%.\n","authors":["Konstantinos Kontras","Christos Chatzichristos","Matthew Blaschko","Maarten De Vos"],"pdf_url":"https://arxiv.org/pdf/2405.07930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10178v1","updated":"2024-10-14T05:51:53Z","published":"2024-10-14T05:51:53Z","title":"GUISE: Graph GaUssIan Shading watErmark","summary":"  In the expanding field of generative artificial intelligence, integrating\nrobust watermarking technologies is essential to protect intellectual property\nand maintain content authenticity. Traditionally, watermarking techniques have\nbeen developed primarily for rich information media such as images and audio.\nHowever, these methods have not been adequately adapted for graph-based data,\nparticularly molecular graphs. Latent 3D graph diffusion(LDM-3DG) is an\nascendant approach in the molecular graph generation field. This model\neffectively manages the complexities of molecular structures, preserving\nessential symmetries and topological features. We adapt the Gaussian Shading, a\nproven performance lossless watermarking technique, to the latent graph\ndiffusion domain to protect this sophisticated new technology. Our adaptation\nsimplifies the watermark diffusion process through duplication and padding,\nmaking it adaptable and suitable for various message types. We conduct several\nexperiments using the LDM-3DG model on publicly available datasets QM9 and\nDrugs, to assess the robustness and effectiveness of our technique. Our results\ndemonstrate that the watermarked molecules maintain statistical parity in 9 out\nof 10 performance metrics compared to the original. Moreover, they exhibit a\n100% detection rate and a 99% extraction rate in a 2D decoded pipeline, while\nalso showing robustness against post-editing attacks.\n","authors":["Renyi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.10178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13711v2","updated":"2024-10-14T01:43:38Z","published":"2024-08-25T02:56:26Z","title":"SceneDreamer360: Text-Driven 3D-Consistent Scene Generation with\n  Panoramic Gaussian Splatting","summary":"  Text-driven 3D scene generation has seen significant advancements recently.\nHowever, most existing methods generate single-view images using generative\nmodels and then stitch them together in 3D space. This independent generation\nfor each view often results in spatial inconsistency and implausibility in the\n3D scenes. To address this challenge, we proposed a novel text-driven\n3D-consistent scene generation model: SceneDreamer360. Our proposed method\nleverages a text-driven panoramic image generation model as a prior for 3D\nscene generation and employs 3D Gaussian Splatting (3DGS) to ensure consistency\nacross multi-view panoramic images. Specifically, SceneDreamer360 enhances the\nfine-tuned Panfusion generator with a three-stage panoramic enhancement,\nenabling the generation of high-resolution, detail-rich panoramic images.\nDuring the 3D scene construction, a novel point cloud fusion initialization\nmethod is used, producing higher quality and spatially consistent point clouds.\nOur extensive experiments demonstrate that compared to other methods,\nSceneDreamer360 with its panoramic image generation and 3DGS can produce higher\nquality, spatially consistent, and visually appealing 3D scenes from any text\nprompt. Our codes are available at\n\\url{https://github.com/liwrui/SceneDreamer360}.\n","authors":["Wenrui Li","Fucheng Cai","Yapeng Mi","Zhe Yang","Wangmeng Zuo","Xingtao Wang","Xiaopeng Fan"],"pdf_url":"https://arxiv.org/pdf/2408.13711v2.pdf","comment":null}]},"2024-10-13T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.09999v1","updated":"2024-10-13T20:45:00Z","published":"2024-10-13T20:45:00Z","title":"Leveraging Customer Feedback for Multi-modal Insight Extraction","summary":"  Businesses can benefit from customer feedback in different modalities, such\nas text and images, to enhance their products and services. However, it is\ndifficult to extract actionable and relevant pairs of text segments and images\nfrom customer feedback in a single pass. In this paper, we propose a novel\nmulti-modal method that fuses image and text information in a latent space and\ndecodes it to extract the relevant feedback segments using an image-text\ngrounded text decoder. We also introduce a weakly-supervised data generation\ntechnique that produces training data for this task. We evaluate our model on\nunseen data and demonstrate that it can effectively mine actionable insights\nfrom multi-modal customer feedback, outperforming the existing baselines by\n$14$ points in F1 score.\n","authors":["Sandeep Sricharan Mukku","Abinesh Kanagarajan","Pushpendu Ghosh","Chetan Aggarwal"],"pdf_url":"https://arxiv.org/pdf/2410.09999v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2410.09942v1","updated":"2024-10-13T17:53:50Z","published":"2024-10-13T17:53:50Z","title":"Learning to Rank for Multiple Retrieval-Augmented Models through\n  Iterative Utility Maximization","summary":"  This paper investigates the design of a unified search engine to serve\nmultiple retrieval-augmented generation (RAG) agents, each with a distinct\ntask, backbone large language model (LLM), and retrieval-augmentation strategy.\nWe introduce an iterative approach where the search engine generates retrieval\nresults for these RAG agents and gathers feedback on the quality of the\nretrieved documents during an offline phase. This feedback is then used to\niteratively optimize the search engine using a novel expectation-maximization\nalgorithm, with the goal of maximizing each agent's utility function.\nAdditionally, we adapt this approach to an online setting, allowing the search\nengine to refine its behavior based on real-time individual agents feedback to\nbetter serve the results for each of them. Experiments on diverse datasets from\nthe Knowledge-Intensive Language Tasks (KILT) benchmark demonstrates that our\napproach significantly on average outperforms competitive baselines across 18\nRAG models. We also demonstrate that our method effectively ``personalizes''\nthe retrieval process for each RAG agent based on the collected feedback.\nFinally, we provide a comprehensive ablation study to explore various aspects\nof our method.\n","authors":["Alireza Salemi","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2410.09942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09936v1","updated":"2024-10-13T17:44:04Z","published":"2024-10-13T17:44:04Z","title":"The Role of Fake Users in Sequential Recommender Systems","summary":"  Sequential Recommender Systems (SRSs) are widely used to model user behavior\nover time, yet their robustness remains an under-explored area of research. In\nthis paper, we conduct an empirical study to assess how the presence of fake\nusers, who engage in random interactions, follow popular or unpopular items, or\nfocus on a single genre, impacts the performance of SRSs in real-world\nscenarios. We evaluate two SRS models across multiple datasets, using\nestablished metrics such as Normalized Discounted Cumulative Gain (NDCG) and\nRank Sensitivity List (RLS) to measure performance. While traditional metrics\nlike NDCG remain relatively stable, our findings reveal that the presence of\nfake users severely degrades RLS metrics, often reducing them to near-zero\nvalues. These results highlight the need for further investigation into the\neffects of fake users on training data and emphasize the importance of\ndeveloping more resilient SRSs that can withstand different types of\nadversarial attacks.\n","authors":["Filippo Betello"],"pdf_url":"https://arxiv.org/pdf/2410.09936v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.09923v1","updated":"2024-10-13T17:08:16Z","published":"2024-10-13T17:08:16Z","title":"Analysis and Design of a Personalized Recommendation System Based on a\n  Dynamic User Interest Model","summary":"  With the rapid development of the internet and the explosion of information,\nproviding users with accurate personalized recommendations has become an\nimportant research topic. This paper designs and analyzes a personalized\nrecommendation system based on a dynamic user interest model. The system\ncaptures user behavior data, constructs a dynamic user interest model, and\ncombines multiple recommendation algorithms to provide personalized content to\nusers. The research results show that this system significantly improves\nrecommendation accuracy and user satisfaction. This paper discusses the\nsystem's architecture design, algorithm implementation, and experimental\nresults in detail and explores future research directions.\n","authors":["Chunyan Mao","Shuaishuai Huang","Mingxiu Sui","Haowei Yang","Xueshe Wang"],"pdf_url":"https://arxiv.org/pdf/2410.09923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09875v1","updated":"2024-10-13T15:34:11Z","published":"2024-10-13T15:34:11Z","title":"ViFi-ReID: A Two-Stream Vision-WiFi Multimodal Approach for Person\n  Re-identification","summary":"  Person re-identification(ReID), as a crucial technology in the field of\nsecurity, plays a vital role in safety inspections, personnel counting, and\nmore. Most current ReID approaches primarily extract features from images,\nwhich are easily affected by objective conditions such as clothing changes and\nocclusions. In addition to cameras, we leverage widely available routers as\nsensing devices by capturing gait information from pedestrians through the\nChannel State Information (CSI) in WiFi signals and contribute a multimodal\ndataset. We employ a two-stream network to separately process video\nunderstanding and signal analysis tasks, and conduct multi-modal fusion and\ncontrastive learning on pedestrian video and WiFi data. Extensive experiments\nin real-world scenarios demonstrate that our method effectively uncovers the\ncorrelations between heterogeneous data, bridges the gap between visual and\nsignal modalities, significantly expands the sensing range, and improves ReID\naccuracy across multiple sensors.\n","authors":["Chen Mao","Chong Tan","Jingqi Hu","Min Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.09875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10808v2","updated":"2024-10-13T15:33:52Z","published":"2024-08-20T12:58:16Z","title":"ColBERT Retrieval and Ensemble Response Scoring for Language Model\n  Question Answering","summary":"  Domain-specific question answering remains challenging for language models,\ngiven the deep technical knowledge required to answer questions correctly. This\ndifficulty is amplified for smaller language models that cannot encode as much\ninformation in their parameters as larger models. The \"Specializing Large\nLanguage Models for Telecom Networks\" challenge aimed to enhance the\nperformance of two small language models, Phi-2 and Falcon-7B in\ntelecommunication question answering. In this paper, we present our question\nanswering systems for this challenge. Our solutions achieved leading marks of\n81.9% accuracy for Phi-2 and 57.3% for Falcon-7B. We have publicly released our\ncode and fine-tuned models.\n","authors":["Alex Gichamba","Tewodros Kederalah Idris","Brian Ebiyau","Eric Nyberg","Teruko Mitamura"],"pdf_url":"https://arxiv.org/pdf/2408.10808v2.pdf","comment":"7 pages, 2 figures, and 8 tables. This paper has been accepted at the\n  2024 IEEE Global Communications (GLOBECOM) Workshops"},{"id":"http://arxiv.org/abs/2406.16048v2","updated":"2024-10-13T15:30:32Z","published":"2024-06-23T08:24:08Z","title":"Evaluating D-MERIT of Partial-annotation on Information Retrieval","summary":"  Retrieval models are often evaluated on partially-annotated datasets. Each\nquery is mapped to a few relevant texts and the remaining corpus is assumed to\nbe irrelevant. As a result, models that successfully retrieve false negatives\nare punished in evaluation. Unfortunately, completely annotating all texts for\nevery query is not resource efficient. In this work, we show that using\npartially-annotated datasets in evaluation can paint a distorted picture. We\ncurate D-MERIT, a passage retrieval evaluation set from Wikipedia, aspiring to\ncontain all relevant passages for each query. Queries describe a group (e.g.,\n\"journals about linguistics\") and relevant passages are evidence that entities\nbelong to the group (e.g., a passage indicating that \"Language\" is a journal\nabout linguistics). We show that evaluating on a dataset containing annotations\nfor only a subset of the relevant passages might result in misleading ranking\nof the retrieval systems and that as more relevant texts are included in the\nevaluation set, the rankings converge. We propose our dataset as a resource for\nevaluation and our study as a recommendation for balance between\nresource-efficiency and reliable evaluation when annotating evaluation sets for\ntext retrieval.\n","authors":["Royi Rassin","Yaron Fairstein","Oren Kalinsky","Guy Kushilevitz","Nachshon Cohen","Alexander Libov","Yoav Goldberg"],"pdf_url":"https://arxiv.org/pdf/2406.16048v2.pdf","comment":"Accepted to EMNLP 2024 main track. Our dataset can be downloaded from\n  https://D-MERIT.github.io"},{"id":"http://arxiv.org/abs/2410.09871v1","updated":"2024-10-13T15:11:31Z","published":"2024-10-13T15:11:31Z","title":"A Comparative Study of PDF Parsing Tools Across Diverse Document\n  Categories","summary":"  PDF is one of the most prominent data formats, making PDF parsing crucial for\ninformation extraction and retrieval, particularly with the rise of RAG\nsystems. While various PDF parsing tools exist, their effectiveness across\ndifferent document types remains understudied, especially beyond academic\npapers. Our research aims to address this gap by comparing 10 popular PDF\nparsing tools across 6 document categories using the DocLayNet dataset. These\ntools include PyPDF, pdfminer.six, PyMuPDF, pdfplumber, pypdfium2,\nUnstructured, Tabula, Camelot, as well as the deep learning-based tools Nougat\nand Table Transformer(TATR). We evaluated both text extraction and table\ndetection capabilities. For text extraction, PyMuPDF and pypdfium generally\noutperformed others, but all parsers struggled with Scientific and Patent\ndocuments. For these challenging categories, learning-based tools like Nougat\ndemonstrated superior performance. In table detection, TATR excelled in the\nFinancial, Patent, Law & Regulations, and Scientific categories. Table\ndetection tool Camelot performed best for tender documents, while PyMuPDF\nperformed superior in the Manual category. Our findings highlight the\nimportance of selecting appropriate parsing tools based on document type and\nspecific tasks, providing valuable insights for researchers and practitioners\nworking with diverse document sources.\n","authors":["Narayan S. Adhikari","Shradha Agarwal"],"pdf_url":"https://arxiv.org/pdf/2410.09871v1.pdf","comment":"17 pages,11 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.09829v1","updated":"2024-10-13T13:07:31Z","published":"2024-10-13T13:07:31Z","title":"Generating Driving Simulations via Conversation","summary":"  Cyber-physical systems like autonomous vehicles are tested in simulation\nbefore deployment, using domain-specific programs for scenario specification.\nTo aid the testing of autonomous vehicles in simulation, we design a natural\nlanguage interface, using an instruction-following large language model, to\nassist a non-coding domain expert in synthesising the desired scenarios and\nvehicle behaviours. We show that using it to convert utterances to the symbolic\nprogram is feasible, despite the very small training dataset. Human experiments\nshow that dialogue is critical to successful simulation generation, leading to\na 4.5 times higher success rate than a generation without engaging in extended\nconversation.\n","authors":["Rimvydas Rubavicius","Antonio Valerio Miceli-Barone","Alex Lascarides","Subramanian Ramamoorthy"],"pdf_url":"https://arxiv.org/pdf/2410.09829v1.pdf","comment":"6 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.09781v1","updated":"2024-10-13T08:58:54Z","published":"2024-10-13T08:58:54Z","title":"ContextWIN: Whittle Index Based Mixture-of-Experts Neural Model For\n  Restless Bandits Via Deep RL","summary":"  This study introduces ContextWIN, a novel architecture that extends the\nNeural Whittle Index Network (NeurWIN) model to address Restless Multi-Armed\nBandit (RMAB) problems with a context-aware approach. By integrating a mixture\nof experts within a reinforcement learning framework, ContextWIN adeptly\nutilizes contextual information to inform decision-making in dynamic\nenvironments, particularly in recommendation systems. A key innovation is the\nmodel's ability to assign context-specific weights to a subset of NeurWIN\nnetworks, thus enhancing the efficiency and accuracy of the Whittle index\ncomputation for each arm. The paper presents a thorough exploration of\nContextWIN, from its conceptual foundation to its implementation and potential\napplications. We delve into the complexities of RMABs and the significance of\nincorporating context, highlighting how ContextWIN effectively harnesses these\nelements. The convergence of both the NeurWIN and ContextWIN models is\nrigorously proven, ensuring theoretical robustness. This work lays the\ngroundwork for future advancements in applying contextual information to\ncomplex decision-making scenarios, recognizing the need for comprehensive\ndataset exploration and environment development for full potential realization.\n","authors":["Zhanqiu Guo","Wayne Wang"],"pdf_url":"https://arxiv.org/pdf/2410.09781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09761v1","updated":"2024-10-13T07:38:44Z","published":"2024-10-13T07:38:44Z","title":"ChartKG: A Knowledge-Graph-Based Representation for Chart Images","summary":"  Chart images, such as bar charts, pie charts, and line charts, are\nexplosively produced due to the wide usage of data visualizations. Accordingly,\nknowledge mining from chart images is becoming increasingly important, which\ncan benefit downstream tasks like chart retrieval and knowledge graph\ncompletion. However, existing methods for chart knowledge mining mainly focus\non converting chart images into raw data and often ignore their visual\nencodings and semantic meanings, which can result in information loss for many\ndownstream tasks. In this paper, we propose ChartKG, a novel knowledge graph\n(KG) based representation for chart images, which can model the visual elements\nin a chart image and semantic relations among them including visual encodings\nand visual insights in a unified manner. Further, we develop a general\nframework to convert chart images to the proposed KG-based representation. It\nintegrates a series of image processing techniques to identify visual elements\nand relations, e.g., CNNs to classify charts, yolov5 and optical character\nrecognition to parse charts, and rule-based methods to construct graphs. We\npresent four cases to illustrate how our knowledge-graph-based representation\ncan model the detailed visual elements and semantic relations in charts, and\nfurther demonstrate how our approach can benefit downstream applications such\nas semantic-aware chart retrieval and chart question answering. We also conduct\nquantitative evaluations to assess the two fundamental building blocks of our\nchart-to-KG framework, i.e., object recognition and optical character\nrecognition. The results provide support for the usefulness and effectiveness\nof ChartKG.\n","authors":["Zhiguang Zhou","Haoxuan Wang","Zhengqing Zhao","Fengling Zheng","Yongheng Wang","Wei Chen","Yong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.09761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11890v1","updated":"2024-10-13T07:20:47Z","published":"2024-10-13T07:20:47Z","title":"Online Digital Investigative Journalism using SociaLens","summary":"  Media companies witnessed a significant transformation with the rise of the\ninternet, bigdata, machine learning (ML) and AI. Recent emergence of large\nlanguage models (LLM) have added another aspect to this transformation.\nResearchers believe that with the help of these technologies, investigative\ndigital journalism will enter a new era. Using a smart set of data gathering\nand analysis tools, journalists will be able to create data driven contents and\ninsights in unprecedented ways. In this paper, we introduce a versatile and\nautonomous investigative journalism tool, called {\\em SociaLens}, for\nidentifying and extracting query specific data from online sources, responding\nto probing queries and drawing conclusions entailed by large volumes of data\nusing ML analytics fully autonomously. We envision its use in investigative\njournalism, law enforcement and social policy planning. The proposed system\ncapitalizes on the integration of ML technology with LLMs and advanced bigdata\nsearch techniques. We illustrate the functionality of SociaLens using a focused\ncase study on rape incidents in a developing country and demonstrate that\njournalists can gain nuanced insights without requiring coding expertise they\nmight lack. SociaLens is designed as a ChatBot that is capable of contextual\nconversation, find and collect data relevant to queries, initiate ML tasks to\nrespond to queries, generate textual and visual reports, all fully autonomously\nwithin the ChatBot environment.\n","authors":["Hasan M. Jamil","Sajratul Y. Rubaiat"],"pdf_url":"https://arxiv.org/pdf/2410.11890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04408v2","updated":"2024-10-13T07:05:51Z","published":"2024-01-09T08:04:11Z","title":"Fine-Grained Embedding Dimension Optimization During Training for\n  Recommender Systems","summary":"  Huge embedding tables in modern deep learning recommender models (DLRM)\nrequire prohibitively large memory during training and inference. This paper\nproposes FIITED, a system to automatically reduce the memory footprint via\nFIne-grained In-Training Embedding Dimension pruning. By leveraging the key\ninsight that embedding vectors are not equally important, FIITED adaptively\nadjusts the dimension of each individual embedding vector during model\ntraining, assigning larger dimensions to more important embeddings while\nadapting to dynamic changes in data. We prioritize embedding dimensions with\nhigher frequencies and gradients as more important. To enable efficient pruning\nof embeddings and their dimensions during model training, we propose an\nembedding storage system based on virtually-hashed physically-indexed hash\ntables. Experiments on two industry models and months of realistic datasets\nshow that FIITED can reduce DLRM embedding size by more than 65% while\npreserving model quality, outperforming state-of-the-art in-training embedding\npruning methods. On public datasets, FIITED can reduce the size of embedding\ntables by 2.1x to 800x with negligible accuracy drop, while improving model\nthroughput.\n","authors":["Qinyi Luo","Penghan Wang","Wei Zhang","Fan Lai","Jiachen Mao","Xiaohan Wei","Jun Song","Wei-Yu Tsai","Shuai Yang","Yuxi Hu","Xuehai Qian"],"pdf_url":"https://arxiv.org/pdf/2401.04408v2.pdf","comment":"12 pages, 15 figures"},{"id":"http://arxiv.org/abs/2310.08891v2","updated":"2024-10-13T04:49:45Z","published":"2023-10-13T06:53:02Z","title":"EHI: End-to-end Learning of Hierarchical Index for Efficient Dense\n  Retrieval","summary":"  Dense embedding-based retrieval is widely used for semantic search and\nranking. However, conventional two-stage approaches, involving contrastive\nembedding learning followed by approximate nearest neighbor search (ANNS), can\nsuffer from misalignment between these stages. This mismatch degrades retrieval\nperformance. We propose End-to-end Hierarchical Indexing (EHI), a novel method\nthat directly addresses this issue by jointly optimizing embedding generation\nand ANNS structure. EHI leverages a dual encoder for embedding queries and\ndocuments while simultaneously learning an inverted file index (IVF)-style tree\nstructure. To facilitate the effective learning of this discrete structure, EHI\nintroduces dense path embeddings that encodes the path traversed by queries and\ndocuments within the tree. Extensive evaluations on standard benchmarks,\nincluding MS MARCO (Dev set) and TREC DL19, demonstrate EHI's superiority over\ntraditional ANNS index. Under the same computational constraints, EHI\noutperforms existing state-of-the-art methods by +1.45% in MRR@10 on MS MARCO\n(Dev) and +8.2% in nDCG@10 on TREC DL19, highlighting the benefits of our\nend-to-end approach.\n","authors":["Ramnath Kumar","Anshul Mittal","Nilesh Gupta","Aditya Kusupati","Inderjit Dhillon","Prateek Jain"],"pdf_url":"https://arxiv.org/pdf/2310.08891v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09713v1","updated":"2024-10-13T03:45:24Z","published":"2024-10-13T03:45:24Z","title":"Agentic Information Retrieval","summary":"  What will information entry look like in the next generation of digital\nproducts? Since the 1970s, user access to relevant information has relied on\ndomain-specific architectures of information retrieval (IR). Over the past two\ndecades, the advent of modern IR systems, including web search engines and\npersonalized recommender systems, has greatly improved the efficiency of\nretrieving relevant information from vast data corpora. However, the core\nparadigm of these IR systems remains largely unchanged, relying on filtering a\npredefined set of candidate items. Since 2022, breakthroughs in large language\nmodels (LLMs) have begun transforming how information is accessed, establishing\na new technical paradigm. In this position paper, we introduce Agentic\nInformation Retrieval (Agentic IR), a novel IR paradigm shaped by the\ncapabilities of LLM agents. Agentic IR expands the scope of accessible tasks\nand leverages a suite of new techniques to redefine information retrieval. We\ndiscuss three types of cutting-edge applications of agentic IR and the\nchallenges faced. We propose that agentic IR holds promise for generating\ninnovative applications, potentially becoming a central information entry point\nin future digital ecosystems.\n","authors":["Weinan Zhang","Junwei Liao","Ning Li","Kounianhua Du"],"pdf_url":"https://arxiv.org/pdf/2410.09713v1.pdf","comment":"11 pages, position paper"}],"Multimedia":[{"id":"http://arxiv.org/abs/2406.04321v2","updated":"2024-10-13T17:59:22Z","published":"2024-06-06T17:58:11Z","title":"VidMuse: A Simple Video-to-Music Generation Framework with\n  Long-Short-Term Modeling","summary":"  In this work, we systematically study music generation conditioned solely on\nthe video. First, we present a large-scale dataset comprising 360K video-music\npairs, including various genres such as movie trailers, advertisements, and\ndocumentaries. Furthermore, we propose VidMuse, a simple framework for\ngenerating music aligned with video inputs. VidMuse stands out by producing\nhigh-fidelity music that is both acoustically and semantically aligned with the\nvideo. By incorporating local and global visual cues, VidMuse enables the\ncreation of musically coherent audio tracks that consistently match the video\ncontent through Long-Short-Term modeling. Through extensive experiments,\nVidMuse outperforms existing models in terms of audio quality, diversity, and\naudio-visual alignment. The code and datasets will be available at\nhttps://github.com/ZeyueT/VidMuse/.\n","authors":["Zeyue Tian","Zhaoyang Liu","Ruibin Yuan","Jiahao Pan","Qifeng Liu","Xu Tan","Qifeng Chen","Wei Xue","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2406.04321v2.pdf","comment":"The code and datasets will be available at\n  https://github.com/ZeyueT/VidMuse/"},{"id":"http://arxiv.org/abs/2410.09872v1","updated":"2024-10-13T15:13:00Z","published":"2024-10-13T15:13:00Z","title":"Towards Reproducible Learning-based Compression","summary":"  A deep learning system typically suffers from a lack of reproducibility that\nis partially rooted in hardware or software implementation details. The\nirreproducibility leads to skepticism in deep learning technologies and it can\nhinder them from being deployed in many applications. In this work, the\nirreproducibility issue is analyzed where deep learning is employed in\ncompression systems while the encoding and decoding may be run on devices from\ndifferent manufacturers. The decoding process can even crash due to a single\nbit difference, e.g., in a learning-based entropy coder. For a given deep\nlearning-based module with limited resources for protection, we first suggest\nthat reproducibility can only be assured when the mismatches are bounded. Then\na safeguarding mechanism is proposed to tackle the challenges. The proposed\nmethod may be applied for different levels of protection either at the\nreconstruction level or at a selected decoding level. Furthermore, the overhead\nintroduced for the protection can be scaled down accordingly when the error\nbound is being suppressed. Experiments demonstrate the effectiveness of the\nproposed approach for learning-based compression systems, e.g., in image\ncompression and point cloud compression.\n","authors":["Jiahao Pang","Muhammad Asad Lodhi","Junghyun Ahn","Yuning Huang","Dong Tian"],"pdf_url":"https://arxiv.org/pdf/2410.09872v1.pdf","comment":"Accepted at MMSP 2024"},{"id":"http://arxiv.org/abs/2405.13984v2","updated":"2024-10-13T14:24:49Z","published":"2024-05-22T20:40:53Z","title":"Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption\n  Generation and Fine-Grained NLI Evaluation","summary":"  Scientific language models drive research innovation but require extensive\nfine-tuning on large datasets. This work enhances such models by improving\ntheir inference and evaluation capabilities with minimal or no additional\ntraining. Focusing on molecule caption generation, we explore synergies between\nalignment fine-tuning and model merging in a cross-modal setup. We reveal\nintriguing insights into the behaviour and suitability of such methods while\nsignificantly surpassing state-of-the-art models. Moreover, we propose a novel\natomic-level evaluation method leveraging off-the-shelf Natural Language\nInference (NLI) models for use in the unseen chemical domain. Our experiments\ndemonstrate that our evaluation operates at the right level of granularity,\neffectively handling multiple content units and subsentence reasoning, while\nwidely adopted NLI methods consistently misalign with assessment criteria.\n","authors":["Dimitris Gkoumas","Maria Liakata"],"pdf_url":"https://arxiv.org/pdf/2405.13984v2.pdf","comment":null}]},"2024-10-12T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2311.16207v2","updated":"2024-10-12T22:21:41Z","published":"2023-11-27T15:34:14Z","title":"ALNSynergy: a graph convolutional network with multi-representation\n  alignment for drug synergy prediction","summary":"  Drug combination refers to the use of two or more drugs to treat a specific\ndisease at the same time. It is currently the mainstream way to treat complex\ndiseases. Compared with single drugs, drug combinations have better efficacy\nand can better inhibit toxicity and drug resistance. The computational model\nbased on deep learning concatenates the representation of multiple drugs and\nthe corresponding cell line feature as input, and the output is whether the\ndrug combination can have an inhibitory effect on the cell line. However, this\nstrategy of concatenating multiple representations has the following defects:\nthe alignment of drug representation and cell line representation is ignored,\nresulting in the synergistic relationship not being reflected positionally in\nthe embedding space. Moreover, the alignment measurement function in deep\nlearning cannot be suitable for drug synergy prediction tasks due to\ndifferences in input types. Therefore, in this work, we propose ALNSynergy, a\ngraph convolutional network with multi-representation alignment for predicting\ndrug synergy. In the ALNSynergy model, we designed a multi-representation\nalignment function suitable for the drug synergy prediction task so that the\npositional relationship between drug representations and cell line\nrepresentation is reflected in the embedding space. In addition, the vector\nmodulus of drug representations and cell line representation is considered to\nimprove the accuracy of calculation results and accelerate model convergence.\nFinally, many relevant experiments were run on multiple drug synergy datasets\nto verify the effectiveness of the above innovative elements and the excellence\nof the ALNSynergy model.\n","authors":["Xinxing Yang","Jiachen Li","Xiao Kang","Guojin Pei","Keyu Liu","Genke Yang","Jian Chu"],"pdf_url":"https://arxiv.org/pdf/2311.16207v2.pdf","comment":"9 pages;"},{"id":"http://arxiv.org/abs/2410.09629v1","updated":"2024-10-12T19:38:09Z","published":"2024-10-12T19:38:09Z","title":"Synthetic Knowledge Ingestion: Towards Knowledge Refinement and\n  Injection for Enhancing Large Language Models","summary":"  Large language models (LLMs) are proficient in capturing factual knowledge\nacross various domains. However, refining their capabilities on previously seen\nknowledge or integrating new knowledge from external sources remains a\nsignificant challenge. In this work, we propose a novel synthetic knowledge\ningestion method called Ski, which leverages fine-grained synthesis,\ninterleaved generation, and assemble augmentation strategies to construct\nhigh-quality data representations from raw knowledge sources. We then integrate\nSki and its variations with three knowledge injection techniques: Retrieval\nAugmented Generation (RAG), Supervised Fine-tuning (SFT), and Continual\nPre-training (CPT) to inject and refine knowledge in language models. Extensive\nempirical experiments are conducted on various question-answering tasks\nspanning finance, biomedicine, and open-generation domains to demonstrate that\nSki significantly outperforms baseline methods by facilitating effective\nknowledge injection. We believe that our work is an important step towards\nenhancing the factual accuracy of LLM outputs by refining knowledge\nrepresentation and injection capabilities.\n","authors":["Jiaxin Zhang","Wendi Cui","Yiran Huang","Kamalika Das","Sricharan Kumar"],"pdf_url":"https://arxiv.org/pdf/2410.09629v1.pdf","comment":"EMNLP 2024 main conference long paper"},{"id":"http://arxiv.org/abs/2403.17901v2","updated":"2024-10-12T17:41:12Z","published":"2024-03-26T17:43:08Z","title":"Search and Society: Reimagining Information Access for Radical Futures","summary":"  Information retrieval (IR) technologies and research are undergoing\ntransformative changes. It is our perspective that the community should accept\nthis opportunity to re-center our research agendas on societal needs while\ndismantling the artificial separation between the work on fairness,\naccountability, transparency, and ethics in IR and the rest of IR research.\nInstead of adopting a reactionary strategy of trying to mitigate potential\nsocial harms from emerging technologies, the community should aim to\nproactively set the research agenda for the kinds of systems we should build\ninspired by diverse explicitly stated sociotechnical imaginaries. The\nsociotechnical imaginaries that underpin the design and development of\ninformation access technologies needs to be explicitly articulated, and we need\nto develop theories of change in context of these diverse perspectives. Our\nguiding future imaginaries must be informed by other academic fields, such as\nsocial and political sciences, and should be co-developed with\ncross-disciplinary scholars, legal and policy experts, civil rights and social\njustice activists, and artists, among others. In this perspective paper, we\nmotivate why the community must consider this radical shift in how we do\nresearch and what we work on, and sketch a path forward towards this\ntransformation.\n","authors":["Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2403.17901v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09584v1","updated":"2024-10-12T16:30:51Z","published":"2024-10-12T16:30:51Z","title":"Toward General Instruction-Following Alignment for Retrieval-Augmented\n  Generation","summary":"  Following natural instructions is crucial for the effective application of\nRetrieval-Augmented Generation (RAG) systems. Despite recent advancements in\nLarge Language Models (LLMs), research on assessing and improving\ninstruction-following (IF) alignment within the RAG domain remains limited. To\naddress this issue, we propose VIF-RAG, the first automated, scalable, and\nverifiable synthetic pipeline for instruction-following alignment in RAG\nsystems. We start by manually crafting a minimal set of atomic instructions\n(<100) and developing combination rules to synthesize and verify complex\ninstructions for a seed set. We then use supervised models for instruction\nrewriting while simultaneously generating code to automate the verification of\ninstruction quality via a Python executor. Finally, we integrate these\ninstructions with extensive RAG and general data samples, scaling up to a\nhigh-quality VIF-RAG-QA dataset (>100k) through automated processes. To further\nbridge the gap in instruction-following auto-evaluation for RAG systems, we\nintroduce FollowRAG Benchmark, which includes approximately 3K test samples,\ncovering 22 categories of general instruction constraints and four\nknowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG\ncan seamlessly integrate with different RAG benchmarks. Using FollowRAG and\neight widely-used IF and foundational abilities benchmarks for LLMs, we\ndemonstrate that VIF-RAG markedly enhances LLM performance across a broad range\nof general instruction constraints while effectively leveraging its\ncapabilities in RAG scenarios. Further analysis offers practical insights for\nachieving IF alignment in RAG systems. Our code and datasets are released at\nhttps://FollowRAG.github.io.\n","authors":["Guanting Dong","Xiaoshuai Song","Yutao Zhu","Runqi Qiao","Zhicheng Dou","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2410.09584v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2402.03025v2","updated":"2024-10-12T16:28:32Z","published":"2024-02-05T14:06:15Z","title":"Understanding and Guiding Weakly Supervised Entity Alignment with\n  Potential Isomorphism Propagation","summary":"  Weakly Supervised Entity Alignment (EA) is the task of identifying equivalent\nentities across diverse knowledge graphs (KGs) using only a limited number of\nseed alignments. Despite substantial advances in aggregation-based weakly\nsupervised EA, the underlying mechanisms in this setting remain unexplored. In\nthis paper, we present a propagation perspective to analyze weakly supervised\nEA and explain the existing aggregation-based EA models. Our theoretical\nanalysis reveals that these models essentially seek propagation operators for\npairwise entity similarities. We further prove that, despite the structural\nheterogeneity of different KGs, the potentially aligned entities within\naggregation-based EA models have isomorphic subgraphs, which is the core\npremise of EA but has not been investigated. Leveraging this insight, we\nintroduce a potential isomorphism propagation operator to enhance the\npropagation of neighborhood information across KGs. We develop a general EA\nframework, PipEA, incorporating this operator to improve the accuracy of every\ntype of aggregation-based model without altering the learning process.\nExtensive experiments substantiate our theoretical findings and demonstrate\nPipEA's significant performance gains over state-of-the-art weakly supervised\nEA methods. Our work not only advances the field but also enhances our\ncomprehension of aggregation-based weakly supervised EA.\n","authors":["Yuanyi Wang","Wei Tang","Haifeng Sun","Zirui Zhuang","Xiaoyuan Fu","Jingyu Wang","Qi Qi","Jianxin Liao"],"pdf_url":"https://arxiv.org/pdf/2402.03025v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05763v2","updated":"2024-10-12T15:14:41Z","published":"2024-10-08T07:41:01Z","title":"Information Discovery in e-Commerce","summary":"  Electronic commerce, or e-commerce, is the buying and selling of goods and\nservices, or the transmitting of funds or data online. E-commerce platforms\ncome in many kinds, with global players such as Amazon, Airbnb, Alibaba, eBay\nand platforms targeting specific geographic regions. Information retrieval has\na natural role to play in e-commerce, especially in connecting people to goods\nand services. Information discovery in e-commerce concerns different types of\nsearch (e.g., exploratory search vs. lookup tasks), recommender systems, and\nnatural language processing in e-commerce portals. The rise in popularity of\ne-commerce sites has made research on information discovery in e-commerce an\nincreasingly active research area. This is witnessed by an increase in\npublications and dedicated workshops in this space. Methods for information\ndiscovery in e-commerce largely focus on improving the effectiveness of\ne-commerce search and recommender systems, on enriching and using knowledge\ngraphs to support e-commerce, and on developing innovative question answering\nand bot-based solutions that help to connect people to goods and services. In\nthis survey, an overview is given of the fundamental infrastructure,\nalgorithms, and technical solutions for information discovery in e-commerce.\nThe topics covered include user behavior and profiling, search, recommendation,\nand language technology in e-commerce.\n","authors":["Zhaochun Ren","Xiangnan He","Dawei Yin","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2410.05763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09560v1","updated":"2024-10-12T15:10:56Z","published":"2024-10-12T15:10:56Z","title":"Towards Scalable Semantic Representation for Recommendation","summary":"  With recent advances in large language models (LLMs), there has been emerging\nnumbers of research in developing Semantic IDs based on LLMs to enhance the\nperformance of recommendation systems. However, the dimension of these\nembeddings needs to match that of the ID embedding in recommendation, which is\nusually much smaller than the original length. Such dimension compression\nresults in inevitable losses in discriminability and dimension robustness of\nthe LLM embeddings, which motivates us to scale up the semantic representation.\nIn this paper, we propose Mixture-of-Codes, which first constructs multiple\nindependent codebooks for LLM representation in the indexing stage, and then\nutilizes the Semantic Representation along with a fusion module for the\ndownstream recommendation stage. Extensive analysis and experiments demonstrate\nthat our method achieves superior discriminability and dimension robustness\nscalability, leading to the best scale-up performance in recommendations.\n","authors":["Taolin Zhang","Junwei Pan","Jinpeng Wang","Yaohua Zha","Tao Dai","Bin Chen","Ruisheng Luo","Xiaoxiang Deng","Yuan Wang","Ming Yue","Jie Jiang","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2410.09560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09514v1","updated":"2024-10-12T12:26:04Z","published":"2024-10-12T12:26:04Z","title":"Eco-Aware Graph Neural Networks for Sustainable Recommendations","summary":"  Recommender systems play a crucial role in alleviating information overload\nby providing personalized recommendations tailored to users' preferences and\ninterests. Recently, Graph Neural Networks (GNNs) have emerged as a promising\napproach for recommender systems, leveraging their ability to effectively\ncapture complex relationships and dependencies between users and items by\nrepresenting them as nodes in a graph structure. In this study, we investigate\nthe environmental impact of GNN-based recommender systems, an aspect that has\nbeen largely overlooked in the literature. Specifically, we conduct a\ncomprehensive analysis of the carbon emissions associated with training and\ndeploying GNN models for recommendation tasks. We evaluate the energy\nconsumption and carbon footprint of different GNN architectures and\nconfigurations, considering factors such as model complexity, training\nduration, hardware specifications and embedding size. By addressing the\nenvironmental impact of resource-intensive algorithms in recommender systems,\nthis study contributes to the ongoing efforts towards sustainable and\nresponsible artificial intelligence, promoting the development of eco-friendly\nrecommendation technologies that balance performance and environmental\nconsiderations. Code is available at:\nhttps://github.com/antoniopurificato/gnn_recommendation_and_environment.\n","authors":["Antonio Purificato","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2410.09514v1.pdf","comment":"9 pages, 2 tables, 3 figures, RecSoGood Workshop"},{"id":"http://arxiv.org/abs/2407.13284v4","updated":"2024-10-12T08:17:41Z","published":"2024-07-18T08:36:28Z","title":"Semantic-aware Representation Learning for Homography Estimation","summary":"  Homography estimation is the task of determining the transformation from an\nimage pair. Our approach focuses on employing detector-free feature matching\nmethods to address this issue. Previous work has underscored the importance of\nincorporating semantic information, however there still lacks an efficient way\nto utilize semantic information. Previous methods suffer from treating the\nsemantics as a pre-processing, causing the utilization of semantics overly\ncoarse-grained and lack adaptability when dealing with different tasks. In our\nwork, we seek another way to use the semantic information, that is\nsemantic-aware feature representation learning framework.Based on this, we\npropose SRMatcher, a new detector-free feature matching method, which\nencourages the network to learn integrated semantic feature\nrepresentation.Specifically, to capture precise and rich semantics, we leverage\nthe capabilities of recently popularized vision foundation models (VFMs)\ntrained on extensive datasets. Then, a cross-images Semantic-aware Fusion Block\n(SFB) is proposed to integrate its fine-grained semantic features into the\nfeature representation space. In this way, by reducing errors stemming from\nsemantic inconsistencies in matching pairs, our proposed SRMatcher is able to\ndeliver more accurate and realistic outcomes. Extensive experiments show that\nSRMatcher surpasses solid baselines and attains SOTA results on multiple\nreal-world datasets. Compared to the previous SOTA approach GeoFormer,\nSRMatcher increases the area under the cumulative curve (AUC) by about 11% on\nHPatches. Additionally, the SRMatcher could serve as a plug-and-play framework\nfor other matching methods like LoFTR, yielding substantial precision\nimprovement.\n","authors":["Yuhan Liu","Qianxin Huang","Siqi Hui","Jingwen Fu","Sanping Zhou","Kangyi Wu","Pengna Li","Jinjun Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13284v4.pdf","comment":"Accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2410.07786v2","updated":"2024-10-12T05:40:20Z","published":"2024-10-10T10:17:54Z","title":"Orthogonal Nonnegative Matrix Factorization with the Kullback-Leibler\n  divergence","summary":"  Orthogonal nonnegative matrix factorization (ONMF) has become a standard\napproach for clustering. As far as we know, most works on ONMF rely on the\nFrobenius norm to assess the quality of the approximation. This paper presents\na new model and algorithm for ONMF that minimizes the Kullback-Leibler (KL)\ndivergence. As opposed to the Frobenius norm which assumes Gaussian noise, the\nKL divergence is the maximum likelihood estimator for Poisson-distributed data,\nwhich can model better sparse vectors of word counts in document data sets and\nphoto counting processes in imaging. We develop an algorithm based on\nalternating optimization, KL-ONMF, and show that it performs favorably with the\nFrobenius-norm based ONMF for document classification and hyperspectral image\nunmixing.\n","authors":["Jean Pacifique Nkurunziza","Fulgence Nahayo","Nicolas Gillis"],"pdf_url":"https://arxiv.org/pdf/2410.07786v2.pdf","comment":"10 pages, corrected some typos"},{"id":"http://arxiv.org/abs/2410.09359v1","updated":"2024-10-12T04:00:55Z","published":"2024-10-12T04:00:55Z","title":"Green Recommender Systems: Optimizing Dataset Size for Energy-Efficient\n  Algorithm Performance","summary":"  As recommender systems become increasingly prevalent, the environmental\nimpact and energy efficiency of training large-scale models have come under\nscrutiny. This paper investigates the potential for energy-efficient algorithm\nperformance by optimizing dataset sizes through downsampling techniques in the\ncontext of Green Recommender Systems. We conducted experiments on the MovieLens\n100K, 1M, 10M, and Amazon Toys and Games datasets, analyzing the performance of\nvarious recommender algorithms under different portions of dataset size. Our\nresults indicate that while more training data generally leads to higher\nalgorithm performance, certain algorithms, such as FunkSVD and BiasedMF,\nparticularly with unbalanced and sparse datasets like Amazon Toys and Games,\nmaintain high-quality recommendations with up to a 50% reduction in training\ndata, achieving nDCG@10 scores within approximately 13% of full dataset\nperformance. These findings suggest that strategic dataset reduction can\ndecrease computational and environmental costs without substantially\ncompromising recommendation quality. This study advances sustainable and green\nrecommender systems by providing insights for reducing energy consumption while\nmaintaining effectiveness.\n","authors":["Ardalan Arabzadeh","Tobias Vente","Joeran Beel"],"pdf_url":"https://arxiv.org/pdf/2410.09359v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.09670v1","updated":"2024-10-12T23:35:45Z","published":"2024-10-12T23:35:45Z","title":"Making Beshbarmak: Games for Central Asian Cultural Heritage","summary":"  This paper introduces \"Making Beshbarmak\", an interactive cooking game that\ncelebrates the nomadic ancestry and cultural heritage of Central Asian\ncommunities worldwide. Designed to promote cultural appreciation and identity\nformation, the game invites players to learn and recreate the traditional dish\nBeshbarmak through an engaging step-by-step process, incorporating storytelling\nelements that explain the cultural significance of the meal. Our project\ncontributes to digital cultural heritage and games research by offering an\naccessible, open-source prototype on p5.js, enabling users to connect with and\nexplore Central Asian traditions. \"Making Beshbarmak\" serves as both an\neducational tool and a platform for cultural preservation, fostering a sense of\nbelonging among Central Asian immigrant populations.\n","authors":["Amina Kobenova","Adina Kaiymova"],"pdf_url":"https://arxiv.org/pdf/2410.09670v1.pdf","comment":"5 pages, 2 figures, EAI ArtsIT Conference"},{"id":"http://arxiv.org/abs/2410.00557v2","updated":"2024-10-12T10:40:40Z","published":"2024-10-01T10:10:43Z","title":"STanH : Parametric Quantization for Variable Rate Learned Image\n  Compression","summary":"  In end-to-end learned image compression, encoder and decoder are jointly\ntrained to minimize a $R + {\\lambda}D$ cost function, where ${\\lambda}$\ncontrols the trade-off between rate of the quantized latent representation and\nimage quality. Unfortunately, a distinct encoder-decoder pair with millions of\nparameters must be trained for each ${\\lambda}$, hence the need to switch\nencoders and to store multiple encoders and decoders on the user device for\nevery target rate. This paper proposes to exploit a differentiable quantizer\ndesigned around a parametric sum of hyperbolic tangents, called STanH , that\nrelaxes the step-wise quantization function. STanH is implemented as a\ndifferentiable activation layer with learnable quantization parameters that can\nbe plugged into a pre-trained fixed rate model and refined to achieve different\ntarget bitrates. Experimental results show that our method enables variable\nrate coding with comparable efficiency to the state-of-the-art, yet with\nsignificant savings in terms of ease of deployment, training time, and storage\ncosts\n","authors":["Alberto Presta","Enzo Tartaglione","Attilio Fiandrotti","Marco Grangetto"],"pdf_url":"https://arxiv.org/pdf/2410.00557v2.pdf","comment":"Submitted to IEEE Transactions on Image Processing"}]},"2024-10-11T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.09141v1","updated":"2024-10-11T17:57:06Z","published":"2024-10-11T17:57:06Z","title":"ACER: Automatic Language Model Context Extension via Retrieval","summary":"  Long-context modeling is one of the critical capabilities of language AI for\ndigesting and reasoning over complex information pieces. In practice,\nlong-context capabilities are typically built into a pre-trained language\nmodel~(LM) through a carefully designed context extension stage, with the goal\nof producing generalist long-context capabilities. In our preliminary\nexperiments, however, we discovered that the current open-weight generalist\nlong-context models are still lacking in practical long-context processing\ntasks. While this means perfectly effective long-context modeling demands\ntask-specific data, the cost can be prohibitive. In this paper, we draw\ninspiration from how humans process a large body of information: a lossy\n\\textbf{retrieval} stage ranks a large set of documents while the reader ends\nup reading deeply only the top candidates. We build an \\textbf{automatic} data\nsynthesis pipeline that mimics this process using short-context LMs. The\nshort-context LMs are further tuned using these self-generated data to obtain\ntask-specific long-context capabilities. Similar to how pre-training learns\nfrom imperfect data, we hypothesize and further demonstrate that the\nshort-context model can bootstrap over the synthetic data, outperforming not\nonly long-context generalist models but also the retrieval and read pipeline\nused to synthesize the training data in real-world tasks such as long-context\nretrieval augmented generation.\n","authors":["Luyu Gao","Yunyi Zhang","Jamie Callan"],"pdf_url":"https://arxiv.org/pdf/2410.09141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11557v4","updated":"2024-10-11T15:13:51Z","published":"2024-08-21T12:09:37Z","title":"A Quick, trustworthy spectral knowledge Q&A system leveraging\n  retrieval-augmented generation on LLM","summary":"  Large Language Model (LLM) has demonstrated significant success in a range of\nnatural language processing (NLP) tasks within general domain. The emergence of\nLLM has introduced innovative methodologies across diverse fields, including\nthe natural sciences. Researchers aim to implement automated, concurrent\nprocess driven by LLM to supplant conventional manual, repetitive and\nlabor-intensive work. In the domain of spectral analysis and detection, it is\nimperative for researchers to autonomously acquire pertinent knowledge across\nvarious research objects, which encompasses the spectroscopic techniques and\nthe chemometric methods that are employed in experiments and analysis.\nParadoxically, despite the recognition of spectroscopic detection as an\neffective analytical method, the fundamental process of knowledge retrieval\nremains both time-intensive and repetitive. In response to this challenge, we\nfirst introduced the Spectral Detection and Analysis Based Paper(SDAAP)\ndataset, which is the first open-source textual knowledge dataset for spectral\nanalysis and detection and contains annotated literature data as well as\ncorresponding knowledge instruction data. Subsequently, we also designed an\nautomated Q\\&A framework based on the SDAAP dataset, which can retrieve\nrelevant knowledge and generate high-quality responses by extracting entities\nin the input as retrieval parameters. It is worth noting that: within this\nframework, LLM is only used as a tool to provide generalizability, while RAG\ntechnique is used to accurately capture the source of the knowledge.This\napproach not only improves the quality of the generated responses, but also\nensures the traceability of the knowledge. Experimental results show that our\nframework generates responses with more reliable expertise compared to the\nbaseline.\n","authors":["Jiheng Liang","Ziru Yu","Zujie Xie","Xiangyang Yu"],"pdf_url":"https://arxiv.org/pdf/2408.11557v4.pdf","comment":"16 pages,10 figures,3 tables"},{"id":"http://arxiv.org/abs/2410.08877v1","updated":"2024-10-11T14:54:08Z","published":"2024-10-11T14:54:08Z","title":"Interdependency Matters: Graph Alignment for Multivariate Time Series\n  Anomaly Detection","summary":"  Anomaly detection in multivariate time series (MTS) is crucial for various\napplications in data mining and industry. Current industrial methods typically\napproach anomaly detection as an unsupervised learning task, aiming to identify\ndeviations by estimating the normal distribution in noisy, label-free datasets.\nThese methods increasingly incorporate interdependencies between channels\nthrough graph structures to enhance accuracy. However, the role of\ninterdependencies is more critical than previously understood, as shifts in\ninterdependencies between MTS channels from normal to anomalous data are\nsignificant. This observation suggests that \\textit{anomalies could be detected\nby changes in these interdependency graph series}. To capitalize on this\ninsight, we introduce MADGA (MTS Anomaly Detection via Graph Alignment), which\nredefines anomaly detection as a graph alignment (GA) problem that explicitly\nutilizes interdependencies for anomaly detection. MADGA dynamically transforms\nsubsequences into graphs to capture the evolving interdependencies, and Graph\nalignment is performed between these graphs, optimizing an alignment plan that\nminimizes cost, effectively minimizing the distance for normal data and\nmaximizing it for anomalous data. Uniquely, our GA approach involves explicit\nalignment of both nodes and edges, employing Wasserstein distance for nodes and\nGromov-Wasserstein distance for edges. To our knowledge, this is the first\napplication of GA to MTS anomaly detection that explicitly leverages\ninterdependency for this purpose. Extensive experiments on diverse real-world\ndatasets validate the effectiveness of MADGA, demonstrating its capability to\ndetect anomalies and differentiate interdependencies, consistently achieving\nstate-of-the-art across various scenarios.\n","authors":["Yuanyi Wang","Haifeng Sun","Chengsen Wang","Mengde Zhu","Jingyu Wang","Wei Tang","Qi Qi","Zirui Zhuang","Jianxin Liao"],"pdf_url":"https://arxiv.org/pdf/2410.08877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08801v1","updated":"2024-10-11T13:36:13Z","published":"2024-10-11T13:36:13Z","title":"A Methodology for Evaluating RAG Systems: A Case Study On Configuration\n  Dependency Validation","summary":"  Retrieval-augmented generation (RAG) is an umbrella of different components,\ndesign decisions, and domain-specific adaptations to enhance the capabilities\nof large language models and counter their limitations regarding hallucination\nand outdated and missing knowledge. Since it is unclear which design decisions\nlead to a satisfactory performance, developing RAG systems is often\nexperimental and needs to follow a systematic and sound methodology to gain\nsound and reliable results. However, there is currently no generally accepted\nmethodology for RAG evaluation despite a growing interest in this technology.\nIn this paper, we propose a first blueprint of a methodology for a sound and\nreliable evaluation of RAG systems and demonstrate its applicability on a\nreal-world software engineering research task: the validation of configuration\ndependencies across software technologies. In summary, we make two novel\ncontributions: (i) A novel, reusable methodological design for evaluating RAG\nsystems, including a demonstration that represents a guideline, and (ii) a RAG\nsystem, which has been developed following this methodology, that achieves the\nhighest accuracy in the field of dependency validation. For the blueprint's\ndemonstration, the key insights are the crucial role of choosing appropriate\nbaselines and metrics, the necessity for systematic RAG refinements derived\nfrom qualitative failure analysis, as well as the reporting practices of key\ndesign decision to foster replication and evaluation.\n","authors":["Sebastian Simon","Alina Mailach","Johannes Dorn","Norbert Siegmund"],"pdf_url":"https://arxiv.org/pdf/2410.08801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14038v2","updated":"2024-10-11T12:40:50Z","published":"2024-09-21T06:49:34Z","title":"OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model\n  Hallucinations in Ontology Matching","summary":"  Hallucinations of large language models (LLMs) commonly occur in\ndomain-specific downstream tasks, with no exception in ontology matching (OM).\nThe prevalence of using LLMs for OM raises the need for benchmarks to better\nunderstand LLM hallucinations. The OAEI-LLM dataset is an extended version of\nthe Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate\nLLM-specific hallucinations in OM tasks. We outline the methodology used in\ndataset construction and schema extension, and provide examples of potential\nuse cases.\n","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang","Jing Jiang"],"pdf_url":"https://arxiv.org/pdf/2409.14038v2.pdf","comment":"4 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.08740v1","updated":"2024-10-11T11:59:40Z","published":"2024-10-11T11:59:40Z","title":"Hespi: A pipeline for automatically detecting information from hebarium\n  specimen sheets","summary":"  Specimen associated biodiversity data are sought after for biological,\nenvironmental, climate, and conservation sciences. A rate shift is required for\nthe extraction of data from specimen images to eliminate the bottleneck that\nthe reliance on human-mediated transcription of these data represents. We\napplied advanced computer vision techniques to develop the `Hespi' (HErbarium\nSpecimen sheet PIpeline), which extracts a pre-catalogue subset of collection\ndata on the institutional labels on herbarium specimens from their digital\nimages. The pipeline integrates two object detection models; the first detects\nbounding boxes around text-based labels and the second detects bounding boxes\naround text-based data fields on the primary institutional label. The pipeline\nclassifies text-based institutional labels as printed, typed, handwritten, or a\ncombination and applies Optical Character Recognition (OCR) and Handwritten\nText Recognition (HTR) for data extraction. The recognized text is then\ncorrected against authoritative databases of taxon names. The extracted text is\nalso corrected with the aide of a multimodal Large Language Model (LLM). Hespi\naccurately detects and extracts text for test datasets including specimen sheet\nimages from international herbaria. The components of the pipeline are modular\nand users can train their own models with their own data and use them in place\nof the models provided.\n","authors":["Robert Turnbull","Emily Fitzgerald","Karen Thompson","Joanne L. Birch"],"pdf_url":"https://arxiv.org/pdf/2410.08740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02793v2","updated":"2024-10-11T10:45:46Z","published":"2024-07-03T03:42:13Z","title":"Learning Positional Attention for Sequential Recommendation","summary":"  Self-attention-based networks have achieved remarkable performance in\nsequential recommendation tasks. A crucial component of these models is\npositional encoding. In this study, we delve into the learned positional\nembedding, demonstrating that it often captures the distance between tokens.\nBuilding on this insight, we introduce novel attention models that directly\nlearn positional relations. Extensive experiments reveal that our proposed\nmodels, \\textbf{PARec} and \\textbf{FPARec} outperform previous\nself-attention-based approaches.Our code is available at the link for anonymous\nreview: https://anonymous.4open.science/ r/FPARec-2C55/\n","authors":["Fan Luo","Haibo He","Juan Zhang","Shenghui Xu"],"pdf_url":"https://arxiv.org/pdf/2407.02793v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09128v1","updated":"2024-10-11T09:44:33Z","published":"2024-10-11T09:44:33Z","title":"TIGER: Temporally Improved Graph Entity Linker","summary":"  Knowledge graphs change over time, for example, when new entities are\nintroduced or entity descriptions change. This impacts the performance of\nentity linking, a key task in many uses of knowledge graphs such as web search\nand recommendation. Specifically, entity linking models exhibit temporal\ndegradation - their performance decreases the further a knowledge graph moves\nfrom its original state on which an entity linking model was trained. To tackle\nthis challenge, we introduce \\textbf{TIGER}: a \\textbf{T}emporally\n\\textbf{I}mproved \\textbf{G}raph \\textbf{E}ntity Linke\\textbf{r}. By\nincorporating structural information between entities into the model, we\nenhance the learned representation, making entities more distinguishable over\ntime. The core idea is to integrate graph-based information into text-based\ninformation, from which both distinct and shared embeddings are based on an\nentity's feature and structural relationships and their interaction.\nExperiments on three datasets show that our model can effectively prevent\ntemporal degradation, demonstrating a 16.24\\% performance boost over the\nstate-of-the-art in a temporal setting when the time gap is one year and an\nimprovement to 20.93\\% as the gap expands to three years. The code and data are\nmade available at\n\\url{https://github.com/pengyu-zhang/TIGER-Temporally-Improved-Graph-Entity-Linker}.\n","authors":["Pengyu Zhang","Congfeng Cao","Paul Groth"],"pdf_url":"https://arxiv.org/pdf/2410.09128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08623v1","updated":"2024-10-11T08:42:02Z","published":"2024-10-11T08:42:02Z","title":"Retrieving Contextual Information for Long-Form Question Answering using\n  Weak Supervision","summary":"  Long-form question answering (LFQA) aims at generating in-depth answers to\nend-user questions, providing relevant information beyond the direct answer.\nHowever, existing retrievers are typically optimized towards information that\ndirectly targets the question, missing out on such contextual information.\nFurthermore, there is a lack of training data for relevant context. To this\nend, we propose and compare different weak supervision techniques to optimize\nretrieval for contextual information. Experiments demonstrate improvements on\nthe end-to-end QA performance on ASQA, a dataset for long-form question\nanswering. Importantly, as more contextual information is retrieved, we improve\nthe relevant page recall for LFQA by 14.7% and the groundedness of generated\nlong-form answers by 12.5%. Finally, we show that long-form answers often\nanticipate likely follow-up questions, via experiments on a conversational QA\ndataset.\n","authors":["Philipp Christmann","Svitlana Vakulenko","Ionut Teodor Sorodoc","Bill Byrne","Adrià de Gispert"],"pdf_url":"https://arxiv.org/pdf/2410.08623v1.pdf","comment":"Accepted at EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2410.08583v1","updated":"2024-10-11T07:23:45Z","published":"2024-10-11T07:23:45Z","title":"Intent-Enhanced Data Augmentation for Sequential Recommendation","summary":"  The research on intent-enhanced sequential recommendation algorithms focuses\non how to better mine dynamic user intent based on user behavior data for\nsequential recommendation tasks. Various data augmentation methods are widely\napplied in current sequential recommendation algorithms, effectively enhancing\nthe ability to capture user intent. However, these widely used data\naugmentation methods often rely on a large amount of random sampling, which can\nintroduce excessive noise into the training data, blur user intent, and thus\nnegatively affect recommendation performance. Additionally, these methods have\nlimited approaches to utilizing augmented data, failing to fully leverage the\naugmented samples. We propose an intent-enhanced data augmentation method for\nsequential recommendation(\\textbf{IESRec}), which constructs positive and\nnegative samples based on user behavior sequences through intent-segment\ninsertion. On one hand, the generated positive samples are mixed with the\noriginal training data, and they are trained together to improve recommendation\nperformance. On the other hand, the generated positive and negative samples are\nused to build a contrastive loss function, enhancing recommendation performance\nthrough self-supervised training. Finally, the main recommendation task is\njointly trained with the contrastive learning loss minimization task.\nExperiments on three real-world datasets validate the effectiveness of our\nIESRec model.\n","authors":["Shuai Chen","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2410.08583v1.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.08521v1","updated":"2024-10-11T04:51:28Z","published":"2024-10-11T04:51:28Z","title":"Improving Legal Entity Recognition Using a Hybrid Transformer Model and\n  Semantic Filtering Approach","summary":"  Legal Entity Recognition (LER) is critical in automating legal workflows such\nas contract analysis, compliance monitoring, and litigation support. Existing\napproaches, including rule-based systems and classical machine learning models,\nstruggle with the complexity of legal documents and domain specificity,\nparticularly in handling ambiguities and nested entity structures. This paper\nproposes a novel hybrid model that enhances the accuracy and precision of\nLegal-BERT, a transformer model fine-tuned for legal text processing, by\nintroducing a semantic similarity-based filtering mechanism. We evaluate the\nmodel on a dataset of 15,000 annotated legal documents, achieving an F1 score\nof 93.4%, demonstrating significant improvements in precision and recall over\nprevious methods.\n","authors":["Duraimurugan Rajamanickam"],"pdf_url":"https://arxiv.org/pdf/2410.08521v1.pdf","comment":"7 pages, 1 table"}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.19340v5","updated":"2024-10-11T18:52:25Z","published":"2024-07-27T21:00:36Z","title":"Integrating Large Language Models into a Tri-Modal Architecture for\n  Automated Depression Classification on the DAIC-WOZ","summary":"  Major Depressive Disorder (MDD) is a pervasive mental health condition that\naffects 300 million people worldwide. This work presents a novel, BiLSTM-based\ntri-modal model-level fusion architecture for the binary classification of\ndepression from clinical interview recordings. The proposed architecture\nincorporates Mel Frequency Cepstral Coefficients, Facial Action Units, and uses\na two-shot learning based GPT-4 model to process text data. This is the first\nwork to incorporate large language models into a multi-modal architecture for\nthis task. It achieves impressive results on the DAIC-WOZ AVEC 2016 Challenge\ncross-validation split and Leave-One-Subject-Out cross-validation split,\nsurpassing all baseline models and multiple state-of-the-art models. In\nLeave-One-Subject-Out testing, it achieves an accuracy of 91.01%, an F1-Score\nof 85.95%, a precision of 80%, and a recall of 92.86%.\n","authors":["Santosh V. Patapati"],"pdf_url":"https://arxiv.org/pdf/2407.19340v5.pdf","comment":"Keywords: Multi-Modal Neural Networks, Deep Learning, Large Language\n  Models, Depression Diagnosis, Biomedical Informatics, DAIC-WOZ"},{"id":"http://arxiv.org/abs/2410.08877v1","updated":"2024-10-11T14:54:08Z","published":"2024-10-11T14:54:08Z","title":"Interdependency Matters: Graph Alignment for Multivariate Time Series\n  Anomaly Detection","summary":"  Anomaly detection in multivariate time series (MTS) is crucial for various\napplications in data mining and industry. Current industrial methods typically\napproach anomaly detection as an unsupervised learning task, aiming to identify\ndeviations by estimating the normal distribution in noisy, label-free datasets.\nThese methods increasingly incorporate interdependencies between channels\nthrough graph structures to enhance accuracy. However, the role of\ninterdependencies is more critical than previously understood, as shifts in\ninterdependencies between MTS channels from normal to anomalous data are\nsignificant. This observation suggests that \\textit{anomalies could be detected\nby changes in these interdependency graph series}. To capitalize on this\ninsight, we introduce MADGA (MTS Anomaly Detection via Graph Alignment), which\nredefines anomaly detection as a graph alignment (GA) problem that explicitly\nutilizes interdependencies for anomaly detection. MADGA dynamically transforms\nsubsequences into graphs to capture the evolving interdependencies, and Graph\nalignment is performed between these graphs, optimizing an alignment plan that\nminimizes cost, effectively minimizing the distance for normal data and\nmaximizing it for anomalous data. Uniquely, our GA approach involves explicit\nalignment of both nodes and edges, employing Wasserstein distance for nodes and\nGromov-Wasserstein distance for edges. To our knowledge, this is the first\napplication of GA to MTS anomaly detection that explicitly leverages\ninterdependency for this purpose. Extensive experiments on diverse real-world\ndatasets validate the effectiveness of MADGA, demonstrating its capability to\ndetect anomalies and differentiate interdependencies, consistently achieving\nstate-of-the-art across various scenarios.\n","authors":["Yuanyi Wang","Haifeng Sun","Chengsen Wang","Mengde Zhu","Jingyu Wang","Wei Tang","Qi Qi","Zirui Zhuang","Jianxin Liao"],"pdf_url":"https://arxiv.org/pdf/2410.08877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08692v1","updated":"2024-10-11T10:24:36Z","published":"2024-10-11T10:24:36Z","title":"Contrastive Knowledge Distillation for Robust Multimodal Sentiment\n  Analysis","summary":"  Multimodal sentiment analysis (MSA) systems leverage information from\ndifferent modalities to predict human sentiment intensities. Incomplete\nmodality is an important issue that may cause a significant performance drop in\nMSA systems. By generative imputation, i.e., recovering the missing data from\navailable data, systems may achieve robust performance but will lead to high\ncomputational costs. This paper introduces a knowledge distillation method,\ncalled `Multi-Modal Contrastive Knowledge Distillation' (MM-CKD), to address\nthe issue of incomplete modality in video sentiment analysis with lower\ncomputation cost, as a novel non-imputation-based method. We employ Multi-view\nSupervised Contrastive Learning (MVSC) to transfer knowledge from a teacher\nmodel to student models. This approach not only leverages cross-modal knowledge\nbut also introduces cross-sample knowledge with supervision, jointly improving\nthe performance of both teacher and student models through online learning. Our\nmethod gives competitive results with significantly lower computational costs\nthan state-of-the-art imputation-based methods.\n","authors":["Zhongyi Sang","Kotaro Funakoshi","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2410.08692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08642v1","updated":"2024-10-11T09:10:26Z","published":"2024-10-11T09:10:26Z","title":"More than Memes: A Multimodal Topic Modeling Approach to Conspiracy\n  Theories on Telegram","summary":"  Research on conspiracy theories and related content online has traditionally\nfocused on textual data. To address the increasing prevalence of (audio-)visual\ndata on social media, and to capture the evolving and dynamic nature of this\ncommunication, researchers have begun to explore the potential of unsupervised\napproaches for analyzing multimodal online content. Our research contributes to\nthis field by exploring the potential of multimodal topic modeling for\nanalyzing conspiracy theories in German-language Telegram channels. Our work\nuses the BERTopic topic modeling approach in combination with CLIP for the\nanalysis of textual and visual data. We analyze a corpus of ~40, 000 Telegram\nmessages posted in October 2023 in 571 German-language Telegram channels known\nfor disseminating conspiracy theories and other deceptive content. We explore\nthe potentials and challenges of this approach for studying a medium-sized\ncorpus of user-generated, text-image online content. We offer insights into the\ndominant topics across modalities, different text and image genres discovered\nduring the analysis, quantitative inter-modal topic analyses, and a qualitative\ncase study of textual, visual, and multimodal narrative strategies in the\ncommunication of conspiracy theories.\n","authors":["Elisabeth Steffen"],"pdf_url":"https://arxiv.org/pdf/2410.08642v1.pdf","comment":"11 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.08620v1","updated":"2024-10-11T08:36:07Z","published":"2024-10-11T08:36:07Z","title":"Natural Language Induced Adversarial Images","summary":"  Research of adversarial attacks is important for AI security because it shows\nthe vulnerability of deep learning models and helps to build more robust\nmodels. Adversarial attacks on images are most widely studied, which include\nnoise-based attacks, image editing-based attacks, and latent space-based\nattacks. However, the adversarial examples crafted by these methods often lack\nsufficient semantic information, making it challenging for humans to understand\nthe failure modes of deep learning models under natural conditions. To address\nthis limitation, we propose a natural language induced adversarial image attack\nmethod. The core idea is to leverage a text-to-image model to generate\nadversarial images given input prompts, which are maliciously constructed to\nlead to misclassification for a target model. To adopt commercial text-to-image\nmodels for synthesizing more natural adversarial images, we propose an adaptive\ngenetic algorithm (GA) for optimizing discrete adversarial prompts without\nrequiring gradients and an adaptive word space reduction method for improving\nquery efficiency. We further used CLIP to maintain the semantic consistency of\nthe generated images. In our experiments, we found that some high-frequency\nsemantic information such as \"foggy\", \"humid\", \"stretching\", etc. can easily\ncause classifier errors. This adversarial semantic information exists not only\nin generated images but also in photos captured in the real world. We also\nfound that some adversarial semantic information can be transferred to unknown\nclassification tasks. Furthermore, our attack method can transfer to different\ntext-to-image models (e.g., Midjourney, DALL-E 3, etc.) and image classifiers.\nOur code is available at:\nhttps://github.com/zxp555/Natural-Language-Induced-Adversarial-Images.\n","authors":["Xiaopei Zhu","Peiyang Xu","Guanning Zeng","Yingpeng Dong","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2410.08620v1.pdf","comment":"Carmera-ready version. To appear in ACM MM 2024"},{"id":"http://arxiv.org/abs/2410.08530v1","updated":"2024-10-11T05:02:31Z","published":"2024-10-11T05:02:31Z","title":"Ego3DT: Tracking Every 3D Object in Ego-centric Videos","summary":"  The growing interest in embodied intelligence has brought ego-centric\nperspectives to contemporary research. One significant challenge within this\nrealm is the accurate localization and tracking of objects in ego-centric\nvideos, primarily due to the substantial variability in viewing angles.\nAddressing this issue, this paper introduces a novel zero-shot approach for the\n3D reconstruction and tracking of all objects from the ego-centric video. We\npresent Ego3DT, a novel framework that initially identifies and extracts\ndetection and segmentation information of objects within the ego environment.\nUtilizing information from adjacent video frames, Ego3DT dynamically constructs\na 3D scene of the ego view using a pre-trained 3D scene reconstruction model.\nAdditionally, we have innovated a dynamic hierarchical association mechanism\nfor creating stable 3D tracking trajectories of objects in ego-centric videos.\nMoreover, the efficacy of our approach is corroborated by extensive experiments\non two newly compiled datasets, with 1.04x - 2.90x in HOTA, showcasing the\nrobustness and accuracy of our method in diverse ego-centric scenarios.\n","authors":["Shengyu Hao","Wenhao Chai","Zhonghan Zhao","Meiqi Sun","Wendi Hu","Jieyang Zhou","Yixian Zhao","Qi Li","Yizhou Wang","Xi Li","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.08530v1.pdf","comment":"Accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/1609.07630v4","updated":"2024-10-11T00:55:00Z","published":"2016-09-24T14:49:31Z","title":"Low-complexity Image and Video Coding Based on an Approximate Discrete\n  Tchebichef Transform","summary":"  The usage of linear transformations has great relevance for data\ndecorrelation applications, like image and video compression. In that sense,\nthe discrete Tchebichef transform (DTT) possesses useful coding and\ndecorrelation properties. The DTT transform kernel does not depend on the input\ndata and fast algorithms can be developed to real time applications. However,\nthe DTT fast algorithm presented in literature possess high computational\ncomplexity. In this work, we introduce a new low-complexity approximation for\nthe DTT. The fast algorithm of the proposed transform is multiplication-free\nand requires a reduced number of additions and bit-shifting operations. Image\nand video compression simulations in popular standards shows good performance\nof the proposed transform. Regarding hardware resource consumption for FPGA\nshows 43.1% reduction of configurable logic blocks and ASIC place and route\nrealization shows 57.7% reduction in the area-time figure when compared with\nthe 2-D version of the exact DTT.\n","authors":["P. A. M. Oliveira","R. J. Cintra","F. M. Bayer","S. Kulasekera","A. Madanayake","V. A. Coutinho"],"pdf_url":"https://arxiv.org/pdf/1609.07630v4.pdf","comment":"Fixed typo in $C_g$ and $\\eta$ measurements from Table 1 (W A S\n  Aleixo); 11 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.08435v1","updated":"2024-10-11T00:41:46Z","published":"2024-10-11T00:41:46Z","title":"Symbolic Music Generation with Fine-grained Interactive Textural\n  Guidance","summary":"  The problem of symbolic music generation presents unique challenges due to\nthe combination of limited data availability and the need for high precision in\nnote pitch. To overcome these difficulties, we introduce Fine-grained Textural\nGuidance (FTG) within diffusion models to correct errors in the learned\ndistributions. By incorporating FTG, the diffusion models improve the accuracy\nof music generation, which makes them well-suited for advanced tasks such as\nprogressive music generation, improvisation and interactive music creation. We\nderive theoretical characterizations for both the challenges in symbolic music\ngeneration and the effect of the FTG approach. We provide numerical experiments\nand a demo page for interactive music generation with user input to showcase\nthe effectiveness of our approach.\n","authors":["Tingyu Zhu","Haoyu Liu","Zhimin Jiang","Zeyu Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.08435v1.pdf","comment":null}]},"2024-10-10T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.08393v1","updated":"2024-10-10T22:00:16Z","published":"2024-10-10T22:00:16Z","title":"The Effects of Hallucinations in Synthetic Training Data for Relation\n  Extraction","summary":"  Relation extraction is crucial for constructing knowledge graphs, with large\nhigh-quality datasets serving as the foundation for training, fine-tuning, and\nevaluating models. Generative data augmentation (GDA) is a common approach to\nexpand such datasets. However, this approach often introduces hallucinations,\nsuch as spurious facts, whose impact on relation extraction remains\nunderexplored. In this paper, we examine the effects of hallucinations on the\nperformance of relation extraction on the document and sentence levels. Our\nempirical study reveals that hallucinations considerably compromise the ability\nof models to extract relations from text, with recall reductions between 19.1%\nand 39.2%. We identify that relevant hallucinations impair the model's\nperformance, while irrelevant hallucinations have a minimal impact.\nAdditionally, we develop methods for the detection of hallucinations to improve\ndata quality and model performance. Our approaches successfully classify texts\nas either 'hallucinated' or 'clean,' achieving high F1-scores of 83.8% and\n92.2%. These methods not only assist in removing hallucinations but also help\nin estimating their prevalence within datasets, which is crucial for selecting\nhigh-quality data. Overall, our work confirms the profound impact of relevant\nhallucinations on the effectiveness of relation extraction models.\n","authors":["Steven Rogulsky","Nicholas Popovic","Michael Färber"],"pdf_url":"https://arxiv.org/pdf/2410.08393v1.pdf","comment":"Accepted at KBC-LM@ISWC'24"},{"id":"http://arxiv.org/abs/2410.08352v1","updated":"2024-10-10T20:15:28Z","published":"2024-10-10T20:15:28Z","title":"Revealing COVID-19's Social Dynamics: Diachronic Semantic Analysis of\n  Vaccine and Symptom Discourse on Twitter","summary":"  Social media is recognized as an important source for deriving insights into\npublic opinion dynamics and social impacts due to the vast textual data\ngenerated daily and the 'unconstrained' behavior of people interacting on these\nplatforms. However, such analyses prove challenging due to the semantic shift\nphenomenon, where word meanings evolve over time. This paper proposes an\nunsupervised dynamic word embedding method to capture longitudinal semantic\nshifts in social media data without predefined anchor words. The method\nleverages word co-occurrence statistics and dynamic updating to adapt\nembeddings over time, addressing the challenges of data sparseness, imbalanced\ndistributions, and synergistic semantic effects. Evaluated on a large COVID-19\nTwitter dataset, the method reveals semantic evolution patterns of vaccine- and\nsymptom-related entities across different pandemic stages, and their potential\ncorrelations with real-world statistics. Our key contributions include the\ndynamic embedding technique, empirical analysis of COVID-19 semantic shifts,\nand discussions on enhancing semantic shift modeling for computational social\nscience research. This study enables capturing longitudinal semantic dynamics\non social media to understand public discourse and collective phenomena.\n","authors":["Zeqiang Wang","Jiageng Wu","Yuqi Wang","Wei Wang","Jie Yang","Jon Johnson","Nishanth Sastry","Suparna De"],"pdf_url":"https://arxiv.org/pdf/2410.08352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06371v2","updated":"2024-10-10T19:58:55Z","published":"2024-10-08T21:09:55Z","title":"Improved Estimation of Ranks for Learning Item Recommenders with\n  Negative Sampling","summary":"  In recommendation systems, there has been a growth in the number of\nrecommendable items (# of movies, music, products). When the set of\nrecommendable items is large, training and evaluation of item recommendation\nmodels becomes computationally expensive. To lower this cost, it has become\ncommon to sample negative items. However, the recommendation quality can suffer\nfrom biases introduced by traditional negative sampling mechanisms. In this\nwork, we demonstrate the benefits from correcting the bias introduced by\nsampling of negatives. We first provide sampled batch version of the\nwell-studied WARP and LambdaRank methods. Then, we present how these methods\ncan benefit from improved ranking estimates. Finally, we evaluate the\nrecommendation quality as a result of correcting rank estimates and demonstrate\nthat WARP and LambdaRank can be learned efficiently with negative sampling and\nour proposed correction technique.\n","authors":["Anushya Subbiah","Steffen Rendle","Vikram Aggarwal"],"pdf_url":"https://arxiv.org/pdf/2410.06371v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08324v1","updated":"2024-10-10T19:24:13Z","published":"2024-10-10T19:24:13Z","title":"The language of sound search: Examining User Queries in Audio Search\n  Engines","summary":"  This study examines textual, user-written search queries within the context\nof sound search engines, encompassing various applications such as foley, sound\neffects, and general audio retrieval. Current research inadequately addresses\nreal-world user needs and behaviours in designing text-based audio retrieval\nsystems. To bridge this gap, we analysed search queries from two sources: a\ncustom survey and Freesound website query logs. The survey was designed to\ncollect queries for an unrestricted, hypothetical sound search engine,\nresulting in a dataset that captures user intentions without the constraints of\nexisting systems. This dataset is also made available for sharing with the\nresearch community. In contrast, the Freesound query logs encompass\napproximately 9 million search requests, providing a comprehensive view of\nreal-world usage patterns. Our findings indicate that survey queries are\ngenerally longer than Freesound queries, suggesting users prefer detailed\nqueries when not limited by system constraints. Both datasets predominantly\nfeature keyword-based queries, with few survey participants using full\nsentences. Key factors influencing survey queries include the primary sound\nsource, intended usage, perceived location, and the number of sound sources.\nThese insights are crucial for developing user-centred, effective text-based\naudio retrieval systems, enhancing our understanding of user behaviour in sound\nsearch contexts.\n","authors":["Benno Weck","Frederic Font"],"pdf_url":"https://arxiv.org/pdf/2410.08324v1.pdf","comment":"Accepted at DCASE 2024. Supplementary materials at\n  https://doi.org/10.5281/zenodo.13622537"},{"id":"http://arxiv.org/abs/2410.07797v1","updated":"2024-10-10T10:30:28Z","published":"2024-10-10T10:30:28Z","title":"Rewriting Conversational Utterances with Instructed Large Language\n  Models","summary":"  Many recent studies have shown the ability of large language models (LLMs) to\nachieve state-of-the-art performance on many NLP tasks, such as question\nanswering, text summarization, coding, and translation. In some cases, the\nresults provided by LLMs are on par with those of human experts. These models'\nmost disruptive innovation is their ability to perform tasks via zero-shot or\nfew-shot prompting. This capability has been successfully exploited to train\ninstructed LLMs, where reinforcement learning with human feedback is used to\nguide the model to follow the user's requests directly. In this paper, we\ninvestigate the ability of instructed LLMs to improve conversational search\neffectiveness by rewriting user questions in a conversational setting. We study\nwhich prompts provide the most informative rewritten utterances that lead to\nthe best retrieval performance. Reproducible experiments are conducted on\npublicly-available TREC CAST datasets. The results show that rewriting\nconversational utterances with instructed LLMs achieves significant\nimprovements of up to 25.2% in MRR, 31.7% in Precision@1, 27% in NDCG@3, and\n11.5% in Recall@500 over state-of-the-art techniques.\n","authors":["Elnara Galimzhanova","Cristina Ioana Muntean","Franco Maria Nardini","Raffaele Perego","Guido Rocchietti"],"pdf_url":"https://arxiv.org/pdf/2410.07797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16182v2","updated":"2024-10-10T07:10:50Z","published":"2024-09-24T15:26:38Z","title":"TiM4Rec: An Efficient Sequential Recommendation Model Based on\n  Time-Aware Structured State Space Duality Model","summary":"  Sequential recommendation represents a pivotal branch of recommendation\nsystems, centered around dynamically analyzing the sequential dependencies\nbetween user preferences and their interactive behaviors. Despite the\nTransformer architecture-based models achieving commendable performance within\nthis domain, their quadratic computational complexity relative to the sequence\ndimension impedes efficient modeling. In response, the innovative Mamba\narchitecture, characterized by linear computational complexity, has emerged.\nMamba4Rec further pioneers the application of Mamba in sequential\nrecommendation. Nonetheless, Mamba 1's hardware-aware algorithm struggles to\nefficiently leverage modern matrix computational units, which lead to the\nproposal of the improved State Space Duality (SSD), also known as Mamba 2.\nWhile the SSD4Rec successfully adapts the SSD architecture for sequential\nrecommendation, showing promising results in high-dimensional contexts, it\nsuffers significant performance drops in low-dimensional scenarios crucial for\npure ID sequential recommendation tasks. Addressing this challenge, we propose\na novel sequential recommendation backbone model, TiM4Rec, which ameliorates\nthe low-dimensional performance loss of the SSD architecture while preserving\nits computational efficiency. Drawing inspiration from TiSASRec, we develop a\ntime-aware enhancement method tailored for the linear computation demands of\nthe SSD architecture, thereby enhancing its adaptability and achieving\nstate-of-the-art (SOTA) performance in both low and high-dimensional modeling.\nThe code for our model is publicly accessible at\nhttps://github.com/AlwaysFHao/TiM4Rec.\n","authors":["Hao Fan","Mengyi Zhu","Yanrong Hu","Hailin Feng","Zhijie He","Hongjiu Liu","Qingyang Liu"],"pdf_url":"https://arxiv.org/pdf/2409.16182v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07654v1","updated":"2024-10-10T06:48:27Z","published":"2024-10-10T06:48:27Z","title":"Firzen: Firing Strict Cold-Start Items with Frozen Heterogeneous and\n  Homogeneous Graphs for Recommendation","summary":"  Recommendation models utilizing unique identities (IDs) to represent distinct\nusers and items have dominated the recommender systems literature for over a\ndecade. Since multi-modal content of items (e.g., texts and images) and\nknowledge graphs (KGs) may reflect the interaction-related users' preferences\nand items' characteristics, they have been utilized as useful side information\nto further improve the recommendation quality. However, the success of such\nmethods often limits to either warm-start or strict cold-start item\nrecommendation in which some items neither appear in the training data nor have\nany interactions in the test stage: (1) Some fail to learn the embedding of a\nstrict cold-start item since side information is only utilized to enhance the\nwarm-start ID representations; (2) The others deteriorate the performance of\nwarm-start recommendation since unrelated multi-modal content or entities in\nKGs may blur the final representations. In this paper, we propose a unified\nframework incorporating multi-modal content of items and KGs to effectively\nsolve both strict cold-start and warm-start recommendation termed Firzen, which\nextracts the user-item collaborative information over frozen heterogeneous\ngraph (collaborative knowledge graph), and exploits the item-item semantic\nstructures and user-user behavioral association over frozen homogeneous graphs\n(item-item relation graph and user-user co-occurrence graph). Furthermore, we\nbuild four unified strict cold-start evaluation benchmarks based on publicly\navailable Amazon datasets and a real-world industrial dataset from Weixin\nChannels via rearranging the interaction data and constructing KGs. Extensive\nempirical results demonstrate that our model yields significant improvements\nfor strict cold-start recommendation and outperforms or matches the\nstate-of-the-art performance in the warm-start scenario.\n","authors":["Hulingxiao He","Xiangteng He","Yuxin Peng","Zifei Shan","Xin Su"],"pdf_url":"https://arxiv.org/pdf/2410.07654v1.pdf","comment":"Accepted by ICDE 2024. The code is available at\n  https://github.com/PKU-ICST-MIPL/Firzen_ICDE2024"},{"id":"http://arxiv.org/abs/2410.06062v2","updated":"2024-10-10T06:13:41Z","published":"2024-10-08T14:09:12Z","title":"LLM-based SPARQL Query Generation from Natural Language over Federated\n  Knowledge Graphs","summary":"  We introduce a Retrieval-Augmented Generation (RAG) system for translating\nuser questions into accurate federated SPARQL queries over bioinformatics\nknowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance\naccuracy and reduce hallucinations in query generation, our system utilises\nmetadata from the KGs, including query examples and schema information, and\nincorporates a validation step to correct generated queries. The system is\navailable online at chat.expasy.org.\n","authors":["Vincent Emonet","Jerven Bolleman","Severine Duvaud","Tarcisio Mendes de Farias","Ana Claudia Sima"],"pdf_url":"https://arxiv.org/pdf/2410.06062v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.04735v2","updated":"2024-10-10T05:15:06Z","published":"2024-05-08T00:52:57Z","title":"Cryptanalysis of the SIMON Cypher Using Neo4j","summary":"  The exponential growth in the number of Internet of Things (IoT) devices has\nseen the introduction of several Lightweight Encryption Algorithms (LEA). While\nLEAs are designed to enhance the integrity, privacy and security of data\ncollected and transmitted by IoT devices, it is hazardous to assume that all\nLEAs are secure and exhibit similar levels of protection. To improve encryption\nstrength, cryptanalysts and algorithm designers routinely probe LEAs using\nvarious cryptanalysis techniques to identify vulnerabilities and limitations of\nLEAs. Despite recent improvements in the efficiency of cryptanalysis utilising\nheuristic methods and a Partial Difference Distribution Table (PDDT), the\nprocess remains inefficient, with the random nature of the heuristic inhibiting\nreproducible results. However, the use of a PDDT presents opportunities to\nidentify relationships between differentials utilising knowledge graphs,\nleading to the identification of efficient paths throughout the PDDT. This\npaper introduces the novel use of knowledge graphs to identify intricate\nrelationships between differentials in the SIMON LEA, allowing for the\nidentification of optimal paths throughout the differentials, and increasing\nthe effectiveness of the differential security analyses of SIMON.\n","authors":["Jonathan Cook","Sabih ur Rehman","M. Arif Khan"],"pdf_url":"https://arxiv.org/pdf/2405.04735v2.pdf","comment":"J. Cook, S. u. Rehman and M. A. Khan, \"Cryptanalysis of the SIMON\n  Cypher Using Neo4j,\" 2024 International Conference on Electrical, Computer\n  and Energy Technologies (ICECET, Sydney, Australia, 2024, pp. 1-6, doi:\n  10.1109/ICECET61485.2024.10698687. 979-8-3503-9591-4/24/$31.00\n  \\c{opyright}2024 IEEE https://ieeexplore.ieee.org/document/10698687"},{"id":"http://arxiv.org/abs/2410.07610v1","updated":"2024-10-10T04:54:37Z","published":"2024-10-10T04:54:37Z","title":"CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features","summary":"  Multimodal encoders like CLIP excel in tasks such as zero-shot image\nclassification and cross-modal retrieval. However, they require excessive\ntraining data. We propose canonical similarity analysis (CSA), which uses two\nunimodal encoders to replicate multimodal encoders using limited data. CSA maps\nunimodal features into a multimodal space, using a new similarity score to\nretain only the multimodal information. CSA only involves the inference of\nunimodal encoders and a cubic-complexity matrix decomposition, eliminating the\nneed for extensive GPU-based model training. Experiments show that CSA\noutperforms CLIP while requiring $300,000\\times$ fewer multimodal data pairs\nand $6\\times$ fewer unimodal data for ImageNet classification and\nmisinformative news captions detection. CSA surpasses the state-of-the-art\nmethod to map unimodal features to multimodal features. We also demonstrate the\nability of CSA with modalities beyond image and text, paving the way for future\nmodality pairs with limited paired multimodal data but abundant unpaired\nunimodal data, such as lidar and text.\n","authors":["Po-han Li","Sandeep P. Chinchali","Ufuk Topcu"],"pdf_url":"https://arxiv.org/pdf/2410.07610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07589v1","updated":"2024-10-10T03:51:58Z","published":"2024-10-10T03:51:58Z","title":"No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in\n  LLMs, Even for Vigilant Users","summary":"  Retrieval-Augmented Generation (RAG) is widely adopted for its effectiveness\nand cost-efficiency in mitigating hallucinations and enhancing the\ndomain-specific generation capabilities of large language models (LLMs).\nHowever, is this effectiveness and cost-efficiency truly a free lunch? In this\nstudy, we comprehensively investigate the fairness costs associated with RAG by\nproposing a practical three-level threat model from the perspective of user\nawareness of fairness. Specifically, varying levels of user fairness awareness\nresult in different degrees of fairness censorship on the external dataset. We\nexamine the fairness implications of RAG using uncensored, partially censored,\nand fully censored datasets. Our experiments demonstrate that fairness\nalignment can be easily undermined through RAG without the need for fine-tuning\nor retraining. Even with fully censored and supposedly unbiased external\ndatasets, RAG can lead to biased outputs. Our findings underscore the\nlimitations of current alignment methods in the context of RAG-based LLMs and\nhighlight the urgent need for new strategies to ensure fairness. We propose\npotential mitigations and call for further research to develop robust fairness\nsafeguards in RAG-based LLMs.\n","authors":["Mengxuan Hu","Hongyi Wu","Zihan Guan","Ronghang Zhu","Dongliang Guo","Daiqing Qi","Sheng Li"],"pdf_url":"https://arxiv.org/pdf/2410.07589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06095v3","updated":"2024-10-10T03:11:49Z","published":"2023-03-10T17:24:41Z","title":"HiNet: Novel Multi-Scenario & Multi-Task Learning with Hierarchical\n  Information Extraction","summary":"  Multi-scenario & multi-task learning has been widely applied to many\nrecommendation systems in industrial applications, wherein an effective and\npractical approach is to carry out multi-scenario transfer learning on the\nbasis of the Mixture-of-Expert (MoE) architecture. However, the MoE-based\nmethod, which aims to project all information in the same feature space, cannot\neffectively deal with the complex relationships inherent among various\nscenarios and tasks, resulting in unsatisfactory performance. To tackle the\nproblem, we propose a Hierarchical information extraction Network (HiNet) for\nmulti-scenario and multi-task recommendation, which achieves hierarchical\nextraction based on coarse-to-fine knowledge transfer scheme. The multiple\nextraction layers of the hierarchical network enable the model to enhance the\ncapability of transferring valuable information across scenarios while\npreserving specific features of scenarios and tasks. Furthermore, a novel\nscenario-aware attentive network module is proposed to model correlations\nbetween scenarios explicitly. Comprehensive experiments conducted on real-world\nindustrial datasets from Meituan Meishi platform demonstrate that HiNet\nachieves a new state-of-the-art performance and significantly outperforms\nexisting solutions. HiNet is currently fully deployed in two scenarios and has\nachieved 2.87% and 1.75% order quantity gain respectively.\n","authors":["Jie Zhou","Xianshuai Cao","Wenhao Li","Lin Bo","Kun Zhang","Chuan Luo","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2303.06095v3.pdf","comment":"The paper has been accepted by ICDE2023"}],"Multimedia":[{"id":"http://arxiv.org/abs/2404.12257v2","updated":"2024-10-10T20:02:07Z","published":"2024-04-18T15:23:37Z","title":"Food Portion Estimation via 3D Object Scaling","summary":"  Image-based methods to analyze food images have alleviated the user burden\nand biases associated with traditional methods. However, accurate portion\nestimation remains a major challenge due to the loss of 3D information in the\n2D representation of foods captured by smartphone cameras or wearable devices.\nIn this paper, we propose a new framework to estimate both food volume and\nenergy from 2D images by leveraging the power of 3D food models and physical\nreference in the eating scene. Our method estimates the pose of the camera and\nthe food object in the input image and recreates the eating occasion by\nrendering an image of a 3D model of the food with the estimated poses. We also\nintroduce a new dataset, SimpleFood45, which contains 2D images of 45 food\nitems and associated annotations including food volume, weight, and energy. Our\nmethod achieves an average error of 31.10 kCal (17.67%) on this dataset,\noutperforming existing portion estimation methods. The dataset can be accessed\nat: https://lorenz.ecn.purdue.edu/~gvinod/simplefood45/ and the code can be\naccessed at: https://gitlab.com/viper-purdue/monocular-food-volume-3d\n","authors":["Gautham Vinod","Jiangpeng He","Zeman Shao","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.12257v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08174v1","updated":"2024-10-10T17:50:42Z","published":"2024-10-10T17:50:42Z","title":"Sample then Identify: A General Framework for Risk Control and\n  Assessment in Multimodal Large Language Models","summary":"  Multimodal Large Language Models (MLLMs) exhibit promising advancements\nacross various tasks, yet they still encounter significant trustworthiness\nissues. Prior studies apply Split Conformal Prediction (SCP) in language\nmodeling to construct prediction sets with statistical guarantees. However,\nthese methods typically rely on internal model logits or are restricted to\nmultiple-choice settings, which hampers their generalizability and adaptability\nin dynamic, open-ended environments. In this paper, we introduce TRON, a\ntwo-step framework for risk control and assessment, applicable to any MLLM that\nsupports sampling in both open-ended and closed-ended scenarios. TRON comprises\ntwo main components: (1) a novel conformal score to sample response sets of\nminimum size, and (2) a nonconformity score to identify high-quality responses\nbased on self-consistency theory, controlling the error rates by two specific\nrisk levels. Furthermore, we investigate semantic redundancy in prediction sets\nwithin open-ended contexts for the first time, leading to a promising\nevaluation metric for MLLMs based on average set size. Our comprehensive\nexperiments across four Video Question-Answering (VideoQA) datasets utilizing\neight MLLMs show that TRON achieves desired error rates bounded by two\nuser-specified risk levels. Additionally, deduplicated prediction sets maintain\nadaptiveness while being more efficient and stable for risk assessment under\ndifferent risk levels.\n","authors":["Qingni Wang","Tiantian Geng","Zhiyuan Wang","Teng Wang","Bo Fu","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.08174v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.10719v2","updated":"2024-10-10T14:47:58Z","published":"2024-09-16T20:47:00Z","title":"Benchmarking VLMs' Reasoning About Persuasive Atypical Images","summary":"  Vision language models (VLMs) have shown strong zero-shot generalization\nacross various tasks, especially when integrated with large language models\n(LLMs). However, their ability to comprehend rhetorical and persuasive visual\nmedia, such as advertisements, remains understudied. Ads often employ atypical\nimagery, using surprising object juxtapositions to convey shared properties.\nFor example, Fig. 1 (e) shows a beer with a feather-like texture. This requires\nadvanced reasoning to deduce that this atypical representation signifies the\nbeer's lightness. We introduce three novel tasks, Multi-label Atypicality\nClassification, Atypicality Statement Retrieval, and Aypical Object\nRecognition, to benchmark VLMs' understanding of atypicality in persuasive\nimages. We evaluate how well VLMs use atypicality to infer an ad's message and\ntest their reasoning abilities by employing semantically challenging negatives.\nFinally, we pioneer atypicality-aware verbalization by extracting comprehensive\nimage descriptions sensitive to atypical elements. Our findings reveal that:\n(1) VLMs lack advanced reasoning capabilities compared to LLMs; (2) simple,\neffective strategies can extract atypicality-aware information, leading to\ncomprehensive image verbalization; (3) atypicality aids persuasive\nadvertisement understanding. Code and data will be made available.\n","authors":["Sina Malakouti","Aysan Aghazadeh","Ashmit Khandelwal","Adriana Kovashka"],"pdf_url":"https://arxiv.org/pdf/2409.10719v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07854v1","updated":"2024-10-10T12:20:58Z","published":"2024-10-10T12:20:58Z","title":"HeGraphAdapter: Tuning Multi-Modal Vision-Language Models with\n  Heterogeneous Graph Adapter","summary":"  Adapter-based tuning methods have shown significant potential in transferring\nknowledge from pre-trained Vision-Language Models to the downstream tasks.\nHowever, after reviewing existing adapters, we find they generally fail to\nfully explore the interactions between different modalities in constructing\ntask-specific knowledge. Also, existing works usually only focus on similarity\nmatching between positive text prompts, making it challenging to distinguish\nthe classes with high similar visual contents. To address these issues, in this\npaper, we propose a novel Heterogeneous Graph Adapter to achieve tuning VLMs\nfor the downstream tasks. To be specific, we first construct a unified\nheterogeneous graph mode, which contains i) visual nodes, positive text nodes\nand negative text nodes, and ii) several types of edge connections to\ncomprehensively model the intra-modality, inter-modality and inter-class\nstructure knowledge together. Next, we employ a specific Heterogeneous Graph\nNeural Network to excavate multi-modality structure knowledge for adapting both\nvisual and textual features for the downstream tasks. Finally, after\nHeGraphAdapter, we construct both text-based and visual-based classifiers\nsimultaneously to comprehensively enhance the performance of the CLIP model.\nExperimental results on 11 benchmark datasets demonstrate the effectiveness and\nbenefits of the proposed HeGraphAdapter.\n","authors":["Yumiao Zhao","Bo Jiang","Xiao Wang","Qin Xu","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2410.07854v1.pdf","comment":null}]},"2024-10-09T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.00009v2","updated":"2024-10-09T19:13:41Z","published":"2024-08-15T15:13:16Z","title":"Web Retrieval Agents for Evidence-Based Misinformation Detection","summary":"  This paper develops an agent-based automated fact-checking approach for\ndetecting misinformation. We demonstrate that combining a powerful LLM agent,\nwhich does not have access to the internet for searches, with an online web\nsearch agent yields better results than when each tool is used independently.\nOur approach is robust across multiple models, outperforming alternatives and\nincreasing the macro F1 of misinformation detection by as much as 20 percent\ncompared to LLMs without search. We also conduct extensive analyses on the\nsources our system leverages and their biases, decisions in the construction of\nthe system like the search tool and the knowledge base, the type of evidence\nneeded and its impact on the results, and other parts of the overall process.\nBy combining strong performance with in-depth understanding, we hope to provide\nbuilding blocks for future search-enabled misinformation mitigation systems.\n","authors":["Jacob-Junqi Tian","Hao Yu","Yury Orlovskiy","Tyler Vergho","Mauricio Rivera","Mayank Goel","Zachary Yang","Jean-Francois Godbout","Reihaneh Rabbany","Kellin Pelrine"],"pdf_url":"https://arxiv.org/pdf/2409.00009v2.pdf","comment":"1 main figure, 8 tables, 10 pages, 12 figures in Appendix, 7 tables\n  in Appendix GitHub URL: https://github.com/ComplexData-MILA/webretrieval"},{"id":"http://arxiv.org/abs/2410.07022v1","updated":"2024-10-09T16:05:16Z","published":"2024-10-09T16:05:16Z","title":"Exploiting Distribution Constraints for Scalable and Efficient Image\n  Retrieval","summary":"  Image retrieval is crucial in robotics and computer vision, with downstream\napplications in robot place recognition and vision-based product\nrecommendations. Modern retrieval systems face two key challenges: scalability\nand efficiency. State-of-the-art image retrieval systems train specific neural\nnetworks for each dataset, an approach that lacks scalability. Furthermore,\nsince retrieval speed is directly proportional to embedding size, existing\nsystems that use large embeddings lack efficiency. To tackle scalability,\nrecent works propose using off-the-shelf foundation models. However, these\nmodels, though applicable across datasets, fall short in achieving performance\ncomparable to that of dataset-specific models. Our key observation is that,\nwhile foundation models capture necessary subtleties for effective retrieval,\nthe underlying distribution of their embedding space can negatively impact\ncosine similarity searches. We introduce Autoencoders with Strong Variance\nConstraints (AE-SVC), which, when used for projection, significantly improves\nthe performance of foundation models. We provide an in-depth theoretical\nanalysis of AE-SVC. Addressing efficiency, we introduce Single-shot Similarity\nSpace Distillation ((SS)$_2$D), a novel approach to learn embeddings with\nadaptive sizes that offers a better trade-off between size and performance. We\nconducted extensive experiments on four retrieval datasets, including Stanford\nOnline Products (SoP) and Pittsburgh30k, using four different off-the-shelf\nfoundation models, including DinoV2 and CLIP. AE-SVC demonstrates up to a\n$16\\%$ improvement in retrieval performance, while (SS)$_2$D shows a further\n$10\\%$ improvement for smaller embedding sizes.\n","authors":["Mohammad Omama","Po-han Li","Sandeep P. Chinchali"],"pdf_url":"https://arxiv.org/pdf/2410.07022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06948v1","updated":"2024-10-09T14:45:43Z","published":"2024-10-09T14:45:43Z","title":"An Overview of zbMATH Open Digital Library","summary":"  Mathematical research thrives on the effective dissemination and discovery of\nknowledge.\n  zbMATH Open has emerged as a pivotal platform in this landscape, offering a\ncomprehensive repository of mathematical literature. Beyond indexing and\nabstracting, it serves as a unified quality-assured infrastructure for finding,\nevaluating, and connecting mathematical information that advances mathematical\nresearch as well as interdisciplinary exploration. zbMATH Open enables\nscientific quality control by post-publication reviews and promotes connections\nbetween researchers, institutions, and research outputs. This paper represents\nthe functionalities of the most significant features of this open-access\nservice, highlighting its role in shaping the future of mathematical\ninformation retrieval.\n","authors":["Madhurima Deb","Isabel Beckenbach","Matteo Petrera","Dariush Ehsani","Marcel Fuhrmann","Yun Hao","Olaf Teschke","Moritz Schubotz"],"pdf_url":"https://arxiv.org/pdf/2410.06948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.09666v3","updated":"2024-10-09T11:48:18Z","published":"2022-05-19T16:29:17Z","title":"Personalized Prompt for Sequential Recommendation","summary":"  Pre-training models have shown their power in sequential recommendation.\nRecently, prompt has been widely explored and verified for tuning in NLP\npre-training, which could help to more effectively and efficiently extract\nuseful knowledge from pre-training models for downstream tasks, especially in\ncold-start scenarios. However, it is challenging to bring prompt-tuning from\nNLP to recommendation, since the tokens in recommendation (i.e., items) do not\nhave explicit explainable semantics, and the sequence modeling should be\npersonalized. In this work, we first introduces prompt to recommendation and\npropose a novel Personalized prompt-based recommendation (PPR) framework for\ncold-start recommendation. Specifically, we build the personalized soft prefix\nprompt via a prompt generator based on user profiles and enable a sufficient\ntraining of prompts via a prompt-oriented contrastive learning with both\nprompt- and behavior-based augmentations. We conduct extensive evaluations on\nvarious tasks. In both few-shot and zero-shot recommendation, PPR models\nachieve significant improvements over baselines on various metrics in three\nlarge-scale open datasets. We also conduct ablation tests and sparsity analysis\nfor a better understanding of PPR. Moreover, We further verify PPR's\nuniversality on different pre-training models, and conduct explorations on\nPPR's other promising downstream tasks including cross-domain recommendation\nand user profile prediction.\n","authors":["Yiqing Wu","Ruobing Xie","Yongchun Zhu","Fuzhen Zhuang","Xu Zhang","Leyu Lin","Qing He"],"pdf_url":"https://arxiv.org/pdf/2205.09666v3.pdf","comment":"accepted by TKDE"},{"id":"http://arxiv.org/abs/2405.00469v2","updated":"2024-10-09T08:52:33Z","published":"2024-05-01T12:12:59Z","title":"Exploiting Positional Bias for Query-Agnostic Generative Content in\n  Search","summary":"  In recent years, neural ranking models (NRMs) have been shown to\nsubstantially outperform their lexical counterparts in text retrieval. In\ntraditional search pipelines, a combination of features leads to well-defined\nbehaviour. However, as neural approaches become increasingly prevalent as the\nfinal scoring component of engines or as standalone systems, their robustness\nto malicious text and, more generally, semantic perturbation needs to be better\nunderstood. We posit that the transformer attention mechanism can induce\nexploitable defects through positional bias in search models, leading to an\nattack that could generalise beyond a single query or topic. We demonstrate\nsuch defects by showing that non-relevant text--such as promotional\ncontent--can be easily injected into a document without adversely affecting its\nposition in search results. Unlike previous gradient-based attacks, we\ndemonstrate these biases in a query-agnostic fashion. In doing so, without the\nknowledge of topicality, we can still reduce the negative effects of\nnon-relevant content injection by controlling injection position. Our\nexperiments are conducted with simulated on-topic promotional text\nautomatically generated by prompting LLMs with topical context from target\ndocuments. We find that contextualisation of a non-relevant text further\nreduces negative effects whilst likely circumventing existing content filtering\nmechanisms. In contrast, lexical models are found to be more resilient to such\ncontent injection attacks. We then investigate a simple yet effective\ncompensation for the weaknesses of the NRMs in search, validating our\nhypotheses regarding transformer bias.\n","authors":["Andrew Parry","Sean MacAvaney","Debasis Ganguly"],"pdf_url":"https://arxiv.org/pdf/2405.00469v2.pdf","comment":"8 pages, 4 main figures, 7 appendix pages, 2 appendix figures,\n  Accepted to ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2408.11345v4","updated":"2024-10-09T08:50:06Z","published":"2024-08-21T05:09:53Z","title":"Learning Deep Tree-based Retriever for Efficient Recommendation: Theory\n  and Method","summary":"  Although advancements in deep learning have significantly enhanced the\nrecommendation accuracy of deep recommendation models, these methods still\nsuffer from low recommendation efficiency. Recently proposed tree-based deep\nrecommendation models alleviate the problem by directly learning tree structure\nand representations under the guidance of recommendation objectives. To\nguarantee the effectiveness of beam search for recommendation accuracy, these\nmodels strive to ensure that the tree adheres to the max-heap assumption, where\na parent node's preference should be the maximum among its children's\npreferences. However, they employ a one-versus-all strategy, framing the\ntraining task as a series of independent binary classification objectives for\neach node, which limits their ability to fully satisfy the max-heap assumption.\nTo this end, we propose a Deep Tree-based Retriever (DTR for short) for\nefficient recommendation. DTR frames the training task as a softmax-based\nmulti-class classification over tree nodes at the same level, enabling explicit\nhorizontal competition and more discriminative top-k selection among them,\nwhich mimics the beam search behavior during training. To mitigate the\nsuboptimality induced by the labeling of non-leaf nodes, we propose a\nrectification method for the loss function, which further aligns with the\nmax-heap assumption in expectation. As the number of tree nodes grows\nexponentially with the levels, we employ sampled softmax to approximate\noptimization and thereby enhance efficiency. Furthermore, we propose a\ntree-based sampling method to reduce the bias inherent in sampled softmax.\nTheoretical results reveal DTR's generalization capability, and both the\nrectification method and tree-based sampling contribute to improved\ngeneralization. The experiments are conducted on four real-world datasets,\nvalidating the effectiveness of the proposed method.\n","authors":["Ze Liu","Jin Zhang","Chao Feng","Defu Lian","Jie Wang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2408.11345v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06096v3","updated":"2024-10-09T08:11:47Z","published":"2024-09-09T22:16:48Z","title":"Latent Diffusion Bridges for Unsupervised Musical Audio Timbre Transfer","summary":"  Music timbre transfer is a challenging task that involves modifying the\ntimbral characteristics of an audio signal while preserving its melodic\nstructure. In this paper, we propose a novel method based on dual diffusion\nbridges, trained using the CocoChorales Dataset, which consists of unpaired\nmonophonic single-instrument audio data. Each diffusion model is trained on a\nspecific instrument with a Gaussian prior. During inference, a model is\ndesignated as the source model to map the input audio to its corresponding\nGaussian prior, and another model is designated as the target model to\nreconstruct the target audio from this Gaussian prior, thereby facilitating\ntimbre transfer. We compare our approach against existing unsupervised timbre\ntransfer models such as VAEGAN and Gaussian Flow Bridges (GFB). Experimental\nresults demonstrate that our method achieves both better Fr\\'echet Audio\nDistance (FAD) and melody preservation, as reflected by lower pitch distances\n(DPD) compared to VAEGAN and GFB. Additionally, we discover that the noise\nlevel from the Gaussian prior, $\\sigma$, can be adjusted to control the degree\nof melody preservation and amount of timbre transferred.\n","authors":["Michele Mancusi","Yurii Halychanskyi","Kin Wai Cheuk","Chieh-Hsin Lai","Stefan Uhlich","Junghyun Koo","Marco A. Martínez-Ramírez","Wei-Hsiang Liao","Giorgio Fabbro","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2409.06096v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06654v1","updated":"2024-10-09T08:06:15Z","published":"2024-10-09T08:06:15Z","title":"Performance Evaluation in Multimedia Retrieval","summary":"  Performance evaluation in multimedia retrieval, as in the information\nretrieval domain at large, relies heavily on retrieval experiments, employing a\nbroad range of techniques and metrics. These can involve human-in-the-loop and\nmachine-only settings for the retrieval process itself and the subsequent\nverification of results. Such experiments can be elaborate and\nuse-case-specific, which can make them difficult to compare or replicate. In\nthis paper, we present a formal model to express all relevant aspects of such\nretrieval experiments, as well as a flexible open-source evaluation\ninfrastructure that implements the model. These contributions intend to make a\nstep towards lowering the hurdles for conducting retrieval experiments and\nimproving their reproducibility.\n","authors":["Loris Sauter","Ralph Gasser","Heiko Schuldt","Abraham Bernstein","Luca Rossetto"],"pdf_url":"https://arxiv.org/pdf/2410.06654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06628v1","updated":"2024-10-09T07:23:02Z","published":"2024-10-09T07:23:02Z","title":"Does Vec2Text Pose a New Corpus Poisoning Threat?","summary":"  The emergence of Vec2Text -- a method for text embedding inversion -- has\nraised serious privacy concerns for dense retrieval systems which use text\nembeddings. This threat comes from the ability for an attacker with access to\nembeddings to reconstruct the original text. In this paper, we take a new look\nat Vec2Text and investigate how much of a threat it poses to the different\nattacks of corpus poisoning, whereby an attacker injects adversarial passages\ninto a retrieval corpus with the intention of misleading dense retrievers.\nTheoretically, Vec2Text is far more dangerous than previous attack methods\nbecause it does not need access to the embedding model's weights and it can\nefficiently generate many adversarial passages. We show that under certain\nconditions, corpus poisoning with Vec2Text can pose a serious threat to dense\nretriever system integrity and user experience by injecting adversarial\npassaged into top ranked positions. Code and data are made available at\nhttps://github.com/ielab/vec2text-corpus-poisoning\n","authors":["Shengyao Zhuang","Bevan Koopman","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2410.06628v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2402.12784"},{"id":"http://arxiv.org/abs/2410.04927v2","updated":"2024-10-09T07:17:03Z","published":"2024-10-07T11:19:05Z","title":"FELLAS: Enhancing Federated Sequential Recommendation with LLM as\n  External Services","summary":"  Federated sequential recommendation (FedSeqRec) has gained growing attention\ndue to its ability to protect user privacy. Unfortunately, the performance of\nFedSeqRec is still unsatisfactory because the models used in FedSeqRec have to\nbe lightweight to accommodate communication bandwidth and clients' on-device\ncomputational resource constraints. Recently, large language models (LLMs) have\nexhibited strong transferable and generalized language understanding abilities\nand therefore, in the NLP area, many downstream tasks now utilize LLMs as a\nservice to achieve superior performance without constructing complex models.\nInspired by this successful practice, we propose a generic FedSeqRec framework,\nFELLAS, which aims to enhance FedSeqRec by utilizing LLMs as an external\nservice. Specifically, FELLAS employs an LLM server to provide both item-level\nand sequence-level representation assistance. The item-level representation\nservice is queried by the central server to enrich the original ID-based item\nembedding with textual information, while the sequence-level representation\nservice is accessed by each client. However, invoking the sequence-level\nrepresentation service requires clients to send sequences to the external LLM\nserver. To safeguard privacy, we implement dx-privacy satisfied sequence\nperturbation, which protects clients' sensitive data with guarantees.\nAdditionally, a contrastive learning-based method is designed to transfer\nknowledge from the noisy sequence representation to clients' sequential\nrecommendation models. Furthermore, to empirically validate the privacy\nprotection capability of FELLAS, we propose two interacted item inference\nattacks. Extensive experiments conducted on three datasets with two widely used\nsequential recommendation models demonstrate the effectiveness and\nprivacy-preserving capability of FELLAS.\n","authors":["Wei Yuan","Chaoqun Yang","Guanhua Ye","Tong Chen","Quoc Viet Hung Nguyen","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2410.04927v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06618v1","updated":"2024-10-09T07:14:49Z","published":"2024-10-09T07:14:49Z","title":"Decomposing Relationship from 1-to-N into N 1-to-1 for Text-Video\n  Retrieval","summary":"  Text-video retrieval (TVR) has seen substantial advancements in recent years,\nfueled by the utilization of pre-trained models and large language models\n(LLMs). Despite these advancements, achieving accurate matching in TVR remains\nchallenging due to inherent disparities between video and textual modalities\nand irregularities in data representation. In this paper, we propose\nText-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the\nconventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships.\nBy replacing a single text query with a series of text proxies, TV-ProxyNet not\nonly broadens the query scope but also achieves a more precise expansion. Each\ntext proxy is crafted through a refined iterative process, controlled by\nmechanisms we term as the director and dash, which regulate the proxy's\ndirection and distance relative to the original text query. This setup not only\nfacilitates more precise semantic alignment but also effectively manages the\ndisparities and noise inherent in multimodal data. Our experiments on three\nrepresentative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet\nCaptions, demonstrate the effectiveness of TV-ProxyNet. The results show an\nimprovement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved\nstate-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0%\nimprovement on DiDeMo compared to existing methods, validating our approach's\nability to enhance semantic mapping and reduce error propensity.\n","authors":["Jian Xiao","Zhenzhen Hu","Jia Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2410.06618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01848v2","updated":"2024-10-09T06:32:41Z","published":"2024-05-03T04:43:24Z","title":"RankSHAP: Shapley Value Based Feature Attributions for Learning to Rank","summary":"  Numerous works propose post-hoc, model-agnostic explanations for learning to\nrank, focusing on ordering entities by their relevance to a query through\nfeature attribution methods. However, these attributions often weakly correlate\nor contradict each other, confusing end users. We adopt an axiomatic\ngame-theoretic approach, popular in the feature attribution community, to\nidentify a set of fundamental axioms that every ranking-based feature\nattribution method should satisfy. We then introduce Rank-SHAP, extending\nclassical Shapley values to ranking. We evaluate the RankSHAP framework through\nextensive experiments on two datasets, multiple ranking methods and evaluation\nmetrics. Additionally, a user study confirms RankSHAP's alignment with human\nintuition. We also perform an axiomatic analysis of existing rank attribution\nalgorithms to determine their compliance with our proposed axioms. Ultimately,\nour aim is to equip practitioners with a set of axiomatically backed feature\nattribution methods for studying IR ranking models, that ensure generality as\nwell as consistency.\n","authors":["Tanya Chowdhury","Yair Zick","James Allan"],"pdf_url":"https://arxiv.org/pdf/2405.01848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06581v1","updated":"2024-10-09T06:26:39Z","published":"2024-10-09T06:26:39Z","title":"Enhancing Legal Case Retrieval via Scaling High-quality Synthetic\n  Query-Candidate Pairs","summary":"  Legal case retrieval (LCR) aims to provide similar cases as references for a\ngiven fact description. This task is crucial for promoting consistent judgments\nin similar cases, effectively enhancing judicial fairness and improving work\nefficiency for judges. However, existing works face two main challenges for\nreal-world applications: existing works mainly focus on case-to-case retrieval\nusing lengthy queries, which does not match real-world scenarios; and the\nlimited data scale, with current datasets containing only hundreds of queries,\nis insufficient to satisfy the training requirements of existing data-hungry\nneural models. To address these issues, we introduce an automated method to\nconstruct synthetic query-candidate pairs and build the largest LCR dataset to\ndate, LEAD, which is hundreds of times larger than existing datasets. This data\nconstruction method can provide ample training signals for LCR models.\nExperimental results demonstrate that model training with our constructed data\ncan achieve state-of-the-art results on two widely-used LCR benchmarks.\nBesides, the construction method can also be applied to civil cases and achieve\npromising results. The data and codes can be found in\nhttps://github.com/thunlp/LEAD.\n","authors":["Cheng Gao","Chaojun Xiao","Zhenghao Liu","Huimin Chen","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.06581v1.pdf","comment":"15 pages, 3 figures, accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.06536v1","updated":"2024-10-09T04:20:15Z","published":"2024-10-09T04:20:15Z","title":"Learning Recommender Systems with Soft Target: A Decoupled Perspective","summary":"  Learning recommender systems with multi-class optimization objective is a\nprevalent setting in recommendation. However, as observed user feedback often\naccounts for a tiny fraction of the entire item pool, the standard Softmax loss\ntends to ignore the difference between potential positive feedback and truly\nnegative feedback. To address this challenge, we propose a novel decoupled soft\nlabel optimization framework to consider the objectives as two aspects by\nleveraging soft labels, including target confidence and the latent interest\ndistribution of non-target items. Futhermore, based on our carefully\ntheoretical analysis, we design a decoupled loss function to flexibly adjust\nthe importance of these two aspects. To maximize the performance of the\nproposed method, we additionally present a sensible soft-label generation\nalgorithm that models a label propagation algorithm to explore users' latent\ninterests in unobserved feedback via neighbors. We conduct extensive\nexperiments on various recommendation system models and public datasets, the\nresults demonstrate the effectiveness and generality of the proposed method.\n","authors":["Hao Zhang","Mingyue Cheng","Qi Liu","Yucong Luo","Rui Li","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.06536v1.pdf","comment":"Accepted by DASFAA 2024"},{"id":"http://arxiv.org/abs/2406.13941v2","updated":"2024-10-09T04:11:28Z","published":"2024-06-20T02:20:21Z","title":"UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture","summary":"  Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts.\n","authors":["Sitian Chen","Haobin Tan","Amelie Chi Zhou","Yusen Li","Pavan Balaji"],"pdf_url":"https://arxiv.org/pdf/2406.13941v2.pdf","comment":"Accepted by DAC 2024"},{"id":"http://arxiv.org/abs/2410.11870v1","updated":"2024-10-09T03:16:37Z","published":"2024-10-09T03:16:37Z","title":"Post-Userist Recommender Systems : A Manifesto","summary":"  We define userist recommendation as an approach to recommender systems framed\nsolely in terms of the relation between the user and system. Post-userist\nrecommendation posits a larger field of relations in which stakeholders are\nembedded and distinguishes the recommendation function (which can potentially\nconnect creators with audiences) from generative media. We argue that in the\nera of generative media, userist recommendation becomes indistinguishable from\npersonalized media generation, and therefore post-userist recommendation is the\nonly path forward for recommender systems research.\n","authors":["Robin Burke","Morgan Sylvester"],"pdf_url":"https://arxiv.org/pdf/2410.11870v1.pdf","comment":"Extended abstract for paper presented at AltRecSys Workshop 2024.\n  Held at the 18th ACM Conference on Recommender Systems, Bari, Italy. October\n  18, 2024"},{"id":"http://arxiv.org/abs/2410.06497v1","updated":"2024-10-09T02:51:27Z","published":"2024-10-09T02:51:27Z","title":"ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System","summary":"  The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements.\n","authors":["Fang Zhou","Yaning Huang","Dong Liang","Dai Li","Zhongke Zhang","Kai Wang","Xiao Xin","Abdallah Aboelela","Zheliang Jiang","Yang Wang","Jeff Song","Wei Zhang","Chen Liang","Huayu Li","ChongLin Sun","Hang Yang","Lei Qu","Zhan Shu","Mindi Yuan","Emanuele Maccherani","Taha Hayat","John Guo","Varna Puvvada","Uladzimir Pashkevich"],"pdf_url":"https://arxiv.org/pdf/2410.06497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12218v2","updated":"2024-10-09T02:27:04Z","published":"2023-09-20T14:59:15Z","title":"SR-PredictAO: Session-based Recommendation with High-Capability\n  Predictor Add-On","summary":"  Session-based recommendation, aiming at making the prediction of the user's\nnext item click based on the information in a single session only, even in the\npresence of some random user's behavior, is a complex problem. This complex\nproblem requires a high-capability model of predicting the user's next action.\nMost (if not all) existing models follow the encoder-predictor paradigm where\nall studies focus on how to optimize the encoder module extensively in the\nparadigm, but they overlook how to optimize the predictor module. In this\npaper, we discover the critical issue of the low-capability predictor module\namong existing models. Motivated by this, we propose a novel framework called\n*Session-based Recommendation with Predictor Add-On* (SR-PredictAO). In this\nframework, we propose a high-capability predictor module which could alleviate\nthe effect of random user's behavior for prediction. It is worth mentioning\nthat this framework could be applied to any existing models, which could give\nopportunities for further optimizing the framework. Extensive experiments on\ntwo real-world benchmark datasets for three state-of-the-art models show that\n*SR-PredictAO* out-performs the current state-of-the-art model by up to 2.9% in\nHR@20 and 2.3% in MRR@20. More importantly, the improvement is consistent\nacross almost all the existing models on all datasets, and is statistically\nsignificant, which could be regarded as a significant contribution in the\nfield.\n","authors":["Ruida Wang","Raymond Chi-Wing Wong","Weile Tan"],"pdf_url":"https://arxiv.org/pdf/2309.12218v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06443v1","updated":"2024-10-09T01:06:00Z","published":"2024-10-09T01:06:00Z","title":"Categorizing Social Media Screenshots for Identifying Author\n  Misattribution","summary":"  Mis/disinformation is a common and dangerous occurrence on social media.\nMisattribution is a form of mis/disinformation that deals with a false claim of\nauthorship, which means a user is claiming someone said (posted) something they\nnever did. We discuss the difference between misinformation and disinformation\nand how screenshots are used to spread author misattribution on social media\nplatforms. It is important to be able to find the original post of a screenshot\nto determine if the screenshot is being correctly attributed. To do this we\nhave built several tools to aid in automating this search process. The first is\na Python script that aims to categorize Twitter posts based on their structure,\nextract the metadata from a screenshot, and use this data to group all the\nposts within a screenshot together. We tested this process on 75 Twitter posts\ncontaining screenshots collected by hand to determine how well the script\nextracted metadata and grouped the individual posts, F1 = 0.80. The second is a\nseries of scrapers being used to collect a dataset that can train and test a\nmodel to differentiate between various social media platforms. We collected\n16,620 screenshots have been collected from Facebook, Instagram, Truth Social,\nand Twitter. Screenshots were taken by the scrapers of the web version and\nmobile version of each platform in both light and dark mode.\n","authors":["Ashlyn M. Farris","Michael L. Nelson"],"pdf_url":"https://arxiv.org/pdf/2410.06443v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.07415v1","updated":"2024-10-09T20:29:16Z","published":"2024-10-09T20:29:16Z","title":"3D2M Dataset: A 3-Dimension diverse Mesh Dataset","summary":"  Three-dimensional (3D) reconstruction has emerged as a prominent area of\nresearch, attracting significant attention from academia and industry alike.\nAmong the various applications of 3D reconstruction, facial reconstruction\nposes some of the most formidable challenges. Additionally, each individuals\nfacial structure is unique, requiring algorithms to be robust enough to handle\nthis variability while maintaining fidelity to the original features. This\narticle presents a comprehensive dataset of 3D meshes featuring a diverse range\nof facial structures and corresponding facial landmarks. The dataset comprises\n188 3D facial meshes, including 73 from female candidates and 114 from male\ncandidates. It encompasses a broad representation of ethnic backgrounds, with\ncontributions from 45 different ethnicities, ensuring a rich diversity in\nfacial characteristics. Each facial mesh is accompanied by key points that\naccurately annotate the relevant features, facilitating precise analysis and\nmanipulation. This dataset is particularly valuable for applications such as\nfacial re targeting, the study of facial structure components, and real-time\nperson representation in video streams. By providing a robust resource for\nresearchers and developers, it aims to advance the field of 3D facial\nreconstruction and related technologies.\n","authors":["Sankarshan Dasgupta"],"pdf_url":"https://arxiv.org/pdf/2410.07415v1.pdf","comment":"6 pages, 1 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.07369v1","updated":"2024-10-09T18:33:06Z","published":"2024-10-09T18:33:06Z","title":"An undetectable watermark for generative image models","summary":"  We present the first undetectable watermarking scheme for generative image\nmodels. Undetectability ensures that no efficient adversary can distinguish\nbetween watermarked and un-watermarked images, even after making many adaptive\nqueries. In particular, an undetectable watermark does not degrade image\nquality under any efficiently computable metric. Our scheme works by selecting\nthe initial latents of a diffusion model using a pseudorandom error-correcting\ncode (Christ and Gunn, 2024), a strategy which guarantees undetectability and\nrobustness. We experimentally demonstrate that our watermarks are\nquality-preserving and robust using Stable Diffusion 2.1. Our experiments\nverify that, in contrast to every prior scheme we tested, our watermark does\nnot degrade image quality. Our experiments also demonstrate robustness:\nexisting watermark removal attacks fail to remove our watermark from images\nwithout significantly degrading the quality of the images. Finally, we find\nthat we can robustly encode 512 bits in our watermark, and up to 2500 bits when\nthe images are not subjected to watermark removal attacks. Our code is\navailable at https://github.com/XuandongZhao/PRC-Watermark.\n","authors":["Sam Gunn","Xuandong Zhao","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2410.07369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07336v1","updated":"2024-10-09T18:00:09Z","published":"2024-10-09T18:00:09Z","title":"Positive-Augmented Contrastive Learning for Vision-and-Language\n  Evaluation and Training","summary":"  Despite significant advancements in caption generation, existing evaluation\nmetrics often fail to capture the full quality or fine-grained details of\ncaptions. This is mainly due to their reliance on non-specific human-written\nreferences or noisy pre-training data. Still, finding an effective metric is\ncrucial not only for captions evaluation but also for the generation phase.\nMetrics can indeed play a key role in the fine-tuning stage of captioning\nmodels, ultimately enhancing the quality of the generated captions. In this\npaper, we propose PAC-S++, a learnable metric that leverages the CLIP model,\npre-trained on both web-collected and cleaned data and regularized through\nadditional pairs of generated visual and textual positive samples. Exploiting\nthis stronger and curated pre-training, we also apply PAC-S++ as a reward in\nthe Self-Critical Sequence Training (SCST) stage typically employed to\nfine-tune captioning models. Extensive experiments on different image and video\ndatasets highlight the effectiveness of PAC-S++ compared to popular metrics for\nthe task, including its sensitivity to object hallucinations. Furthermore, we\nshow that integrating PAC-S++ into the fine-tuning stage of a captioning model\nresults in semantically richer captions with fewer repetitions and grammatical\nerrors. Evaluations on out-of-domain benchmarks further demonstrate the\nefficacy of our fine-tuning approach in enhancing model capabilities. Source\ncode and trained models are publicly available at:\nhttps://github.com/aimagelab/pacscore.\n","authors":["Sara Sarto","Nicholas Moratelli","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2410.07336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05412v2","updated":"2024-10-09T16:49:58Z","published":"2023-12-08T23:55:19Z","title":"CMMD: Contrastive Multi-Modal Diffusion for Video-Audio Conditional\n  Modeling","summary":"  We introduce a multi-modal diffusion model tailored for the bi-directional\nconditional generation of video and audio. We propose a joint contrastive\ntraining loss to improve the synchronization between visual and auditory\noccurrences. We present experiments on two datasets to evaluate the efficacy of\nour proposed model. The assessment of generation quality and alignment\nperformance is carried out from various angles, encompassing both objective and\nsubjective metrics. Our findings demonstrate that the proposed model\noutperforms the baseline in terms of quality and generation speed through\nintroduction of our novel cross-modal easy fusion architectural block.\nFurthermore, the incorporation of the contrastive loss results in improvements\nin audio-visual alignment, particularly in the high-correlation video-to-audio\ngeneration task.\n","authors":["Ruihan Yang","Hannes Gamper","Sebastian Braun"],"pdf_url":"https://arxiv.org/pdf/2312.05412v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06729v1","updated":"2024-10-09T09:55:51Z","published":"2024-10-09T09:55:51Z","title":"Perceptual Quality Assessment of Octree-RAHT Encoded 3D Point Clouds","summary":"  No-reference bitstream-layer point cloud quality assessment (PCQA) can be\ndeployed without full decoding at any network node to achieve real-time quality\nmonitoring. In this work, we focus on the PCQA problem dedicated to Octree-RAHT\nencoding mode. First, to address the issue that existing PCQA databases have a\nsmall scale and limited distortion levels, we establish the WPC5.0 database\nwhich is the first one dedicated to Octree-RAHT encoding mode with a scale of\n400 distorted point clouds (PCs) including 4 geometric multiplied by 5 attitude\ndistortion levels. Then, we propose the first PCQA model dedicated to\nOctree-RAHT encoding mode by parsing PC bitstreams without full decoding. The\nmodel introduces texture bitrate (TBPP) to predict texture complexity (TC) and\nfurther derives the texture distortion factor. In addition, the Geometric\nQuantization Parameter (PQS) is used to estimate the geometric distortion\nfactor, which is then integrated into the model along with the texture\ndistortion factor to obtain the proposed PCQA model named streamPCQ-OR. The\nproposed model has been compared with other advanced PCQA methods on the\nWPC5.0, BASICS and M-PCCD databases, and experimental results show that our\nmodel has excellent performance while having very low computational complexity,\nproviding a reliable choice for time-critical applications. To facilitate\nsubsequent research, the database and source code will be publicly released at\nhttps://github.com/qdushl/Waterloo-Point-Cloud-Database-5.0.\n","authors":["Dongshuai Duan","Honglei Su","Qi Liu","Hui Yuan","Wei Gao","Jiarun Song","Zhou Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06725v1","updated":"2024-10-09T09:46:53Z","published":"2024-10-09T09:46:53Z","title":"Evaluating the Impact of Point Cloud Colorization on Semantic\n  Segmentation Accuracy","summary":"  Point cloud semantic segmentation, the process of classifying each point into\npredefined categories, is essential for 3D scene understanding. While\nimage-based segmentation is widely adopted due to its maturity, methods relying\nsolely on RGB information often suffer from degraded performance due to color\ninaccuracies. Recent advancements have incorporated additional features such as\nintensity and geometric information, yet RGB channels continue to negatively\nimpact segmentation accuracy when errors in colorization occur. Despite this,\nprevious studies have not rigorously quantified the effects of erroneous\ncolorization on segmentation performance. In this paper, we propose a novel\nstatistical approach to evaluate the impact of inaccurate RGB information on\nimage-based point cloud segmentation. We categorize RGB inaccuracies into two\ntypes: incorrect color information and similar color information. Our results\ndemonstrate that both types of color inaccuracies significantly degrade\nsegmentation accuracy, with similar color errors particularly affecting the\nextraction of geometric features. These findings highlight the critical need to\nreassess the role of RGB information in point cloud segmentation and its\nimplications for future algorithm design.\n","authors":["Qinfeng Zhu","Jiaze Cao","Yuanzhi Cai","Lei Fan"],"pdf_url":"https://arxiv.org/pdf/2410.06725v1.pdf","comment":"Accepted by 2024 IEEE 8th International Conference on Vision, Image\n  and Signal Processing"},{"id":"http://arxiv.org/abs/2410.06654v1","updated":"2024-10-09T08:06:15Z","published":"2024-10-09T08:06:15Z","title":"Performance Evaluation in Multimedia Retrieval","summary":"  Performance evaluation in multimedia retrieval, as in the information\nretrieval domain at large, relies heavily on retrieval experiments, employing a\nbroad range of techniques and metrics. These can involve human-in-the-loop and\nmachine-only settings for the retrieval process itself and the subsequent\nverification of results. Such experiments can be elaborate and\nuse-case-specific, which can make them difficult to compare or replicate. In\nthis paper, we present a formal model to express all relevant aspects of such\nretrieval experiments, as well as a flexible open-source evaluation\ninfrastructure that implements the model. These contributions intend to make a\nstep towards lowering the hurdles for conducting retrieval experiments and\nimproving their reproducibility.\n","authors":["Loris Sauter","Ralph Gasser","Heiko Schuldt","Abraham Bernstein","Luca Rossetto"],"pdf_url":"https://arxiv.org/pdf/2410.06654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06618v1","updated":"2024-10-09T07:14:49Z","published":"2024-10-09T07:14:49Z","title":"Decomposing Relationship from 1-to-N into N 1-to-1 for Text-Video\n  Retrieval","summary":"  Text-video retrieval (TVR) has seen substantial advancements in recent years,\nfueled by the utilization of pre-trained models and large language models\n(LLMs). Despite these advancements, achieving accurate matching in TVR remains\nchallenging due to inherent disparities between video and textual modalities\nand irregularities in data representation. In this paper, we propose\nText-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the\nconventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships.\nBy replacing a single text query with a series of text proxies, TV-ProxyNet not\nonly broadens the query scope but also achieves a more precise expansion. Each\ntext proxy is crafted through a refined iterative process, controlled by\nmechanisms we term as the director and dash, which regulate the proxy's\ndirection and distance relative to the original text query. This setup not only\nfacilitates more precise semantic alignment but also effectively manages the\ndisparities and noise inherent in multimodal data. Our experiments on three\nrepresentative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet\nCaptions, demonstrate the effectiveness of TV-ProxyNet. The results show an\nimprovement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved\nstate-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0%\nimprovement on DiDeMo compared to existing methods, validating our approach's\nability to enhance semantic mapping and reduce error propensity.\n","authors":["Jian Xiao","Zhenzhen Hu","Jia Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2410.06618v1.pdf","comment":null}]}}